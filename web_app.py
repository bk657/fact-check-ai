import streamlit as st
from supabase import create_client, Client
import re
import requests
import time
import random
import math
from datetime import datetime
from collections import Counter
import yt_dlp
import pandas as pd
from bs4 import BeautifulSoup 

# --- [1. ÏãúÏä§ÌÖú ÏÑ§Ï†ï] ---
st.set_page_config(page_title="Fact-Check Center v47.1 (Final Stable)", layout="wide", page_icon="‚öñÔ∏è")

# üåü Secrets
try:
    YOUTUBE_API_KEY = st.secrets["YOUTUBE_API_KEY"]
    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    ADMIN_PASSWORD = st.secrets["ADMIN_PASSWORD"]
except:
    st.error("‚ùå ÌïÑÏàò ÌÇ§(API Key, DB Key, Password)Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
    st.stop()

@st.cache_resource
def init_supabase():
    return create_client(SUPABASE_URL, SUPABASE_KEY)

supabase = init_supabase()

# --- [Í¥ÄÎ¶¨Ïûê Ïù∏Ï¶ù Î°úÏßÅ] ---
if "is_admin" not in st.session_state: st.session_state["is_admin"] = False

with st.sidebar:
    st.header("üõ°Ô∏è Í¥ÄÎ¶¨Ïûê Î©îÎâ¥")
    # v47.1Ïùò Form Î∞©Ïãù Ïú†ÏßÄ
    with st.form("login_form"):
        password_input = st.text_input("Í¥ÄÎ¶¨Ïûê ÎπÑÎ∞ÄÎ≤àÌò∏", type="password")
        submit_button = st.form_submit_button("Î°úÍ∑∏Ïù∏")
        if submit_button:
            if password_input == ADMIN_PASSWORD:
                st.session_state["is_admin"] = True; st.rerun()
            else:
                st.session_state["is_admin"] = False; st.error("ÎπÑÎ∞ÄÎ≤àÌò∏ Î∂àÏùºÏπò")

    if st.session_state["is_admin"]:
        st.success("‚úÖ Í¥ÄÎ¶¨Ïûê Ïù∏Ï¶ùÎê®")
        if st.button("Î°úÍ∑∏ÏïÑÏõÉ"):
            st.session_state["is_admin"] = False; st.rerun()

# --- [ÏÉÅÏàò ÏÑ§Ï†ï (v47.1 Í∏∞Ï§Ä)] ---
WEIGHT_NEWS_DEFAULT = 45       
WEIGHT_VECTOR = 35     
WEIGHT_CONTENT = 15    
WEIGHT_SENTIMENT_DEFAULT = 10  
PENALTY_ABUSE = 20     
PENALTY_MISMATCH = 30
PENALTY_NO_FACT = 25
PENALTY_SILENT_ECHO = 40  

VITAL_KEYWORDS = ['ÏúÑÎèÖ', 'ÏÇ¨Îßù', 'Î≥ÑÏÑ∏', 'Íµ¨ÏÜç', 'Ï≤¥Ìè¨', 'Í∏∞ÏÜå', 'Ïã§Ìòï', 'ÏùëÍ∏âÏã§', 'Ïì∞Îü¨Ï†∏', 'Ïù¥Ìòº', 'Î∂àÌôî', 'ÌååÍ≤Ω', 'Ï∂©Í≤©', 'Í≤ΩÏïÖ', 'ÏÜçÎ≥¥', 'Í∏¥Í∏â', 'Ìè≠Î°ú', 'ÏñëÏÑ±', 'ÌôïÏßÑ', 'Ïã¨Ï†ïÏßÄ', 'ÎáåÏÇ¨', 'Ï§ëÌÉú', 'ÏïïÏàòÏàòÏÉâ', 'ÏÜåÌôò', 'Ìá¥ÏßÑ', 'ÌÉÑÌïµ', 'Î™ªÎÑòÍ∏¥Îã§']
VIP_ENTITIES = ['Ïú§ÏÑùÏó¥', 'ÎåÄÌÜµÎ†π', 'Ïù¥Ïû¨Î™Ö', 'ÌïúÎèôÌõà', 'ÍπÄÍ±¥Ìù¨', 'Î¨∏Ïû¨Ïù∏', 'Î∞ïÍ∑ºÌòú', 'Ïù¥Î™ÖÎ∞ï', 'Ìä∏ÎüºÌîÑ', 'Î∞îÏù¥Îì†', 'Ìë∏Ìã¥', 'Ï†§Î†åÏä§ÌÇ§', 'ÏãúÏßÑÌïë', 'Ï†ïÏùÄ', 'Ïù¥Ï§ÄÏÑù', 'Ï°∞Íµ≠', 'Ï∂îÎØ∏Ïï†', 'ÌôçÏ§ÄÌëú', 'Ïú†ÏäπÎØº', 'ÏïàÏ≤†Ïàò', 'ÏÜêÌù•ÎØº', 'Ïù¥Í∞ïÏù∏', 'ÍπÄÎØºÏû¨', 'Î•òÌòÑÏßÑ', 'Ïû¨Ïö©', 'Ï†ïÏùòÏÑ†', 'ÏµúÌÉúÏõê']
# v47.2ÏóêÏÑú ÏàòÏ†ï ÏöîÏ≤≠ÌñàÎçò ÏóÑÍ≤©Ìïú Í≥µÏãù Ï±ÑÎÑê Î¶¨Ïä§Ìä∏ Ï†ÅÏö©
OFFICIAL_CHANNELS = ['MBC', 'KBS', 'SBS', 'EBS', 'YTN', 'JTBC', 'TVCHOSUN', 'MBN', 'CHANNEL A', 'OBS', 'Ï±ÑÎÑêA', 'TVÏ°∞ÏÑ†', 'Ïó∞Ìï©Îâ¥Ïä§', 'YONHAP', 'ÌïúÍ≤®Î†à', 'Í≤ΩÌñ•', 'Ï°∞ÏÑ†ÏùºÎ≥¥', 'Ï§ëÏïôÏùºÎ≥¥', 'ÎèôÏïÑÏùºÎ≥¥', 'ÌïúÍµ≠ÏùºÎ≥¥', 'Íµ≠ÎØºÏùºÎ≥¥', 'ÏÑúÏö∏Ïã†Î¨∏', 'ÏÑ∏Í≥ÑÏùºÎ≥¥', 'Î¨∏ÌôîÏùºÎ≥¥', 'Îß§ÏùºÍ≤ΩÏ†ú', 'ÌïúÍµ≠Í≤ΩÏ†ú', 'ÏÑúÏö∏Í≤ΩÏ†ú', 'CHOSUN', 'JOONGANG', 'DONGA', 'HANKYOREH', 'KYUNGHYANG']

STATIC_TRUTH_CORPUS = ["Î∞ïÎÇòÎûò ÏúÑÏû•Ï†ÑÏûÖ ÏùòÌòπ Î¨¥ÌòêÏùò", "ÏûÑÏòÅÏõÖ ÏΩòÏÑúÌä∏ ÏïîÌëú ÎåÄÏùë", "Ï†ïÌù¨Ïõê ÍµêÏàò Ï†ÄÏÜçÎÖ∏Ìôî", "ÎåÄÏ†Ñ Ï∂©ÎÇ® ÌñâÏ†ï ÌÜµÌï©", "ÏÑ†Í±∞ Ï∂úÎßà ÏÑ†Ïñ∏", "Í∞ïÌõàÏãù ÏùòÏõê Ï∂úÎßàÏÑ§"]
STATIC_FAKE_CORPUS = ["Ï∂©Í≤© Ìè≠Î°ú Í≤ΩÏïÖ", "Í∏¥Í∏â ÏÜçÎ≥¥ ÏÜåÎ¶Ñ", "Ï∂©Í≤© Î∞úÏñ∏ ÎÖºÎûÄ", "Íµ¨ÏÜç ÏòÅÏû• Î∞úÎ∂Ä", "ÏòÅÏÉÅ Ïú†Ï∂ú", "ÍøàÏÜç Í≥ÑÏãú ÏòàÏñ∏", "ÏÇ¨Ìòï ÏÑ†Í≥† ÏßëÌñâ", "Í±¥Í∞ï ÏïÖÌôî ÏúÑÎèÖÏÑ§"]

class VectorEngine:
    def __init__(self): self.vocab = set(); self.truth_vectors = []; self.fake_vectors = []
    def tokenize(self, text): return re.findall(r'[Í∞Ä-Ìû£]{2,}', text)
    def train(self, t_corpus, f_corpus):
        for t in t_corpus + f_corpus: self.vocab.update(self.tokenize(t))
        self.vocab = sorted(list(self.vocab))
        self.truth_vectors = [self.text_to_vector(t) for t in t_corpus]
        self.fake_vectors = [self.text_to_vector(t) for t in f_corpus]
    def text_to_vector(self, text):
        c = Counter(self.tokenize(text)); return [c[w] for w in self.vocab]
    def cosine_similarity(self, v1, v2):
        dot = sum(a*b for a,b in zip(v1,v2)); mag = math.sqrt(sum(a*a for a in v1)) * math.sqrt(sum(b*b for b in v2))
        return dot/mag if mag>0 else 0
    def analyze_position(self, query):
        qv = self.text_to_vector(query)
        mt = max([self.cosine_similarity(qv, v) for v in self.truth_vectors] or [0])
        mf = max([self.cosine_similarity(qv, v) for v in self.fake_vectors] or [0])
        return mt, mf

vector_engine = VectorEngine()

def save_analysis(channel, title, prob, url, keywords):
    try: supabase.table("analysis_history").insert({"channel_name": channel, "video_title": title, "fake_prob": prob, "analysis_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'), "video_url": url, "keywords": keywords}).execute()
    except: pass

def train_dynamic_vector_engine():
    try:
        dt = [row['video_title'] for row in supabase.table("analysis_history").select("video_title").lt("fake_prob", 30).execute().data]
        df = [row['video_title'] for row in supabase.table("analysis_history").select("video_title").gt("fake_prob", 70).execute().data]
    except: dt, df = [], []
    vector_engine.train(STATIC_TRUTH_CORPUS + dt, STATIC_FAKE_CORPUS + df)
    return len(STATIC_TRUTH_CORPUS + dt) + len(STATIC_FAKE_CORPUS + df)

# --- [Helper Functions] ---
def colored_progress_bar(label, percent, color):
    st.markdown(f"""<div style="margin-bottom: 10px;"><div style="display: flex; justify-content: space-between; margin-bottom: 3px;"><span style="font-size: 13px; font-weight: 600; color: #555;">{label}</span><span style="font-size: 13px; font-weight: 700; color: {color};">{round(percent * 100, 1)}%</span></div><div style="background-color: #eee; border-radius: 5px; height: 8px; width: 100%;"><div style="background-color: {color}; height: 8px; width: {percent * 100}%; border-radius: 5px;"></div></div></div>""", unsafe_allow_html=True)

def render_score_breakdown(data_list):
    style = """<style>table.score-table { width: 100%; border-collapse: separate; border-spacing: 0; border: 1px solid #e0e0e0; border-radius: 8px; overflow: hidden; font-family: sans-serif; font-size: 14px; margin-top: 10px;} table.score-table th { background-color: #f8f9fa; color: #495057; font-weight: bold; padding: 12px 15px; text-align: left; border-bottom: 1px solid #e0e0e0; } table.score-table td { padding: 12px 15px; border-bottom: 1px solid #f0f0f0; color: #333; } table.score-table tr:last-child td { border-bottom: none; } .badge { padding: 4px 8px; border-radius: 6px; font-weight: 700; font-size: 11px; display: inline-block; text-align: center; min-width: 45px; } .badge-danger { background-color: #ffebee; color: #d32f2f; } .badge-success { background-color: #e8f5e9; color: #2e7d32; } .badge-neutral { background-color: #f5f5f5; color: #757575; border: 1px solid #e0e0e0; }</style>"""
    rows = ""
    for item, score, note in data_list:
        try:
            score_num = int(score)
            badge = f'<span class="badge badge-danger">+{score_num}</span>' if score_num > 0 else f'<span class="badge badge-success">{score_num}</span>' if score_num < 0 else f'<span class="badge badge-neutral">0</span>'
        except: badge = f'<span class="badge badge-neutral">{score}</span>'
        rows += f"<tr><td>{item}<br><span style='color:#888; font-size:11px;'>{note}</span></td><td style='text-align: right;'>{badge}</td></tr>"
    st.markdown(f"{style}<table class='score-table'><thead><tr><th>Î∂ÑÏÑù Ìï≠Î™© (Silent Echo Protocol)</th><th style='text-align: right;'>Î≥ÄÎèô</th></tr></thead><tbody>{rows}</tbody></table>", unsafe_allow_html=True)

def witty_loading_sequence(count):
    messages = [f"üß† [Intelligence Level: {count}] ÎàÑÏ†Å ÏßÄÏãù Î°úÎìú Ï§ë...", "üîÑ 'Ï£ºÏñ¥(Modifier)' + 'ÌïµÏã¨Ïñ¥(Head)' Ïó≠Î∞©Ìñ• Í≤∞Ìï©(Back-Merge) Ï§ë...", "üéØ Î¨∏Îß•ÏùÑ ÌÜµÌï©ÌïòÏó¨ ÏôÑÎ≤ΩÌïú Í≤ÄÏÉâÏñ¥(Contextual Query) ÏÉùÏÑ±...", "üöÄ ÏúÑÏÑ±Ïù¥ Ïú†ÌäúÎ∏å Î≥∏ÏÇ¨ ÏÉÅÍ≥µÏùÑ ÏßÄÎÇòÍ∞ÄÎäî Ï§ë..."]
    with st.status("üïµÔ∏è Context Merger v47.1 Í∞ÄÎèô Ï§ë...", expanded=True) as status:
        for msg in messages: st.write(msg); time.sleep(0.4)
        st.write("‚úÖ Î∂ÑÏÑù Ï§ÄÎπÑ ÏôÑÎ£å!"); status.update(label="Î∂ÑÏÑù ÏôÑÎ£å!", state="complete", expanded=False)

def extract_nouns(text):
    noise = ['Ï∂©Í≤©', 'Í≤ΩÏïÖ', 'Ïã§Ï≤¥', 'ÎÇúÎ¶¨', 'Í≥µÍ∞ú', 'Î∞òÏùë', 'Î™ÖÎã®', 'ÎèôÏòÅÏÉÅ', 'ÏÇ¨ÏßÑ', 'ÏßëÏïà', 'ÏÜçÎ≥¥', 'Îã®ÎèÖ', 'Í≤∞Íµ≠', 'MBC', 'Îâ¥Ïä§', 'Ïù¥ÎØ∏ÏßÄ', 'ÎÑàÎ¨¥', 'Îã§Î•∏', 'ÏïåÍ≥†Î≥¥Îãà', '„Ñ∑„Ñ∑', 'ÏßÑÏßú', 'Ï†ïÎßê', 'ÏòÅÏÉÅ', 'ÏÇ¨Îûå', 'ÏÉùÍ∞Å', 'Ïò§ÎäòÎ∞§', 'Ïò§Îäò', 'ÎÇ¥Ïùº', 'ÏßÄÍ∏à', 'Î™ªÎÑòÍ∏¥Îã§', 'ÎÑòÍ∏¥Îã§', 'Ïù¥Ïú†', 'Ïôú', 'Ïïà']
    return list(dict.fromkeys([n for n in re.findall(r'[Í∞Ä-Ìû£]{2,}', text) if n not in noise]))

def generate_pinpoint_query(title, hashtags):
    clean_text = title + " " + " ".join([h.replace("#", "") for h in hashtags])
    words = clean_text.split()
    subject_chunk, object_word, vital_word = "", "", ""
    for vital in VITAL_KEYWORDS:
        if vital in clean_text: vital_word = vital; break
    for i, word in enumerate(words):
        match = re.match(r'([Í∞Ä-Ìû£A-Za-z0-9]+)(ÏùÄ|Îäî|Ïù¥|Í∞Ä|ÏùÑ|Î•º|Ïóê|ÏóêÍ≤å|Î°úÏÑú|Î°ú)', word)
        if match:
            noun, josa = match.group(1), match.group(2)
            if noun in ['Ïò§ÎäòÎ∞§', 'ÏßÄÍ∏à', 'Ïù¥Ïú†', 'Í≤∞Íµ≠']: continue
            if not subject_chunk and josa in ['ÏùÄ', 'Îäî', 'Ïù¥', 'Í∞Ä']:
                prev_noun = ""
                if i > 0:
                    prev_word = words[i-1]
                    if re.fullmatch(r'[Í∞Ä-Ìû£A-Za-z0-9]+', prev_word) and prev_word not in VITAL_KEYWORDS + ['Ï∂©Í≤©', 'ÏÜçÎ≥¥']: prev_noun = prev_word
                subject_chunk = f"{prev_noun} {noun}" if prev_noun else noun
            elif not object_word and josa in ['ÏùÑ', 'Î•º', 'Ïóê', 'ÏóêÍ≤å', 'Î°ú']:
                if noun not in VITAL_KEYWORDS and noun not in subject_chunk: object_word = noun
    query_parts = [p for p in [subject_chunk, object_word, vital_word] if p]
    if not subject_chunk: return " ".join(extract_nouns(title)[:3])
    return " ".join(query_parts)

def summarize_transcript(text):
    if not text or len(text) < 50: return "‚ö†Ô∏è ÏöîÏïΩÌï† ÏûêÎßâ ÎÇ¥Ïö©Ïù¥ Ï∂©Î∂ÑÌïòÏßÄ ÏïäÏäµÎãàÎã§."
    sents = re.split(r'(?<=[.?!])\s+', text)
    if len(sents) <= 3: return text
    freq = Counter(re.findall(r'[Í∞Ä-Ìû£]{2,}', text))
    ranked = sorted([(i, s, sum(freq[w] for w in re.findall(r'[Í∞Ä-Ìû£]{2,}',s))/len(re.findall(r'[Í∞Ä-Ìû£]{2,}',s) or [1])) for i,s in enumerate(sents) if 10<len(s)<150], key=lambda x:x[2], reverse=True)[:3]
    return f"üìå **ÌïµÏã¨ ÏöîÏïΩ**: {' '.join([r[1] for r in sorted(ranked, key=lambda x:x[0])])}"

def clean_html(raw_html): return BeautifulSoup(raw_html, "html.parser").get_text()

def detect_ai_content(info):
    is_ai, reasons = False, []
    text = (info.get('title', '') + " " + info.get('description', '') + " " + " ".join(info.get('tags', []))).lower()
    for kw in ['ai', 'artificial intelligence', 'chatgpt', 'deepfake', 'synthetic', 'Ïù∏Í≥µÏßÄÎä•', 'Îî•ÌéòÏù¥ÌÅ¨', 'Í∞ÄÏÉÅÏù∏Í∞Ñ']:
        if kw in text: is_ai = True; reasons.append(f"ÌÇ§ÏõåÎìú Í∞êÏßÄ: {kw}"); break
    return is_ai, ", ".join(reasons)

def check_is_official(channel_name):
    norm_name = channel_name.upper().replace(" ", "")
    return any(o in norm_name for o in OFFICIAL_CHANNELS)

def count_sensational_words(text):
    return sum(text.count(w) for w in ['Ï∂©Í≤©', 'Í≤ΩÏïÖ', 'Ïã§Ï≤¥', 'Ìè≠Î°ú', 'ÎÇúÎ¶¨', 'ÏÜçÎ≥¥', 'Í∏¥Í∏â', 'ÏÜåÎ¶Ñ', '„Ñ∑„Ñ∑', 'ÏßÑÏßú', 'Í≤∞Íµ≠', 'Í≥ÑÏãú', 'ÏòàÏñ∏', 'ÏúÑÎèÖ', 'ÏÇ¨Îßù', 'Ï§ëÌÉú'])

def check_tag_abuse(title, hashtags, channel_name):
    if check_is_official(channel_name): return 0, "Í≥µÏãù Ï±ÑÎÑê Î©¥Ï†ú"
    if not hashtags: return 0, "Ìï¥ÏãúÌÉúÍ∑∏ ÏóÜÏùå"
    tn = set(extract_nouns(title)); tgn = set(h.replace("#", "").split(":")[-1].strip() for h in hashtags)
    if len(tgn) < 2: return 0, "ÏñëÌò∏"
    return (PENALTY_ABUSE, "üö® Ïã¨Í∞Å (Î∂àÏùºÏπò)") if not tn.intersection(tgn) else (0, "ÏñëÌò∏")

def fetch_real_transcript(info_dict):
    try:
        url = None
        for key in ['subtitles', 'automatic_captions']:
            if key in info_dict and 'ko' in info_dict[key]:
                for fmt in info_dict[key]['ko']:
                    if fmt['ext'] == 'vtt': url = fmt['url']; break
            if url: break
        if url:
            res = requests.get(url)
            if res.status_code == 200:
                clean = []
                for line in res.text.splitlines():
                    if '-->' not in line and 'WEBVTT' not in line and line.strip():
                        t = re.sub(r'<[^>]+>', '', line).strip()
                        if t and t not in clean: clean.append(t)
                return " ".join(clean), "‚úÖ Ïã§Ï†ú ÏûêÎßâ ÏàòÏßë ÏÑ±Í≥µ"
    except: pass
    return None, "ÏûêÎßâ Îã§Ïö¥Î°úÎìú Ïã§Ìå®"

def fetch_comments_via_api(video_id):
    try:
        url = "https://www.googleapis.com/youtube/v3/commentThreads"
        res = requests.get(url, params={'part': 'snippet', 'videoId': video_id, 'key': YOUTUBE_API_KEY, 'maxResults': 50, 'order': 'relevance'})
        if res.status_code == 200:
            items = [i['snippet']['topLevelComment']['snippet']['textDisplay'] for i in res.json().get('items', [])]
            return items, f"‚úÖ API ÏàòÏßë ÏÑ±Í≥µ (Top {len(items)})"
    except: pass
    return [], "‚ùå API ÌÜµÏã† Ïã§Ìå®"

def calculate_dual_match(news_item, query_nouns, transcript):
    tn = set(extract_nouns(news_item.get('title', ''))); dn = set(extract_nouns(news_item.get('desc', '')))
    qn = set(query_nouns)
    t_score = 1.0 if len(qn & tn) >= 2 else 0.5 if len(qn & tn) >= 1 else 0
    c_cnt = sum(1 for n in dn if n in transcript)
    c_score = 1.0 if (len(dn) > 0 and c_cnt/len(dn) >= 0.3) else 0.5 if (len(dn) > 0 and c_cnt/len(dn) >= 0.15) else 0
    return int((t_score * 0.3 + c_score * 0.7) * 100)

def analyze_comment_relevance(comments, context_text):
    if not comments: return [], 0, "Î∂ÑÏÑù Î∂àÍ∞Ä"
    cn = extract_nouns(" ".join(comments))
    top = Counter(cn).most_common(5)
    ctx = set(extract_nouns(context_text))
    match = sum(1 for w,c in top if w in ctx)
    score = int(match/len(top)*100) if top else 0
    msg = "‚úÖ Ï£ºÏ†ú ÏßëÏ§ë" if score >= 60 else "‚ö†Ô∏è ÏùºÎ∂Ä Í¥ÄÎ†®" if score >= 20 else "‚ùå Î¨¥Í¥Ä"
    return [f"{w}({c})" for w, c in top], score, msg

def check_red_flags(comments):
    detected = [k for c in comments for k in ['Í∞ÄÏßúÎâ¥Ïä§', 'Ï£ºÏûë', 'ÏÇ¨Í∏∞', 'Í±∞ÏßìÎßê', 'ÌóàÏúÑ', 'ÏÑ†Îèô'] if k in c]
    return len(detected), list(set(detected))

# üåü [Fix] XML -> Regex ÍµêÏ≤¥ (Í∏∞Îä• Î≥ÄÍ≤Ω ÏóÜÏùå, Ïò§ÏßÅ ÏóêÎü¨ Î∞©ÏßÄÏö©)
def fetch_news_safe(query):
    news_res = []
    try:
        rss = f"https://news.google.com/rss/search?q={requests.utils.quote(query)}&hl=ko&gl=KR"
        raw = requests.get(rss, timeout=5).text
        items = re.findall(r'<item>(.*?)</item>', raw, re.DOTALL)
        for item in items[:3]:
            t = re.search(r'<title>(.*?)</title>', item); d = re.search(r'<description>(.*?)</description>', item)
            nt = t.group(1).replace("<![CDATA[", "").replace("]]>", "") if t else ""
            nd = clean_html(d.group(1).replace("<![CDATA[", "").replace("]]>", "")) if d else ""
            news_res.append({'title': nt, 'desc': nd})
    except: pass
    return news_res

# üåü [Fix] ÏÇ≠Ï†ú ÏΩúÎ∞± (ÏÇ≠Ï†ú Í∏∞Îä• ÏïàÏ†ï
