import streamlit as st
from supabase import create_client, Client
import re
import requests
import time
import random
import math
import os
import json
from collections import Counter
from datetime import datetime

# --- [ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸] ---
from mistralai import Mistral
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
import yt_dlp
import pandas as pd
import altair as alt
from bs4 import BeautifulSoup

# --- [1. ì‹œìŠ¤í…œ ì„¤ì •] ---
st.set_page_config(page_title="ìœ íŠœë¸Œ ê°€ì§œë‰´ìŠ¤ íŒë…ê¸° (Triple Engine)", layout="wide", page_icon="ğŸ›¡ï¸")

if "is_admin" not in st.session_state:
    st.session_state["is_admin"] = False
if "debug_logs" not in st.session_state:
    st.session_state["debug_logs"] = []

# ğŸŒŸ Secrets ë¡œë“œ
try:
    YOUTUBE_API_KEY = st.secrets["YOUTUBE_API_KEY"]
    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    ADMIN_PASSWORD = st.secrets["ADMIN_PASSWORD"]
    
    MISTRAL_API_KEY = st.secrets["MISTRAL_API_KEY"]
    GOOGLE_API_KEY_A = st.secrets["GOOGLE_API_KEY_A"]
    GOOGLE_API_KEY_B = st.secrets["GOOGLE_API_KEY_B"]
except:
    st.error("âŒ secrets.toml íŒŒì¼ ì„¤ì • ì˜¤ë¥˜")
    st.stop()

@st.cache_resource
def init_clients():
    su = create_client(SUPABASE_URL, SUPABASE_KEY)
    mi = Mistral(api_key=MISTRAL_API_KEY)
    return su, mi

supabase, mistral_client = init_clients()

# --- [2. ëª¨ë¸ ì •ì˜] ---
MISTRAL_MODELS = ["mistral-large-latest", "mistral-medium-latest", "mistral-small-latest", "open-mixtral-8x22b"]

def get_gemini_models_dynamic(api_key):
    genai.configure(api_key=api_key)
    try:
        models = [m.name.replace("models/", "") for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        models.sort(key=lambda x: 0 if 'flash' in x else 1 if 'pro' in x else 2)
        return models
    except: return ["gemini-2.0-flash", "gemini-1.5-flash"]

# --- [3. ìœ í‹¸ë¦¬í‹°] ---
def parse_llm_json(text):
    try:
        parsed = json.loads(text)
    except:
        try:
            text = re.sub(r'```json\s*', '', text).replace('```', '')
            match = re.search(r'(\{.*\}|\[.*\])', text, re.DOTALL)
            if match: parsed = json.loads(match.group(1))
            else: return None
        except: return None
    if isinstance(parsed, list): return parsed[0] if len(parsed) > 0 and isinstance(parsed[0], dict) else None
    if isinstance(parsed, dict): return parsed
    return None

# --- [4. Triple Hybrid Survivor Logic] ---
def call_triple_survivor(prompt, is_json=False):
    logs = []
    response_format = {"type": "json_object"} if is_json else None
    
    # 1. Mistral
    for model_name in MISTRAL_MODELS:
        try:
            resp = mistral_client.chat.complete(
                model=model_name, messages=[{"role": "user", "content": prompt}],
                response_format=response_format, temperature=0.2
            )
            if resp.choices:
                return resp.choices[0].message.content, f"{model_name}", logs
        except Exception as e:
            logs.append(f"âŒ Mistral Failed: {str(e)[:30]}")
            continue

    # 2. Gemini A
    genai.configure(api_key=GOOGLE_API_KEY_A)
    for model_name in get_gemini_models_dynamic(GOOGLE_API_KEY_A):
        try:
            m = genai.GenerativeModel(model_name)
            r = m.generate_content(prompt)
            if r.text: return r.text, f"{model_name} (Key A)", logs
        except: continue

    # 3. Gemini B
    genai.configure(api_key=GOOGLE_API_KEY_B)
    for model_name in get_gemini_models_dynamic(GOOGLE_API_KEY_B):
        try:
            m = genai.GenerativeModel(model_name)
            r = m.generate_content(prompt)
            if r.text: return r.text, f"{model_name} (Key B)", logs
        except: continue

    return None, "All Failed", logs

# --- [5. ìƒìˆ˜ ë° ë¶„ì„ ì—”ì§„] ---
WEIGHT_ALGO = 0.85; WEIGHT_AI = 0.15
OFFICIAL_CHANNELS = ['MBC', 'KBS', 'SBS', 'EBS', 'YTN', 'JTBC', 'TVCHOSUN', 'MBN', 'CHANNEL A', 'OBS', 'ì±„ë„A', 'TVì¡°ì„ ', 'ì—°í•©ë‰´ìŠ¤', 'YONHAP', 'í•œê²¨ë ˆ', 'ê²½í–¥', 'ì¡°ì„ ', 'ì¤‘ì•™', 'ë™ì•„']
CRITICAL_STATE_KEYWORDS = ['ë³„ê±°', 'ì´í˜¼', 'íŒŒê²½', 'ì‚¬ë§', 'ìœ„ë…', 'êµ¬ì†', 'ì²´í¬', 'ì‹¤í˜•', 'ë¶ˆí™”', 'í­ë¡œ', 'ì¶©ê²©', 'ë…¼ë€', 'ì¤‘íƒœ', 'ì‹¬ì •ì§€', 'ë‡Œì‚¬', 'ì••ìˆ˜ìˆ˜ìƒ‰', 'ì†Œí™˜', 'íŒŒì‚°', 'ë¹šë”ë¯¸', 'ì „ê³¼', 'ê°ì˜¥', 'ê°„ì²©']
STATIC_TRUTH_CORPUS = ["ë°•ë‚˜ë˜ ìœ„ì¥ì „ì… ë¬´í˜ì˜", "ì„ì˜ì›… ì•”í‘œ ëŒ€ì‘", "ì •í¬ì› ì €ì†ë…¸í™”", "ëŒ€ì „ ì¶©ë‚¨ í†µí•©", "ì„ ê±° ì¶œë§ˆ ì„ ì–¸"]
STATIC_FAKE_CORPUS = ["ì¶©ê²© í­ë¡œ ê²½ì•…", "ê¸´ê¸‰ ì†ë³´ ì†Œë¦„", "ì¶©ê²© ë°œì–¸ ë…¼ë€", "êµ¬ì† ì˜ì¥ ë°œë¶€", "ì˜ìƒ ìœ ì¶œ", "ê³„ì‹œ ì˜ˆì–¸", "ì‚¬í˜• ì§‘í–‰", "ìœ„ë…ì„¤"]

class VectorEngine:
    def __init__(self):
        self.vocab = set(); self.truth_vectors = []; self.fake_vectors = []
    def tokenize(self, text): return re.findall(r'[ê°€-í£]{2,}', text)
    def train(self, truth, fake):
        for t in truth + fake: self.vocab.update(self.tokenize(t))
        self.vocab = sorted(list(self.vocab))
        self.truth_vectors = [self.text_to_vector(t) for t in truth]
        self.fake_vectors = [self.text_to_vector(t) for t in fake]
    def text_to_vector(self, text):
        c = Counter(self.tokenize(text))
        return [c[w] for w in self.vocab]
    def cosine_similarity(self, v1, v2):
        dot = sum(a*b for a,b in zip(v1,v2))
        mag = math.sqrt(sum(a*a for a in v1)) * math.sqrt(sum(b*b for b in v2))
        return dot/mag if mag>0 else 0
    def analyze_position(self, query):
        if not self.vocab: return 0, 0
        qv = self.text_to_vector(query)
        mt = max([self.cosine_similarity(qv, v) for v in self.truth_vectors] or [0])
        mf = max([self.cosine_similarity(qv, v) for v in self.fake_vectors] or [0])
        return mt, mf

vector_engine = VectorEngine()

# [í•¨ìˆ˜ë“¤]
def get_hybrid_search_keywords(title, transcript):
    prompt = f"Investigator. Input: {title}, {transcript[:15000]}. Task: Extract ONE Korean news search query. Output string only."
    res, model, logs = call_triple_survivor(prompt)
    st.session_state["debug_logs"].extend([f"[Key A] {l}" for l in logs])
    return (res.strip(), f"âœ¨ {model}") if res else (title, "Error")

def scrape_news_content_robust(url):
    try:
        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=5)
        soup = BeautifulSoup(res.text, 'html.parser')
        for t in soup(['script', 'style', 'nav', 'footer']): t.decompose()
        text = " ".join([p.get_text().strip() for p in soup.find_all('p') if len(p.get_text()) > 30])
        return (text[:4000], res.url) if len(text) > 100 else (None, res.url)
    except: return None, url

def deep_verify_news(video_summary, news_url, news_snippet):
    txt, real_url = scrape_news_content_robust(news_url)
    evidence = txt if txt else news_snippet
    source = "Full" if txt else "Snippet"
    prompt = f"Compare Video vs News. Video: {video_summary[:2000]}. News: {evidence}. Match(90-100), Mismatch(0-10). JSON {{score:int, reason:korean_str}}"
    res, model, logs = call_triple_survivor(prompt, is_json=True)
    st.session_state["debug_logs"].extend([f"[Verify] {l}" for l in logs])
    parsed = parse_llm_json(res)
    if parsed: return parsed.get('score', 0), parsed.get('reason', 'N/A'), source, evidence, real_url
    return 0, "Error", "Error", "", news_url

def get_hybrid_verdict_final(title, transcript, news_list):
    summary = "\n".join([f"- {n['ë‰´ìŠ¤ ì œëª©']} (Score:{n['ìµœì¢… ì ìˆ˜']}, Reason:{n['ë¶„ì„ ê·¼ê±°']})" for n in news_list])
    prompt = f"Judge Verdict. Title: {title}. Evidence: {summary}. Truth(0-30), Fake(70-100). JSON {{score:int, reason:korean_str}}"
    res, model, logs = call_triple_survivor(prompt, is_json=True)
    st.session_state["debug_logs"].extend([f"[Judge] {l}" for l in logs])
    parsed = parse_llm_json(res)
    if parsed: return parsed.get('score', 50), f"{parsed.get('reason')} (By {model})"
    return 50, "Judge Failed"

# --- [B2B ë¦¬í¬íŠ¸ ìƒì„± ì—”ì§„ (ì—ëŸ¬ ìˆ˜ì •ë¨)] ---
def generate_b2b_report_logic(df_history):
    if df_history.empty: return pd.DataFrame()
    
    # 1. ë°ì´í„° ê°•ì œ í˜•ë³€í™˜ (NaNì€ 0ìœ¼ë¡œ)
    df_history['fake_prob'] = pd.to_numeric(df_history['fake_prob'], errors='coerce').fillna(0)
    
    # 2. ì•ˆì „í•œ GroupBy (ì§ì ‘ ê³„ì‚° ë°©ì‹)
    grouped = df_history.groupby('channel_name')
    
    # 3. ì»¬ëŸ¼ë³„ ë…ë¦½ ê³„ì‚° í›„ ë³‘í•©
    report = pd.DataFrame({
        'analyzed_count': grouped['fake_prob'].count(),
        'avg_risk': grouped['fake_prob'].mean(),
        'max_risk': grouped['fake_prob'].max(),
        'all_keywords': grouped['keywords'].apply(lambda x: ' '.join([str(k) for k in x if k]))
    }).reset_index()
    
    results = []
    for _, row in report.iterrows():
        avg_score = row['avg_risk']
        
        if avg_score >= 60: grade = "â›” BLACKLIST"
        elif avg_score >= 40: grade = "âš ï¸ CAUTION"
        else: grade = "âœ… SAFE"
        
        tokens = re.findall(r'[ê°€-í£]{2,}', str(row['all_keywords']))
        targets = ", ".join([t[0] for t in Counter(tokens).most_common(3)])
        
        results.append({
            "ì±„ë„ëª…": row['channel_name'],
            "ìœ„í—˜ ë“±ê¸‰": grade,
            "í‰ê·  ê°€ì§œ í™•ë¥ ": f"{int(avg_score)}%",
            "ìµœê³  ê°€ì§œ í™•ë¥ ": f"{int(row['max_risk'])}%",
            "ë¶„ì„ ì˜ìƒ ìˆ˜": f"{int(row['analyzed_count'])}ê°œ",
            "ì£¼ìš” íƒ€ê²Ÿ": targets
        })
        
    return pd.DataFrame(results).sort_values(by='í‰ê·  ê°€ì§œ í™•ë¥ ', ascending=False)

# --- [6. ìœ í‹¸ë¦¬í‹° 2] ---
def normalize_korean_word(word):
    word = re.sub(r'[^ê°€-í£0-9]', '', word)
    for j in ['ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ì˜','ì—','ì—ê²Œ','ë¡œ','ìœ¼ë¡œ']:
        if word.endswith(j): return word[:-len(j)]
    return word

def extract_top_keywords_from_transcript(text, top_n=5):
    raw = re.findall(r'[ê°€-í£]{2,}', text)
    noise = ['ì¶©ê²©','ì†ë³´','ê¸´ê¸‰','ì˜¤ëŠ˜','ì§€ê¸ˆ','ê²°êµ­','ë‰´ìŠ¤','ì˜ìƒ']
    tokens = [normalize_korean_word(w) for w in raw if w not in noise]
    return Counter(tokens).most_common(top_n)

def train_dynamic_vector_engine():
    try:
        res_t = supabase.table("analysis_history").select("video_title").lt("fake_prob", 40).execute()
        res_f = supabase.table("analysis_history").select("video_title").gt("fake_prob", 60).execute()
        dt = [r['video_title'] for r in res_t.data] if res_t.data else []
        df = [r['video_title'] for r in res_f.data] if res_f.data else []
        vector_engine.train(STATIC_TRUTH_CORPUS + dt, STATIC_FAKE_CORPUS + df)
        return len(dt)+len(df), dt, df
    except: 
        vector_engine.train(STATIC_TRUTH_CORPUS, STATIC_FAKE_CORPUS)
        return 0, [], []

def save_analysis(channel, title, prob, url, keywords, report_data):
    try: supabase.table("analysis_history").insert({"channel_name": channel, "video_title": title, "fake_prob": prob, "analysis_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'), "video_url": url, "keywords": keywords, "detail_json": json.dumps(report_data, ensure_ascii=False)}).execute()
    except: pass

def render_score_breakdown(data_list):
    style = """<style>table.score-table { width: 100%; border-collapse: separate; border-spacing: 0; border: 1px solid #e0e0e0; border-radius: 8px; font-size: 14px; margin-top: 10px;} table.score-table th { background-color: #f8f9fa; padding: 12px; text-align: left; } table.score-table td { padding: 12px; border-bottom: 1px solid #f0f0f0; } .badge-danger { background-color: #ffebee; color: #d32f2f; padding: 4px 8px; border-radius: 4px; font-weight: bold; } .badge-success { background-color: #e8f5e9; color: #2e7d32; padding: 4px 8px; border-radius: 4px; font-weight: bold; }</style>"""
    rows = ""
    for item, score, note in data_list:
        try: val = int(score); badge = f'<span class="badge-danger">+{val}</span>' if val > 0 else f'<span class="badge-success">{val}</span>' if val < 0 else "0"
        except: badge = str(score)
        rows += f"<tr><td>{item}<br><small style='color:#888;'>{note}</small></td><td style='text-align:right;'>{badge}</td></tr>"
    st.markdown(f"{style}<table class='score-table'><thead><tr><th>ë¶„ì„ í•­ëª©</th><th style='text-align:right;'>ë³€ë™</th></tr></thead><tbody>{rows}</tbody></table>", unsafe_allow_html=True)

def colored_progress_bar(label, percent, color):
    st.markdown(f"""<div style="margin-bottom: 10px;"><div style="display: flex; justify-content: space-between;"><span style="font-size: 13px; font-weight: 600;">{label}</span><span>{round(percent * 100, 1)}%</span></div><div style="background-color: #eee; height: 8px; border-radius: 5px;"><div style="background-color: {color}; height: 8px; width: {percent * 100}%; border-radius: 5px;"></div></div></div>""", unsafe_allow_html=True)

def summarize_transcript(text, title): return text[:800] + "..."
def clean_html_regex(text): return re.sub('<.*?>', '', text).strip()
def check_is_official(ch): return any(o in ch.upper().replace(" ","") for o in OFFICIAL_CHANNELS)
def count_sensational_words(text): return sum(text.count(w) for w in ['ì¶©ê²©', 'ê²½ì•…', 'ì‹¤ì²´', 'í­ë¡œ', 'ì†ë³´'])
def detect_ai_content(info): return False, ""

def render_report_full_ui(prob, db_count, title, uploader, d, is_cached=False):
    if is_cached: st.success("ğŸ‰ ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ë¡œë“œ (Smart Cache)")

    st.subheader(f"ğŸ•µï¸ Triple-Engine Analysis Result")
    c1, c2, c3 = st.columns(3)
    c1.metric("ìµœì¢… ê°€ì§œë‰´ìŠ¤ í™•ë¥ ", f"{prob}%", delta=f"AI Judge: {d.get('ai_score', 'N/A')}")
    verdict = "ì•ˆì „" if prob < 30 else "ìœ„í—˜" if prob > 60 else "ì£¼ì˜"
    c2.metric("ì¢…í•© íŒì •", f"{verdict}")
    c3.metric("ì§€ëŠ¥ ë…¸ë“œ", f"{db_count} Nodes")
    
    st.divider()
    render_intelligence_distribution(prob)
    
    col1, col2 = st.columns([1, 1.4])
    with col1:
        st.write("**[ì˜ìƒ ì •ë³´]**")
        st.table(pd.DataFrame({"í•­ëª©": ["ì œëª©", "ì±„ë„", "í•´ì‹œíƒœê·¸"], "ë‚´ìš©": [title, uploader, d.get('tags','ì—†ìŒ')]}))
        st.info(f"ê²€ìƒ‰ì–´: {d.get('query', 'N/A')}")
        with st.container(border=True):
            st.markdown("ğŸ“ **ì˜ìƒ ë‚´ìš© ìš”ì•½**")
            st.write(d.get('summary','ë‚´ìš© ì—†ìŒ'))
        st.write("**[Score Breakdown]**")
        render_score_breakdown(d.get('score_breakdown', []))
    
    with col2:
        st.subheader("ğŸ“Š 5ëŒ€ ì¦ê±°")
        colored_progress_bar("ì§„ì‹¤ ê·¼ì ‘ë„", d.get('ts', 0), "#2ecc71")
        colored_progress_bar("ê±°ì§“ ê·¼ì ‘ë„", d.get('fs', 0), "#e74c3c")
        st.write("---")
        st.markdown("**[ì¦ê±° 1] ë‰´ìŠ¤ êµì°¨ ëŒ€ì¡°**")
        if d.get('news_evidence'):
            st.dataframe(pd.DataFrame(d.get('news_evidence', [])), column_config={"ì›ë¬¸": st.column_config.LinkColumn("ë§í¬")}, hide_index=True)
        else: st.warning("ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ")
        
        st.markdown("**[ì¦ê±° 2] ëŒ“ê¸€ ì—¬ë¡  ë¶„ì„**")
        st.table(pd.DataFrame([
            ["ë¶„ì„ ëŒ“ê¸€ ìˆ˜", f"{d.get('cmt_count',0)}ê°œ"],
            ["ë¹ˆì¶œ í‚¤ì›Œë“œ", ", ".join(d.get('top_cmt_kw', []))],
            ["ë…¼ë€ ê°ì§€", f"{d.get('red_cnt',0)}íšŒ"],
            ["ì£¼ì œ ì¼ì¹˜", d.get('cmt_rel', '')]
        ], columns=["í•­ëª©", "ë‚´ìš©"]))
        
        st.markdown("**[ì¦ê±° 3] ìë§‰ ë¶„ì„**")
        st.table(pd.DataFrame([["ì„ ë™ì„± ì§€ìˆ˜", f"{d.get('agitation',0)}íšŒ"]], columns=["í•­ëª©", "ë‚´ìš©"]))
        
        st.markdown("**[ì¦ê±° 4] AI ìµœì¢… íŒê²°**")
        with st.container(border=True):
            st.write(f"âš–ï¸ {d.get('ai_reason', 'N/A')}")

def run_forensic_main(url):
    st.session_state["debug_logs"] = []
    my_bar = st.progress(0, text="Triple Engine ê°€ë™ ì¤‘...")
    db_count, _, _ = train_dynamic_vector_engine()
    
    vid = re.search(r'(?:v=|\/)([0-9A-Za-z_-]{11}).*', url)
    if vid: vid = vid.group(1)

    # ìºì‹œ ì²´í¬
    cached_res = supabase.table("analysis_history").select("*").ilike("video_url", f"%{vid}%").order("id", desc=True).limit(1).execute()
    if cached_res.data:
        c = cached_res.data[0]
        try:
            d = json.loads(c.get('detail_json', '{}'))
            render_report_full_ui(c['fake_prob'], db_count, c['video_title'], c['channel_name'], d, is_cached=True)
            return
        except: pass

    with yt_dlp.YoutubeDL({'quiet': True, 'skip_download': True}) as ydl:
        try:
            info = ydl.extract_info(url, download=False)
            title = info.get('title', ''); uploader = info.get('uploader', '')
            tags = info.get('tags', []); desc = info.get('description', '')
            
            my_bar.progress(10, "1ë‹¨ê³„: ë°ì´í„° ìˆ˜ì§‘...")
            trans, _ = fetch_real_transcript(info)
            full_text = trans if trans else desc
            summary = summarize_transcript(full_text, title)
            
            my_bar.progress(30, "2ë‹¨ê³„: AI ìˆ˜ì‚¬ê´€...")
            query, _ = get_hybrid_search_keywords(title, full_text)

            my_bar.progress(50, "3ë‹¨ê³„: ë‰´ìŠ¤ ëŒ€ì¡°...")
            news_items = fetch_news_regex(query)
            news_ev = []; max_match = 0
            ts, fs = vector_engine.analyze_position(query + " " + title)
            t_impact = int(ts * 30) * -1; f_impact = int(fs * 30)

            for idx, item in enumerate(news_items[:3]):
                ai_s, ai_r, src, txt, real_url = deep_verify_news(summary, item['link'], item['desc'])
                if ai_s > max_match: max_match = ai_s
                news_ev.append({"ë‰´ìŠ¤ ì œëª©": item['title'], "ì¼ì¹˜ë„": f"{ai_s}%", "ìµœì¢… ì ìˆ˜": ai_s, "ë¶„ì„ ê·¼ê±°": ai_r, "ë¹„ê³ ": src, "ì›ë¬¸": real_url})
            
            news_score = -40 if max_match >= 80 else (-15 if max_match >= 70 else (10 if max_match >= 60 else 30)) if news_ev else 0
            
            cmts = fetch_comments_via_api(vid)
            top_cmt, rel_score, rel_msg = analyze_comment_relevance(cmts, title + full_text)
            red_cnt, _ = check_red_flags(cmts)
            
            silent_penalty = 0; is_silent = (len(news_ev) == 0)
            if is_silent:
                if any(k in title for k in CRITICAL_STATE_KEYWORDS): silent_penalty = 10
                elif count_sensational_words(title) >= 3: silent_penalty = 20
            if check_is_official(uploader): news_score = -50; silent_penalty = 0
            
            bait = 10 if any(w in title for w in ['ì¶©ê²©','ê²½ì•…','í­ë¡œ']) else -5
            algo_base = 50 + t_impact + f_impact + news_score + (min(20, red_cnt*3)) + bait + silent_penalty
            
            my_bar.progress(90, "5ë‹¨ê³„: ìµœì¢… íŒê²°...")
            ai_judge_score, ai_judge_reason = get_hybrid_verdict_final(title, full_text, news_ev)
            
            neutralized = False
            if t_impact == 0 and f_impact == 0 and is_silent:
                neutralized = True
                ai_judge_score = int((ai_judge_score + 50) / 2)
                algo_base = int((algo_base + 50) / 2)
            
            final_prob = max(1, min(99, int(algo_base * WEIGHT_ALGO + ai_judge_score * WEIGHT_AI)))
            
            score_bd = [["ê¸°ë³¸ ì ìˆ˜", 50, "ì¤‘ë¦½"], ["ì§„ì‹¤ DB", t_impact, ""], ["ê°€ì§œ íŒ¨í„´", f_impact, ""], ["ë‰´ìŠ¤ ë§¤ì¹­", news_score, ""], ["AI íŒê²°", ai_judge_score, ""]]
            
            report_data = {
                "summary": summary, "news_evidence": news_ev, "ai_score": ai_judge_score, "ai_reason": ai_judge_reason,
                "score_breakdown": score_bd, "ts": ts, "fs": fs, "query": query, "tags": ", ".join(tags),
                "cmt_count": len(cmts), "top_cmt_kw": top_cmt, "red_cnt": red_cnt, "cmt_rel": f"{rel_score}% ({rel_msg})",
                "agitation": count_sensational_words(title)
            }
            
            save_analysis(uploader, title, final_prob, url, query, report_data)
            my_bar.empty()
            render_report_full_ui(final_prob, db_count, title, uploader, report_data, is_cached=False)

        except Exception as e: st.error(f"ì˜¤ë¥˜: {e}")

# --- [UI] ---
st.title("âš–ï¸ìœ íŠœë¸Œ ê°€ì§œë‰´ìŠ¤ íŒë…ê¸° (Triple Engine)")
with st.container(border=True):
    st.markdown("### ğŸ›¡ï¸ ë²•ì  ê³ ì§€")
    agree = st.checkbox("ë™ì˜í•©ë‹ˆë‹¤")
url_input = st.text_input("ğŸ”— URL")
if st.button("ğŸš€ ë¶„ì„", disabled=not agree): run_forensic_main(url_input)

st.divider()
st.subheader("ğŸ—‚ï¸ í•™ìŠµ ë°ì´í„° ê´€ë¦¬")
try:
    resp = supabase.table("analysis_history").select("*").order("id", desc=True).execute()
    df = pd.DataFrame(resp.data)
except: df = pd.DataFrame()

# [ë³µêµ¬] ì¼ë°˜ ì‚¬ìš©ìë„ íˆìŠ¤í† ë¦¬ ì—´ëŒ ê°€ëŠ¥, ê´€ë¦¬ìëŠ” ìˆ˜ì • ê°€ëŠ¥
if not df.empty:
    if st.session_state["is_admin"]:
        df['Delete'] = False
        edited = st.data_editor(df, hide_index=True)
        if st.button("ğŸ—‘ï¸ ì„ íƒ ì‚­ì œ"):
            for _, row in edited[edited.Delete].iterrows():
                supabase.table("analysis_history").delete().eq("id", row['id']).execute()
            st.rerun()
    else:
        st.dataframe(df, hide_index=True)
else:
    st.info("ë°ì´í„° ì—†ìŒ")

# [ê´€ë¦¬ì]
with st.expander("ğŸ” ê´€ë¦¬ì"):
    if st.session_state["is_admin"]:
        st.success("Admin Active")
        st.divider()
        st.subheader("ğŸ¢ B2B ë¦¬í¬íŠ¸ ìƒì„±")
        if st.button("ğŸ“Š ë¦¬í¬íŠ¸ ë¶„ì„"):
            try:
                rpt = generate_b2b_report_logic(df)
                st.dataframe(rpt, use_container_width=True, hide_index=True)
                csv = rpt.to_csv(index=False).encode('utf-8-sig')
                st.download_button("ğŸ“¥ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ", csv, "b2b_report.csv", "text/csv")
            except Exception as e: st.error(f"ì‹¤íŒ¨: {e}")
        
        st.divider()
        st.write("Logs:")
        st.text_area("", "\n".join(st.session_state["debug_logs"]))
        if st.button("Logout"): st.session_state["is_admin"]=False; st.rerun()
    else:
        if st.text_input("PW", type="password") == ADMIN_PASSWORD: st.session_state["is_admin"]=True; st.rerun()
