import streamlit as st
from supabase import create_client, Client
import re
import requests
import time
import random
import math
import google.generativeai as genai
from datetime import datetime
from collections import Counter
import yt_dlp
import pandas as pd
import altair as alt

# --- [1. ì‹œìŠ¤í…œ ì„¤ì •] ---
st.set_page_config(page_title="Fact-Check Center v63.5 (Force Debug)", layout="wide", page_icon="âš–ï¸")

# ğŸŒŸ Secrets ë¡œë“œ
try:
    YOUTUBE_API_KEY = st.secrets["YOUTUBE_API_KEY"]
    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    ADMIN_PASSWORD = st.secrets["ADMIN_PASSWORD"]
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
except:
    st.error("âŒ í•„ìˆ˜ í‚¤(API Key, DB Key, Password)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

@st.cache_resource
def init_supabase():
    return create_client(SUPABASE_URL, SUPABASE_KEY)

supabase = init_supabase()

# --- [2. ìƒìˆ˜ ì •ì˜] ---
WEIGHT_NEWS_DEFAULT = 45; WEIGHT_VECTOR = 35; WEIGHT_CONTENT = 15; WEIGHT_SENTIMENT_DEFAULT = 10
PENALTY_ABUSE = 20; PENALTY_MISMATCH = 30; PENALTY_NO_FACT = 25; PENALTY_SILENT_ECHO = 40

VITAL_KEYWORDS = ['ìœ„ë…', 'ì‚¬ë§', 'ë³„ì„¸', 'êµ¬ì†', 'ì²´í¬', 'ê¸°ì†Œ', 'ì‹¤í˜•', 'ì‘ê¸‰ì‹¤', 'ì´í˜¼', 'ë¶ˆí™”', 'íŒŒê²½', 'ì¶©ê²©', 'ê²½ì•…', 'ì†ë³´', 'ê¸´ê¸‰', 'í­ë¡œ', 'ì–‘ì„±', 'í™•ì§„', 'ì‹¬ì •ì§€', 'ë‡Œì‚¬', 'ì¤‘íƒœ', 'ì••ìˆ˜ìˆ˜ìƒ‰', 'ì†Œí™˜', 'í‡´ì§„', 'íƒ„í•µ', 'ë‚´ë€', 'ê°„ì²©']
CRITICAL_STATE_KEYWORDS = ['ë³„ê±°', 'ì´í˜¼', 'íŒŒê²½', 'ì‚¬ë§', 'ìœ„ë…', 'êµ¬ì†', 'ì²´í¬', 'ì‹¤í˜•', 'ë¶ˆí™”', 'í­ë¡œ', 'ì¶©ê²©', 'ë…¼ë€', 'ì¤‘íƒœ', 'ì‹¬ì •ì§€', 'ë‡Œì‚¬', 'ì••ìˆ˜ìˆ˜ìƒ‰', 'ì†Œí™˜', 'íŒŒì‚°', 'ë¹šë”ë¯¸', 'ì „ê³¼', 'ê°ì˜¥', 'ê°„ì²©']
OFFICIAL_CHANNELS = ['MBC', 'KBS', 'SBS', 'EBS', 'YTN', 'JTBC', 'TVCHOSUN', 'MBN', 'CHANNEL A', 'OBS', 'ì±„ë„A', 'TVì¡°ì„ ', 'ì—°í•©ë‰´ìŠ¤', 'YONHAP', 'í•œê²¨ë ˆ', 'ê²½í–¥', 'ì¡°ì„ ', 'ì¤‘ì•™', 'ë™ì•„']

STATIC_TRUTH_CORPUS = ["ë°•ë‚˜ë˜ ìœ„ì¥ì „ì… ë¬´í˜ì˜", "ì„ì˜ì›… ì•”í‘œ ëŒ€ì‘", "ì •í¬ì› ì €ì†ë…¸í™”", "ëŒ€ì „ ì¶©ë‚¨ í†µí•©", "ì„ ê±° ì¶œë§ˆ ì„ ì–¸"]
STATIC_FAKE_CORPUS = ["ì¶©ê²© í­ë¡œ ê²½ì•…", "ê¸´ê¸‰ ì†ë³´ ì†Œë¦„", "ì¶©ê²© ë°œì–¸ ë…¼ë€", "êµ¬ì† ì˜ì¥ ë°œë¶€", "ì˜ìƒ ìœ ì¶œ", "ê³„ì‹œ ì˜ˆì–¸", "ì‚¬í˜• ì§‘í–‰", "ìœ„ë…ì„¤"]

# --- [3. NLP & Vector Engine] ---
class VectorEngine:
    def __init__(self):
        self.vocab = set()
        self.truth_vectors = []
        self.fake_vectors = []
    def tokenize(self, text): return re.findall(r'[ê°€-í£]{2,}', text)
    def train(self, truth, fake):
        for t in truth + fake: self.vocab.update(self.tokenize(t))
        self.vocab = sorted(list(self.vocab))
        self.truth_vectors = [self.text_to_vector(t) for t in truth]
        self.fake_vectors = [self.text_to_vector(t) for t in fake]
    def text_to_vector(self, text):
        c = Counter(self.tokenize(text))
        return [c[w] for w in self.vocab]
    def cosine_similarity(self, v1, v2):
        dot = sum(a*b for a,b in zip(v1,v2))
        mag = math.sqrt(sum(a*a for a in v1)) * math.sqrt(sum(b*b for b in v2))
        return dot/mag if mag>0 else 0
    def analyze_position(self, query):
        qv = self.text_to_vector(query)
        mt = max([self.cosine_similarity(qv, v) for v in self.truth_vectors] or [0])
        mf = max([self.cosine_similarity(qv, v) for v in self.fake_vectors] or [0])
        return mt, mf

vector_engine = VectorEngine()

# --- [4. Gemini Logic (ëª¨ë¸ ìˆœí™˜ + ì—ëŸ¬ ë…¸ì¶œ)] ---
def get_gemini_search_keywords(title, transcript):
    genai.configure(api_key=GOOGLE_API_KEY)
    
    # 1. ì•ˆì „ ì„¤ì • (ìµœëŒ€ ê°œë°©)
    safety_settings = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
    ]
    
    # 2. ëª¨ë¸ í›„ë³´êµ° (ìˆœì„œëŒ€ë¡œ ì‹œë„)
    models_to_try = ['gemini-1.5-flash', 'gemini-1.5-pro', 'gemini-pro']
    
    prompt = f"""
    Extract ONE simple Korean search query (Nouns only).
    Input: {title}
    Context: {transcript[:500]}
    Rules: Remove emotional words. Return 'Person + Event'. No explanations.
    """
    
    last_error = ""
    
    # 3. ëª¨ë¸ ìˆœí™˜ ì‹œë„
    for model_name in models_to_try:
        try:
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(prompt, safety_settings=safety_settings)
            
            # ì‘ë‹µ ê²€ì¦
            if response.text:
                return response.text.strip(), f"âœ¨ Gemini ({model_name})"
        except Exception as e:
            last_error = str(e)
            continue # ë‹¤ìŒ ëª¨ë¸ ì‹œë„

    # 4. ëª¨ë“  ëª¨ë¸ ì‹¤íŒ¨ ì‹œ -> ë°±ì—… ë¡œì§ (ì—ëŸ¬ ì›ì¸ í¬í•¨)
    tokens = re.findall(r'[ê°€-í£]{2,}', title)
    # ì¡°ì‚¬ ì œê±°
    cleaned = []
    for t in tokens:
        t = re.sub(r'(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì˜)$', '', t)
        if len(t) > 1: cleaned.append(t)
        
    backup_query = " ".join(cleaned[:3]) if cleaned else title
    
    # ğŸš¨ ì‹¤íŒ¨ ì›ì¸ì„ ë¼ë²¨ì— í¬í•¨ì‹œì¼œì„œ ë³´ì—¬ì¤Œ
    return backup_query, f"ğŸ¤– Backup (Error: {last_error[:30]}...)"

# --- [5. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜] ---
def normalize_korean_word(word):
    word = re.sub(r'[^ê°€-í£0-9]', '', word)
    for j in ['ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ì˜','ì—','ì—ê²Œ','ë¡œ','ìœ¼ë¡œ']:
        if word.endswith(j): return word[:-len(j)]
    return word

def extract_meaningful_tokens(text):
    raw = re.findall(r'[ê°€-í£]{2,}', text)
    noise = ['ì¶©ê²©','ì†ë³´','ê¸´ê¸‰','ì˜¤ëŠ˜','ì§€ê¸ˆ','ê²°êµ­','ë‰´ìŠ¤','ì˜ìƒ']
    return [normalize_korean_word(w) for w in raw if w not in noise]

def train_dynamic_vector_engine():
    try:
        dt = [row['video_title'] for row in supabase.table("analysis_history").select("video_title").lt("fake_prob", 40).execute().data]
        df = [row['video_title'] for row in supabase.table("analysis_history").select("video_title").gt("fake_prob", 60).execute().data]
        vector_engine.train(STATIC_TRUTH_CORPUS + dt, STATIC_FAKE_CORPUS + df)
        return len(STATIC_TRUTH_CORPUS + dt) + len(STATIC_FAKE_CORPUS + df), len(dt), len(df)
    except: 
        vector_engine.train(STATIC_TRUTH_CORPUS, STATIC_FAKE_CORPUS)
        return 0, 0, 0

def save_analysis(channel, title, prob, url, keywords):
    try: supabase.table("analysis_history").insert({"channel_name": channel, "video_title": title, "fake_prob": prob, "analysis_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'), "video_url": url, "keywords": keywords}).execute()
    except: pass

def render_intelligence_distribution(current_prob):
    try:
        res = supabase.table("analysis_history").select("fake_prob").execute()
        if not res.data: return
        df = pd.DataFrame(res.data)
        base = alt.Chart(df).transform_density('fake_prob', as_=['fake_prob', 'density'], extent=[0, 100], bandwidth=5).mark_area(opacity=0.3, color='#888').encode(x=alt.X('fake_prob:Q', title='ê°€ì§œë‰´ìŠ¤ í™•ë¥  ë¶„í¬'), y=alt.Y('density:Q', title='ë°ì´í„° ë°€ë„'))
        rule = alt.Chart(pd.DataFrame({'x': [current_prob]})).mark_rule(color='blue', size=3).encode(x='x')
        st.altair_chart(base + rule, use_container_width=True)
    except: pass

def colored_progress_bar(label, percent, color):
    st.markdown(f"""<div style="margin-bottom: 10px;"><div style="display: flex; justify-content: space-between; margin-bottom: 3px;"><span style="font-size: 13px; font-weight: 600; color: #555;">{label}</span><span style="font-size: 13px; font-weight: 700; color: {color};">{round(percent * 100, 1)}%</span></div><div style="background-color: #eee; border-radius: 5px; height: 8px; width: 100%;"><div style="background-color: {color}; height: 8px; width: {percent * 100}%; border-radius: 5px;"></div></div></div>""", unsafe_allow_html=True)

def render_score_breakdown(data_list):
    style = """<style>table.score-table { width: 100%; border-collapse: separate; border-spacing: 0; border: 1px solid #e0e0e0; border-radius: 8px; overflow: hidden; font-family: sans-serif; font-size: 14px; margin-top: 10px;} table.score-table th { background-color: #f8f9fa; color: #495057; font-weight: bold; padding: 12px 15px; text-align: left; border-bottom: 1px solid #e0e0e0; } table.score-table td { padding: 12px 15px; border-bottom: 1px solid #f0f0f0; color: #333; } table.score-table tr:last-child td { border-bottom: none; } .badge { padding: 4px 8px; border-radius: 6px; font-weight: 700; font-size: 11px; display: inline-block; text-align: center; min-width: 45px; } .badge-danger { background-color: #ffebee; color: #d32f2f; } .badge-success { background-color: #e8f5e9; color: #2e7d32; } .badge-neutral { background-color: #f5f5f5; color: #757575; border: 1px solid #e0e0e0; }</style>"""
    rows = ""
    for item, score, note in data_list:
        try:
            score_num = int(score)
            badge = f'<span class="badge badge-danger">+{score_num}</span>' if score_num > 0 else f'<span class="badge badge-success">{score_num}</span>' if score_num < 0 else f'<span class="badge badge-neutral">0</span>'
        except: badge = f'<span class="badge badge-neutral">{score}</span>'
        rows += f"<tr><td>{item}<br><span style='color:#888; font-size:11px;'>{note}</span></td><td style='text-align: right;'>{badge}</td></tr>"
    st.markdown(f"{style}<table class='score-table'><thead><tr><th>ë¶„ì„ í•­ëª© (Silent Echo Protocol)</th><th style='text-align: right;'>ë³€ë™</th></tr></thead><tbody>{rows}</tbody></table>", unsafe_allow_html=True)

def summarize_transcript(text, title):
    if not text or len(text) < 50: return "ìš”ì•½ ë¶ˆê°€"
    return text[:200] + "..."

def check_tag_abuse(title, hashtags, channel):
    if any(o in channel for o in OFFICIAL_CHANNELS): return 0, "ê³µì‹ ì±„ë„"
    if not hashtags: return 0, "íƒœê·¸ ì—†ìŒ"
    return 0, "ì •ìƒ"

def fetch_real_transcript(info):
    try:
        url = None
        for k in ['subtitles', 'automatic_captions']:
            if k in info and 'ko' in info[k]:
                url = info[k]['ko'][0]['url']; break
        if url: return requests.get(url).text
    except: pass
    return info.get('description', '')

def fetch_comments_via_api(vid):
    try:
        url = "https://www.googleapis.com/youtube/v3/commentThreads"
        res = requests.get(url, params={'part':'snippet','videoId':vid,'key':YOUTUBE_API_KEY,'maxResults':20})
        if res.status_code == 200:
            return [i['snippet']['topLevelComment']['snippet']['textDisplay'] for i in res.json().get('items',[])], "ì„±ê³µ"
    except: pass
    return [], "ì‹¤íŒ¨"

def fetch_news_regex(query):
    try:
        rss = f"https://news.google.com/rss/search?q={requests.utils.quote(query)}&hl=ko&gl=KR"
        raw = requests.get(rss, timeout=3).text
        items = re.findall(r'<title>(.*?)</title>', raw)
        return [{'title':t.replace("<![CDATA[","").replace("]]>","")} for t in items[1:6]]
    except: return []

def calculate_dual_match(news, query_nouns, full_text, query):
    if not news: return 0
    return 0 # Placeholder

# --- [UI Layout] ---
st.title("âš–ï¸ Triple-Evidence Intelligence Forensic v63.5")
with st.container(border=True):
    agree = st.checkbox("ë™ì˜í•©ë‹ˆë‹¤.")

url_input = st.text_input("ğŸ”— ë¶„ì„í•  ìœ íŠœë¸Œ URL")
if st.button("ğŸš€ ì •ë°€ ë¶„ì„ ì‹œì‘", use_container_width=True, disabled=not agree):
    if url_input: 
        with yt_dlp.YoutubeDL({'quiet':True}) as ydl:
            info = ydl.extract_info(url_input, download=False)
            title = info['title']
            transcript = fetch_real_transcript(info)
            
            # ğŸš¨ ì—¬ê¸°ê°€ í•µì‹¬: ê²°ê³¼ í™•ì¸
            query, source = get_gemini_search_keywords(title, transcript)
            
            # ë‰´ìŠ¤ ê²€ìƒ‰ ë° ë¡œì§ ìˆ˜í–‰ (ê°„ì†Œí™”)
            news_items = fetch_news_regex(query)
            
            st.success("ë¶„ì„ ì™„ë£Œ")
            st.divider()
            
            # ê²°ê³¼ ì¶œë ¥
            st.info(f"ğŸ¯ **ì¶”ì¶œ ê²€ìƒ‰ì–´**: {query}")
            

[Image of magnifying glass over data]

            if "Error" in source:
                st.error(f"âš ï¸ **Gemini ì‹¤íŒ¨ ì›ì¸**: {source}")
            else:
                st.success(f"âœ… **ì„±ê³µ ì¶œì²˜**: {source}")
                
            st.write(f"ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼: {len(news_items)}ê±´")
            if news_items: st.dataframe(news_items)
            else: st.warning("ê²€ìƒ‰ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
