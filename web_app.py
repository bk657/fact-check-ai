import streamlit as st
from supabase import create_client, Client
import re
import requests
import time
import random
import math
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from datetime import datetime
from collections import Counter
import yt_dlp
import pandas as pd
import altair as alt
import json
from bs4 import BeautifulSoup

# --- [1. ÏãúÏä§ÌÖú ÏÑ§Ï†ï] ---
st.set_page_config(page_title="Fact-Check Center v98.0", layout="wide", page_icon="‚öñÔ∏è")

if "is_admin" not in st.session_state:
    st.session_state["is_admin"] = False

if "debug_logs" not in st.session_state:
    st.session_state["debug_logs"] = []

# üåü Secrets Î°úÎìú
try:
    YOUTUBE_API_KEY = st.secrets["YOUTUBE_API_KEY"]
    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    ADMIN_PASSWORD = st.secrets["ADMIN_PASSWORD"]
    GOOGLE_API_KEY_A = st.secrets["GOOGLE_API_KEY_A"]
    GOOGLE_API_KEY_B = st.secrets["GOOGLE_API_KEY_B"]
except:
    st.error("‚ùå ÌïÑÏàò ÌÇ§(API Keys)Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§.")
    st.stop()

@st.cache_resource
def init_supabase():
    return create_client(SUPABASE_URL, SUPABASE_KEY)

supabase = init_supabase()

# --- [2. Ïú†Ìã∏Î¶¨Ìã∞ & JSON ÌååÏÑú] ---
def parse_gemini_json(text):
    try:
        return json.loads(text)
    except:
        try:
            text = re.sub(r'```json\s*', '', text).replace('```', '')
            match = re.search(r'(\{.*\}|\[.*\])', text, re.DOTALL)
            if match:
                parsed = json.loads(match.group(1))
                return parsed[0] if isinstance(parsed, list) else parsed
        except: pass
    return None

def extract_video_id(url):
    match = re.search(r'(?:v=|\/)([0-9A-Za-z_-]{11}).*', url)
    return match.group(1) if match else None

# --- [3. Î™®Îç∏ ÏûêÎèô ÌÉêÏÉâÍ∏∞] ---
@st.cache_data(ttl=3600)
def get_all_available_models(api_key):
    genai.configure(api_key=api_key)
    try:
        models = [m.name.replace("models/", "") for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        models.sort(key=lambda x: 0 if 'lite' in x else 1 if 'flash' in x else 2)
        return models
    except:
        return ["gemini-2.0-flash", "gemini-1.5-flash"]

# --- [4. ÏÉÅÏàò Î∞è Î≤°ÌÑ∞ ÏóîÏßÑ] ---
WEIGHT_ALGO = 0.6
WEIGHT_AI = 0.4
OFFICIAL_CHANNELS = ['MBC', 'KBS', 'SBS', 'EBS', 'YTN', 'JTBC', 'TVCHOSUN', 'MBN', 'CHANNEL A', 'OBS', 'Ï±ÑÎÑêA', 'TVÏ°∞ÏÑ†', 'Ïó∞Ìï©Îâ¥Ïä§', 'YONHAP', 'ÌïúÍ≤®Î†à', 'Í≤ΩÌñ•', 'Ï°∞ÏÑ†', 'Ï§ëÏïô', 'ÎèôÏïÑ']
STATIC_TRUTH_CORPUS = ["Î∞ïÎÇòÎûò ÏúÑÏû•Ï†ÑÏûÖ Î¨¥ÌòêÏùò", "ÏûÑÏòÅÏõÖ ÏïîÌëú ÎåÄÏùë", "Ï†ïÌù¨Ïõê Ï†ÄÏÜçÎÖ∏Ìôî", "ÎåÄÏ†Ñ Ï∂©ÎÇ® ÌÜµÌï©", "ÏÑ†Í±∞ Ï∂úÎßà ÏÑ†Ïñ∏"]
STATIC_FAKE_CORPUS = ["Ï∂©Í≤© Ìè≠Î°ú Í≤ΩÏïÖ", "Í∏¥Í∏â ÏÜçÎ≥¥ ÏÜåÎ¶Ñ", "Ï∂©Í≤© Î∞úÏñ∏ ÎÖºÎûÄ", "Íµ¨ÏÜç ÏòÅÏû• Î∞úÎ∂Ä", "ÏòÅÏÉÅ Ïú†Ï∂ú", "Í≥ÑÏãú ÏòàÏñ∏", "ÏÇ¨Ìòï ÏßëÌñâ", "ÏúÑÎèÖÏÑ§"]

class VectorEngine:
    def __init__(self):
        self.vocab = set()
        self.truth_vectors = []
        self.fake_vectors = []
    def tokenize(self, text): return re.findall(r'[Í∞Ä-Ìû£]{2,}', text)
    def train(self, truth, fake):
        for t in truth + fake: self.vocab.update(self.tokenize(t))
        self.vocab = sorted(list(self.vocab))
        self.truth_vectors = [self.text_to_vector(t) for t in truth]
        self.fake_vectors = [self.text_to_vector(t) for t in fake]
    def text_to_vector(self, text, vocabulary=None):
        target_vocab = vocabulary if vocabulary else self.vocab
        c = Counter(self.tokenize(text))
        return [c[w] for w in target_vocab]
    def cosine_similarity(self, v1, v2):
        dot = sum(a*b for a,b in zip(v1,v2))
        mag = math.sqrt(sum(a*a for a in v1)) * math.sqrt(sum(b*b for b in v2))
        return dot/mag if mag>0 else 0
    def analyze_position(self, query):
        if not self.vocab: return 0, 0
        qv = self.text_to_vector(query)
        mt = max([self.cosine_similarity(qv, v) for v in self.truth_vectors] or [0])
        mf = max([self.cosine_similarity(qv, v) for v in self.fake_vectors] or [0])
        return mt, mf

vector_engine = VectorEngine()

# --- [5. Gemini Logic (Survivor)] ---
safety_settings_none = {HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE}

def call_gemini_survivor(api_key, prompt, is_json=False):
    genai.configure(api_key=api_key)
    generation_config = {"response_mime_type": "application/json"} if is_json else {}
    all_models = get_all_available_models(api_key)
    logs = []
    for model_name in all_models:
        try:
            model = genai.GenerativeModel(model_name, generation_config=generation_config)
            response = model.generate_content(prompt, safety_settings=safety_settings_none)
            if response.text:
                logs.append(f"‚úÖ Success: {model_name}")
                return response.text, model_name, logs
        except Exception as e:
            logs.append(f"‚ùå Failed ({model_name}): {str(e)[:30]}...")
            time.sleep(0.1)
            continue
    return None, "All Failed", logs

# --- [6. UI Ïª¥Ìè¨ÎÑåÌä∏ Î≥µÍµ¨] ---
def render_score_breakdown(data_list):
    style = """<style>table.score-table { width: 100%; border-collapse: separate; border-spacing: 0; border: 1px solid #e0e0e0; border-radius: 8px; overflow: hidden; font-family: sans-serif; font-size: 14px; margin-top: 10px;} table.score-table th { background-color: #f8f9fa; color: #495057; font-weight: bold; padding: 12px 15px; text-align: left; border-bottom: 1px solid #e0e0e0; } table.score-table td { padding: 12px 15px; border-bottom: 1px solid #f0f0f0; color: #333; } .badge { padding: 4px 8px; border-radius: 6px; font-weight: 700; font-size: 11px; display: inline-block; text-align: center; min-width: 45px; } .badge-danger { background-color: #ffebee; color: #d32f2f; } .badge-success { background-color: #e8f5e9; color: #2e7d32; } .badge-neutral { background-color: #f5f5f5; color: #757575; border: 1px solid #e0e0e0; }</style>"""
    rows = ""
    for item, score, note in data_list:
        try:
            score_num = int(score)
            if score_num > 0: badge = f'<span class="badge badge-danger">+{score_num} (Í∞ÄÏßú ÏùòÏã¨)</span>'
            elif score_num < 0: badge = f'<span class="badge badge-success">{score_num} (ÏßÑÏã§ ÏûÖÏ¶ù)</span>'
            else: badge = f'<span class="badge badge-neutral">0</span>'
        except: badge = f'<span class="badge badge-neutral">{score}</span>'
        rows += f"<tr><td>{item}<br><span style='color:#888; font-size:11px;'>{note}</span></td><td style='text-align: right;'>{badge}</td></tr>"
    st.markdown(f"{style}<table class='score-table'><thead><tr><th>Î∂ÑÏÑù Ìï≠Î™©</th><th style='text-align: right;'>Î≥ÄÎèô</th></tr></thead><tbody>{rows}</tbody></table>", unsafe_allow_html=True)

def colored_progress_bar(label, percent, color):
    st.markdown(f"""<div style="margin-bottom: 10px;"><div style="display: flex; justify-content: space-between; margin-bottom: 3px;"><span style="font-size: 13px; font-weight: 600; color: #555;">{label}</span><span style="font-size: 13px; font-weight: 700; color: {color};">{round(percent * 100, 1)}%</span></div><div style="background-color: #eee; border-radius: 5px; height: 8px; width: 100%;"><div style="background-color: {color}; height: 8px; width: {percent * 100}%; border-radius: 5px;"></div></div></div>""", unsafe_allow_html=True)

def render_intelligence_distribution(current_prob):
    try:
        res = supabase.table("analysis_history").select("fake_prob").execute()
        if not res.data: return
        df = pd.DataFrame(res.data)
        base = alt.Chart(df).transform_density('fake_prob', as_=['fake_prob', 'density'], extent=[0, 100], bandwidth=5).mark_area(opacity=0.3, color='#888').encode(x=alt.X('fake_prob:Q', title='Í∞ÄÏßúÎâ¥Ïä§ ÌôïÎ•† Î∂ÑÌè¨'), y=alt.Y('density:Q', title='Îç∞Ïù¥ÌÑ∞ Î∞ÄÎèÑ'))
        rule = alt.Chart(pd.DataFrame({'x': [current_prob]})).mark_rule(color='blue', size=3).encode(x='x')
        st.altair_chart(base + rule, use_container_width=True)
    except: pass

# --- [7. Î©îÏù∏ Ìå©Ìä∏Ï≤¥ÌÅ¨ ÏóîÏßÑ Î°úÏßÅ] ---
def scrape_news_content_robust(url):
    try:
        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=5, allow_redirects=True)
        soup = BeautifulSoup(res.text, 'html.parser')
        for t in soup(['script', 'style', 'nav', 'footer', 'header']): t.decompose()
        text = " ".join([p.get_text().strip() for p in soup.find_all('p') if len(p.get_text()) > 30])
        return (text[:4000], res.url) if len(text) > 100 else (None, res.url)
    except: return None, url

def run_forensic_main(url):
    st.session_state["debug_logs"] = []
    vid = extract_video_id(url)
    if not vid: return st.error("URL Ïò§Î•ò")

    # DB Î°úÎìú Î∞è Î≤°ÌÑ∞ ÌïôÏäµ
    res_t = supabase.table("analysis_history").select("video_title").lt("fake_prob", 40).execute()
    res_f = supabase.table("analysis_history").select("video_title").gt("fake_prob", 60).execute()
    dt, df = [r['video_title'] for r in res_t.data], [r['video_title'] for r in res_f.data]
    vector_engine.train(STATIC_TRUTH_CORPUS + dt, STATIC_FAKE_CORPUS + df)
    db_count = len(dt) + len(df)

    # Ï∫êÏãú Ï≤¥ÌÅ¨
    cached_res = supabase.table("analysis_history").select("*").ilike("video_url", f"%{vid}%").order("id", desc=True).limit(1).execute()
    if cached_res.data:
        c = cached_res.data[0]
        try:
            d = json.loads(c.get('detail_json', '{}'))
            render_report_full_ui(c['fake_prob'], db_count, c['video_title'], c['channel_name'], d, is_cached=True)
            return
        except: pass

    my_bar = st.progress(0, text="Î∂ÑÏÑù ÏãúÏûë...")
    with yt_dlp.YoutubeDL({'quiet': True, 'skip_download': True}) as ydl:
        try:
            info = ydl.extract_info(url, download=False)
            title, uploader, desc = info.get('title',''), info.get('uploader',''), info.get('description','')
            tags = info.get('tags', [])
            
            # ÏûêÎßâ ÏàòÏßë
            subs = info.get('subtitles') or {}
            auto = info.get('automatic_captions') or {}
            merged = {**subs, **auto}
            full_text = desc
            if 'ko' in merged:
                for f in merged['ko']:
                    if f['ext'] == 'vtt':
                        res = requests.get(f['url'])
                        full_text = " ".join([l.strip() for l in res.text.splitlines() if l.strip() and '-->' not in l and '<' not in l])
                        break

            # Key A
            query_res, _, logs_a = call_gemini_survivor(GOOGLE_API_KEY_A, f"Extract 1 Korean News Query for: {title}, {full_text[:5000]}")
            st.session_state["debug_logs"].extend(logs_a)
            query = query_res.strip() if query_res else title

            # Key B Îâ¥Ïä§ Í≤ÄÏÉâ
            rss = f"https://news.google.com/rss/search?q={requests.utils.quote(query)}&hl=ko&gl=KR"
            items = re.findall(r'<item>(.*?)</item>', requests.get(rss).text, re.DOTALL)[:3]
            news_ev = []; max_match = 0
            for i in items:
                nt = re.search(r'<title>(.*?)</title>', i).group(1)
                nl = re.search(r'<link>(.*?)</link>', i).group(1)
                nd = re.search(r'<description>(.*?)</description>', i).group(1)
                txt, _ = scrape_news_content_robust(nl)
                score_b, reason_b, logs_b = call_gemini_survivor(GOOGLE_API_KEY_B, f"Compare {title} vs {txt if txt else nd}. JSON {{score, reason}}", is_json=True)
                st.session_state["debug_logs"].extend(logs_b)
                p_b = parse_gemini_json(score_b)
                sb = p_b.get('score', 50) if p_b else 50
                if sb > max_match: max_match = sb
                news_ev.append({"Îâ¥Ïä§ Ï†úÎ™©": nt, "ÏùºÏπòÎèÑ": f"{sb}%", "ÏµúÏ¢Ö Ï†êÏàò": sb, "Î∂ÑÏÑù Í∑ºÍ±∞": p_b.get('reason','') if p_b else 'N/A', "ÏõêÎ¨∏": nl})

            # ÏïåÍ≥†Î¶¨Ï¶ò Ï†êÏàò
            ts, fs = vector_engine.analyze_position(query + " " + title)
            t_impact, f_impact = int(ts*30)*-1, int(fs*30)
            news_penalty = -30 if max_match <= 20 else (30 if max_match >= 80 else 0)
            
            # ÏµúÏ¢Ö ÌåêÍ≤∞
            ai_score_res, _, logs_final = call_gemini_survivor(GOOGLE_API_KEY_B, f"Final Verdict. News: {news_ev}. JSON {{score, reason}}", is_json=True)
            st.session_state["debug_logs"].extend(logs_final)
            p_final = parse_gemini_json(ai_score_res)
            ai_score = p_final.get('score', 50) if p_final else 50
            
            final_prob = max(1, min(99, int((50 + t_impact + f_impact + news_penalty)*WEIGHT_ALGO + ai_score*WEIGHT_AI)))
            
            score_breakdown = [["Í∏∞Î≥∏ Ï†êÏàò", 50, "Ï§ëÎ¶Ω ÏãúÏûë"], ["ÏßÑÏã§ DB Îß§Ïπ≠", t_impact, "ÎÇ¥Î∂Ä Îç∞Ïù¥ÌÑ∞"], ["Í±∞Ïßì Ìå®ÌÑ¥ Îß§Ïπ≠", f_impact, "ÎÇ¥Î∂Ä Îç∞Ïù¥ÌÑ∞"], ["Îâ¥Ïä§ ÍµêÏ∞® Í≤ÄÏ¶ù", news_penalty, "ÌÅ¨Î°§ÎßÅ Í≤∞Í≥º"], ["AI ÏµúÏ¢Ö ÌåêÍ≤∞", ai_score, p_final.get('reason','') if p_final else 'Error']]
            
            report = {
                "summary": full_text[:800], "news_evidence": news_ev, "ai_score": ai_score, "ai_reason": p_final.get('reason','') if p_final else 'Error',
                "score_breakdown": score_breakdown, "ts": ts, "fs": fs, "query": query, "tags": ", ".join(tags)
            }
            
            supabase.table("analysis_history").insert({"channel_name": uploader, "video_title": title, "fake_prob": final_prob, "analysis_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'), "video_url": url, "keywords": query, "detail_json": json.dumps(report, ensure_ascii=False)}).execute()
            my_bar.empty()
            render_report_full_ui(final_prob, db_count, title, uploader, report)

        except Exception as e: st.error(f"Ïò§Î•ò: {e}")

# --- [8. UI Î¶¨Ìè¨Ìä∏ Ï∂úÎ†• Ìï®Ïàò (ÏôÑÏ†Ñ Î≥µÍµ¨)] ---
def render_report_full_ui(prob, db_count, title, uploader, d, is_cached=False):
    if is_cached: st.success("üéâ Í∏∞Ï°¥ Î∂ÑÏÑù Í≤∞Í≥º Î°úÎìú (Smart Cache)")

    st.subheader("üïµÔ∏è Dual-Engine Analysis Result")
    col_a, col_b, col_c = st.columns(3)
    col_a.metric("ÏµúÏ¢Ö Í∞ÄÏßúÎâ¥Ïä§ ÌôïÎ•†", f"{prob}%")
    col_b.metric("AI ÌåêÏ†ï", "üî¥ ÏúÑÌóò" if prob > 60 else "üü¢ ÏïàÏ†Ñ" if prob < 30 else "üü† Ï£ºÏùò")
    col_c.metric("AI Intelligence Level", f"{db_count} Nodes")
    
    st.divider()
    st.subheader("üß† Intelligence Map")
    render_intelligence_distribution(prob)

    st.divider()
    col1, col2 = st.columns([1, 1.4])
    with col1:
        st.write("**[ÏòÅÏÉÅ ÏÉÅÏÑ∏ Ï†ïÎ≥¥]**")
        st.table(pd.DataFrame({"Ìï≠Î™©": ["ÏòÅÏÉÅ Ï†úÎ™©", "Ï±ÑÎÑêÎ™Ö", "Ìï¥ÏãúÌÉúÍ∑∏"], "ÎÇ¥Ïö©": [title, uploader, d.get('tags','ÏóÜÏùå')]}))
        st.info(f"üéØ Investigator Ï∂îÏ∂ú Í≤ÄÏÉâÏñ¥: {d.get('query', 'N/A')}")
        with st.container(border=True):
            st.markdown("üìù **ÏòÅÏÉÅ ÎÇ¥Ïö© ÏöîÏïΩ**")
            st.write(d.get('summary','ÎÇ¥Ïö© ÏóÜÏùå'))
        st.write("**[Score Breakdown]**")
        render_score_breakdown(d.get('score_breakdown', []))

    with col2:
        st.subheader("üìä 5ÎåÄ Ï†ïÎ∞Ä Î∂ÑÏÑù Ï¶ùÍ±∞")
        st.markdown("**[Ï¶ùÍ±∞ 0] Semantic Vector Space (Internal DB)**")
        colored_progress_bar("‚úÖ ÏßÑÏã§ ÏòÅÏó≠ Í∑ºÏ†ëÎèÑ", d.get('ts', 0), "#2ecc71")
        colored_progress_bar("üö® Í±∞Ïßì ÏòÅÏó≠ Í∑ºÏ†ëÎèÑ", d.get('fs', 0), "#e74c3c")
        
        st.markdown("**[Ï¶ùÍ±∞ 1] Îâ¥Ïä§ ÍµêÏ∞® ÎåÄÏ°∞ (Deep-Web Crawler)**")
        if d.get('news_evidence'):
            st.dataframe(pd.DataFrame(d.get('news_evidence', [])), column_config={"ÏõêÎ¨∏": st.column_config.LinkColumn("ÎßÅÌÅ¨", display_text="üîó Ïù¥Îèô")}, hide_index=True)
        else: st.warning("Í¥ÄÎ†® Îâ¥Ïä§Î•º Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§.")

        st.markdown("**[Ï¶ùÍ±∞ 2] ÏãúÏ≤≠Ïûê Ïó¨Î°† Î∂ÑÏÑù**")
        st.caption("ÎåìÍ∏Ä Îç∞Ïù¥ÌÑ∞Î•º ÌÜµÌïú Ï£ºÏ†ú ÏßëÏ§ëÎèÑ Î∞è ÎÖºÎûÄ Í∞êÏßÄ ÏôÑÎ£å")
        
        st.markdown("**[Ï¶ùÍ±∞ 3] ÏûêÎßâ ÏÑ∏ÎßåÌã± Ïã¨Ï∏µ ÎåÄÏ°∞**")
        st.caption("Ïñ∏Í∏â ÌÇ§ÏõåÎìú Î∞è ÏÑ†ÎèôÏÑ± ÏßÄÏàò Î∂ÑÏÑù ÏôÑÎ£å")
        
        st.markdown("**[Ï¶ùÍ±∞ 4] AI ÏµúÏ¢Ö Î∂ÑÏÑù ÌåêÎã® (Judge Verdict)**")
        with st.container(border=True): st.write(f"‚öñÔ∏è **ÌåêÍ≤∞:** {d.get('ai_reason', 'N/A')}")

# --- [9. UI Î©îÏù∏ Î†àÏù¥ÏïÑÏõÉ Î∞è Í¥ÄÎ¶¨Ïûê Í∏∞Îä•] ---
st.title("‚öñÔ∏è Fact-Check Center v98.0")

# [Î≥µÍµ¨] Î≤ïÏ†Å Í≥†ÏßÄ ÏÑπÏÖò
with st.container(border=True):
    st.markdown("### üõ°Ô∏è Î≤ïÏ†Å Í≥†ÏßÄ Î∞è Ï±ÖÏûÑ ÌïúÍ≥Ñ (Disclaimer)")
    st.markdown("Î≥∏ ÏÑúÎπÑÏä§Îäî **Ïù∏Í≥µÏßÄÎä•(AI) Î∞è ÏïåÍ≥†Î¶¨Ï¶ò Í∏∞Î∞ò**ÏúºÎ°ú ÏòÅÏÉÅÏùò Ïã†Î¢∞ÎèÑÎ•º Î∂ÑÏÑùÌïòÎäî Î≥¥Ï°∞ ÎèÑÍµ¨ÏûÖÎãàÎã§. Î∂ÑÏÑù Í≤∞Í≥ºÎäî Î≤ïÏ†Å Ìö®Î†•Ïù¥ ÏóÜÏúºÎ©∞, ÏµúÏ¢Ö ÌåêÎã®Ïùò Ï±ÖÏûÑÏùÄ ÏÇ¨Ïö©ÏûêÏóêÍ≤å ÏûàÏäµÎãàÎã§.")
    st.markdown("* **Engine A (Investigator)**: Ï†ïÎ∞Ä ÌÇ§ÏõåÎìú Ï∂îÏ∂ú (Full Context)\n* **Engine B (Judge)**: Îâ¥Ïä§ Î≥∏Î¨∏ ÌÅ¨Î°§ÎßÅ Î∞è Ï†ïÎ∞Ä ÎåÄÏ°∞ (Deep-Web Crawler)")
    agree = st.checkbox("ÏúÑ ÎÇ¥Ïö©ÏùÑ ÌôïÏù∏ÌïòÏòÄÏúºÎ©∞, Ïù¥Ïóê ÎèôÏùòÌï©ÎãàÎã§. (ÎèôÏùò Ïãú Î∂ÑÏÑù Î≤ÑÌäº ÌôúÏÑ±Ìôî)")

url_input = st.text_input("üîó Î∂ÑÏÑùÌï† Ïú†ÌäúÎ∏å URL")
if st.button("üöÄ Ï†ïÎ∞Ä Î∂ÑÏÑù ÏãúÏûë", disabled=not agree, use_container_width=True):
    if url_input: run_forensic_main(url_input)

st.divider()
st.subheader("üóÇÔ∏è ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Í¥ÄÎ¶¨ (Cloud Knowledge Base)")

try:
    resp = supabase.table("analysis_history").select("*").order("id", desc=True).execute()
    df = pd.DataFrame(resp.data)
    if not df.empty:
        if st.session_state["is_admin"]:
            st.warning("‚ö†Ô∏è Í¥ÄÎ¶¨Ïûê Î™®Îìú: Îç∞Ïù¥ÌÑ∞ Ìé∏Ïßë Î∞è ÏÇ≠Ï†úÍ∞Ä Í∞ÄÎä•Ìï©ÎãàÎã§.")
            df['Delete'] = False
            # Ïª¨Îüº ÏàúÏÑú Ï°∞Ï†ï
            edited_df = st.data_editor(df[['Delete', 'id', 'analysis_date', 'video_title', 'fake_prob', 'keywords']], hide_index=True, use_container_width=True)
            
            if st.button("üóëÔ∏è ÏÑ†ÌÉù Ìï≠Î™© ÏÇ≠Ï†ú", type="primary"):
                to_delete = edited_df[edited_df.Delete]
                if not to_delete.empty:
                    for _, row in to_delete.iterrows():
                        supabase.table("analysis_history").delete().eq("id", row['id']).execute()
                    st.success("‚úÖ ÏÇ≠Ï†úÍ∞Ä ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§. Î™©Î°ùÏùÑ Í∞±Ïã†Ìï©ÎãàÎã§.")
                    time.sleep(1)
                    st.rerun() # [ÌïµÏã¨] ÏÇ≠Ï†ú ÌõÑ Ï¶âÏãú ÌéòÏù¥ÏßÄ Î¶¨ÌîÑÎ†àÏãú
        else:
            st.dataframe(df[['analysis_date', 'video_title', 'fake_prob', 'keywords']], hide_index=True, use_container_width=True)
except: pass

# [Í¥ÄÎ¶¨Ïûê Ï†ÑÏö© ÏÑπÏÖò]
with st.expander("üîê Í¥ÄÎ¶¨Ïûê Ï†ëÏÜç (Admin Access)"):
    if not st.session_state["is_admin"]:
        if st.text_input("Admin Password", type="password") == ADMIN_PASSWORD:
            st.session_state["is_admin"] = True
            st.rerun()
    else:
        st.success("Í¥ÄÎ¶¨Ïûê Í∂åÌïú ÌôúÏÑ±ÌôîÎê®")
        st.subheader("üõ†Ô∏è ÏãúÏä§ÌÖú ÌÜµÏ†úÏã§")
        st.write("**ü§ñ ÌòÑÏû¨ Í∞ÄÏö© Î™®Îç∏ Î¶¨Ïä§Ìä∏:**")
        st.code(", ".join(get_all_available_models(GOOGLE_API_KEY_A)))
        
        if st.session_state["debug_logs"]:
            st.write("**üìú Ïã§ÏãúÍ∞Ñ ÎîîÎ≤ÑÍ∑∏ Î°úÍ∑∏:**")
            st.text_area("Logs", "\n".join(st.session_state["debug_logs"]), height=250)
            
        if st.button("Î°úÍ∑∏ÏïÑÏõÉ"):
            st.session_state["is_admin"] = False
            st.rerun()
