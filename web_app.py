import streamlit as st
from supabase import create_client, Client
import re
import requests
import time
import random
import math
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from datetime import datetime
from collections import Counter
import yt_dlp
import pandas as pd
import altair as alt
import json
from bs4 import BeautifulSoup

# --- [1. ì‹œìŠ¤í…œ ì„¤ì •] ---
st.set_page_config(page_title="Fact-Check Center v90.0 (Real Survivor)", layout="wide", page_icon="ğŸ§¬")

if "is_admin" not in st.session_state:
    st.session_state["is_admin"] = False

# ğŸŒŸ Secrets ë¡œë“œ
try:
    YOUTUBE_API_KEY = st.secrets["YOUTUBE_API_KEY"]
    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    ADMIN_PASSWORD = st.secrets["ADMIN_PASSWORD"]
    GOOGLE_API_KEY_A = st.secrets["GOOGLE_API_KEY_A"]
    GOOGLE_API_KEY_B = st.secrets["GOOGLE_API_KEY_B"]
except:
    st.error("âŒ í•„ìˆ˜ í‚¤(API Keys)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

@st.cache_resource
def init_supabase():
    return create_client(SUPABASE_URL, SUPABASE_KEY)

supabase = init_supabase()

# --- [2. ëª¨ë¸ ìë™ íƒìƒ‰ê¸° (Auto-Discovery)] ---
@st.cache_data(ttl=3600) # 1ì‹œê°„ë§ˆë‹¤ ê°±ì‹ 
def get_all_available_models(api_key):
    genai.configure(api_key=api_key)
    models = []
    try:
        # êµ¬ê¸€ APIì— ë“±ë¡ëœ ëª¨ë“  ëª¨ë¸ì„ ê¸ì–´ì˜µë‹ˆë‹¤.
        for m in genai.list_models():
            if 'generateContent' in m.supported_generation_methods:
                # v1beta ëª¨ë¸ ë“± ì´ë¦„ ì •ë¦¬
                model_name = m.name.replace("models/", "")
                models.append(model_name)
    except Exception as e:
        # API í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ë¹„ìƒìš© í•˜ë“œì½”ë”© ë¦¬ìŠ¤íŠ¸
        return ["gemini-2.5-flash-lite", "gemini-flash-lite-latest", "gemini-2.0-flash", "gemini-1.5-flash"]
    
    # [ìš°ì„ ìˆœìœ„ ì •ë ¬] Lite > Flash > Pro ìˆœì„œë¡œ ì •ë ¬ (ì†ë„ ë° ì¿¼í„° ìµœì í™”)
    # 1. Liteê°€ ë“¤ì–´ê°„ ê²ƒ ìš°ì„ 
    # 2. ê·¸ ë‹¤ìŒ Flashê°€ ë“¤ì–´ê°„ ê²ƒ
    # 3. ë‚˜ë¨¸ì§€ëŠ” ë’¤ë¡œ
    def sort_key(name):
        if 'lite' in name: return 0
        if 'flash' in name: return 1
        return 2
    
    models.sort(key=sort_key)
    return models

# --- [3. ìƒìˆ˜ ì •ì˜] ---
WEIGHT_ALGO = 0.6
WEIGHT_AI = 0.4

VITAL_KEYWORDS = ['ìœ„ë…', 'ì‚¬ë§', 'ë³„ì„¸', 'êµ¬ì†', 'ì²´í¬', 'ê¸°ì†Œ', 'ì‹¤í˜•', 'ì‘ê¸‰ì‹¤', 'ì´í˜¼', 'ë¶ˆí™”', 'íŒŒê²½', 'ì¶©ê²©', 'ê²½ì•…', 'ì†ë³´', 'ê¸´ê¸‰', 'í­ë¡œ', 'ì–‘ì„±', 'í™•ì§„', 'ì‹¬ì •ì§€', 'ë‡Œì‚¬', 'ì¤‘íƒœ', 'ì••ìˆ˜ìˆ˜ìƒ‰', 'ì†Œí™˜', 'í‡´ì§„', 'íƒ„í•µ', 'ë‚´ë€', 'ê°„ì²©']
CRITICAL_STATE_KEYWORDS = ['ë³„ê±°', 'ì´í˜¼', 'íŒŒê²½', 'ì‚¬ë§', 'ìœ„ë…', 'êµ¬ì†', 'ì²´í¬', 'ì‹¤í˜•', 'ë¶ˆí™”', 'í­ë¡œ', 'ì¶©ê²©', 'ë…¼ë€', 'ì¤‘íƒœ', 'ì‹¬ì •ì§€', 'ë‡Œì‚¬', 'ì••ìˆ˜ìˆ˜ìƒ‰', 'ì†Œí™˜', 'íŒŒì‚°', 'ë¹šë”ë¯¸', 'ì „ê³¼', 'ê°ì˜¥', 'ê°„ì²©']
OFFICIAL_CHANNELS = ['MBC', 'KBS', 'SBS', 'EBS', 'YTN', 'JTBC', 'TVCHOSUN', 'MBN', 'CHANNEL A', 'OBS', 'ì±„ë„A', 'TVì¡°ì„ ', 'ì—°í•©ë‰´ìŠ¤', 'YONHAP', 'í•œê²¨ë ˆ', 'ê²½í–¥', 'ì¡°ì„ ', 'ì¤‘ì•™', 'ë™ì•„']

STATIC_TRUTH_CORPUS = ["ë°•ë‚˜ë˜ ìœ„ì¥ì „ì… ë¬´í˜ì˜", "ì„ì˜ì›… ì•”í‘œ ëŒ€ì‘", "ì •í¬ì› ì €ì†ë…¸í™”", "ëŒ€ì „ ì¶©ë‚¨ í†µí•©", "ì„ ê±° ì¶œë§ˆ ì„ ì–¸"]
STATIC_FAKE_CORPUS = ["ì¶©ê²© í­ë¡œ ê²½ì•…", "ê¸´ê¸‰ ì†ë³´ ì†Œë¦„", "ì¶©ê²© ë°œì–¸ ë…¼ë€", "êµ¬ì† ì˜ì¥ ë°œë¶€", "ì˜ìƒ ìœ ì¶œ", "ê³„ì‹œ ì˜ˆì–¸", "ì‚¬í˜• ì§‘í–‰", "ìœ„ë…ì„¤"]

# --- [4. VectorEngine] ---
class VectorEngine:
    def __init__(self):
        self.vocab = set()
        self.truth_vectors = []
        self.fake_vectors = []
    def tokenize(self, text): return re.findall(r'[ê°€-í£]{2,}', text)
    def train(self, truth, fake):
        for t in truth + fake: self.vocab.update(self.tokenize(t))
        self.vocab = sorted(list(self.vocab))
        self.truth_vectors = [self.text_to_vector(t) for t in truth]
        self.fake_vectors = [self.text_to_vector(t) for t in fake]
    def text_to_vector(self, text, vocabulary=None):
        target_vocab = vocabulary if vocabulary else self.vocab
        c = Counter(self.tokenize(text))
        return [c[w] for w in target_vocab]
    def cosine_similarity(self, v1, v2):
        dot = sum(a*b for a,b in zip(v1,v2))
        mag = math.sqrt(sum(a*a for a in v1)) * math.sqrt(sum(b*b for b in v2))
        return dot/mag if mag>0 else 0
    def analyze_position(self, query):
        qv = self.text_to_vector(query)
        mt = max([self.cosine_similarity(qv, v) for v in self.truth_vectors] or [0])
        mf = max([self.cosine_similarity(qv, v) for v in self.fake_vectors] or [0])
        return mt, mf
    def compute_content_similarity(self, text1, text2):
        tokens1 = self.tokenize(text1); tokens2 = self.tokenize(text2)
        local_vocab = sorted(list(set(tokens1 + tokens2)))
        if not local_vocab: return 0.0
        v1 = self.text_to_vector(text1, local_vocab)
        v2 = self.text_to_vector(text2, local_vocab)
        return self.cosine_similarity(v1, v2)

vector_engine = VectorEngine()

# --- [5. Gemini Logic (The Real Survivor)] ---

# ğŸš¨ ì•ˆì „ ì„¤ì •
safety_settings_none = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
}

# âš”ï¸ [í•µì‹¬] ë¬´ì ì˜ ìƒì¡´ì í˜¸ì¶œ í•¨ìˆ˜
def call_gemini_survivor(api_key, prompt, is_json=False):
    genai.configure(api_key=api_key)
    generation_config = {"response_mime_type": "application/json"} if is_json else {}
    
    # 1. í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë“  ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    all_models = get_all_available_models(api_key)
    
    logs = []
    
    # 2. ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° ì‚´ì•„ìˆëŠ” ëª¨ë¸ì„ ì°¾ìŠµë‹ˆë‹¤.
    for model_name in all_models:
        try:
            model = genai.GenerativeModel(model_name, generation_config=generation_config)
            # ì•„ì£¼ ì§§ì€ íƒ€ì„ì•„ì›ƒì€ ë‘ì§€ ì•ŠìŒ (ìƒì„± ì‹œê°„ í•„ìš”)
            response = model.generate_content(prompt, safety_settings=safety_settings_none)
            
            if response.text:
                # ì„±ê³µí•˜ë©´ ë¡œê·¸ì— ë‚¨ê¸°ê³  ë¦¬í„´
                return response.text, model_name, logs
                
        except Exception as e:
            # ì‹¤íŒ¨í•˜ë©´ ë¡œê·¸ì— ë‚¨ê¸°ê³  ë‹¤ìŒ ëª¨ë¸ë¡œ ë„˜ì–´ê°
            logs.append(f"âŒ {model_name}: {str(e)[:50]}...")
            time.sleep(0.2) # ê³¼ë¶€í•˜ ë°©ì§€ìš© ë¯¸ì„¸ ë”œë ˆì´
            continue
            
    # 3. ëª¨ë“  ëª¨ë¸ì´ ë‹¤ ì£½ì—ˆì„ ë•Œ (ìµœì•…ì˜ ê²½ìš°)
    return None, "All Failed", logs

# [Engine A] ìˆ˜ì‚¬ê´€: í‚¤ì›Œë“œ ì¶”ì¶œ
def get_gemini_search_keywords(title, transcript):
    # ìë§‰ì´ ë„ˆë¬´ ê¸¸ë©´ ëª¨ë¸ì´ í˜ë“¤ì–´í•˜ë¯€ë¡œ ì•ë¶€ë¶„ë§Œ (ê·¸ëŸ¬ë‚˜ ì¶©ë¶„íˆ ê¸¸ê²Œ)
    context_data = transcript[:15000] 
    
    prompt = f"""
    You are a Fact-Check Investigator.
    
    [Input]
    Title: {title}
    Transcript (Partial): {context_data}
    
    [Task]
    Extract ONE specific search query for Google News.
    
    [Rules]
    1. **IGNORE** generic terms (Vlog, Mukbang, Daily life).
    2. **FOCUS** on specific Proper Nouns (Person's Name, Drug Name, Company Name, Crime Type).
    3. If the video mentions a specific scandal or death, include those keywords.
    4. **Output:** ONLY the Korean search query string (2-4 words).
    """
    
    result_text, model_used, logs = call_gemini_survivor(GOOGLE_API_KEY_A, prompt)
    
    # ì‚¬ì´ë“œë°”ì— ë¡œê·¸ ì¶œë ¥
    with st.sidebar.expander(f"ğŸ•µï¸ Key A (Investigator) Logs", expanded=False):
        for log in logs: st.write(log)
        if result_text: st.success(f"âœ… Used: {model_used}")
        else: st.error("âŒ All models failed")

    return (result_text.strip(), f"âœ¨ {model_used}") if result_text else (title, "âŒ Error")

# [í¬ë¡¤ëŸ¬] ë‰´ìŠ¤ ë³¸ë¬¸ ìˆ˜ì§‘
def scrape_news_content_robust(google_url):
    try:
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        response = session.get(google_url, timeout=5, allow_redirects=True)
        final_url = response.url
        
        soup = BeautifulSoup(response.text, 'html.parser')
        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'iframe']):
            tag.decompose()
            
        paragraphs = soup.find_all('p')
        clean_text = []
        for p in paragraphs:
            text = p.get_text().strip()
            if len(text) > 30: clean_text.append(text)
        
        full_text = " ".join(clean_text)
        if len(full_text) < 100: return None, final_url
        return full_text[:4000], final_url
    except:
        return None, google_url

# [Engine B] íŒì‚¬: ë‰´ìŠ¤ ì •ë°€ ëŒ€ì¡°
def deep_verify_news(video_summary, news_url, news_snippet):
    scraped_text, real_url = scrape_news_content_robust(news_url)
    evidence_text = scraped_text if scraped_text else news_snippet
    source_type = "Full Article" if scraped_text else "Snippet Only"
    
    prompt = f"""
    Compare Video Summary vs News Evidence.
    
    [Video Summary]
    {video_summary[:2000]}
    
    [News Evidence ({source_type})]
    {evidence_text}
    
    [Task]
    Does the news confirm the video's claim?
    - Match: Score 90-100.
    - Related: Score 40-60.
    - Mismatch: Score 0-10.
    
    [Output JSON]
    {{ "score": <int>, "reason": "<short korean reason>" }}
    """
    
    result_text, model_used, logs = call_gemini_survivor(GOOGLE_API_KEY_B, prompt, is_json=True)
    
    try:
        res = json.loads(result_text)
        return res['score'], res['reason'], source_type, evidence_text, real_url
    except:
        return 0, "Analysis Failed", "Error", "", news_url

# [Engine B] ìµœì¢… íŒê²°
def get_gemini_verdict_final(title, transcript, verified_news_list):
    news_summary = ""
    for item in verified_news_list:
        news_summary += f"- News: {item['ë‰´ìŠ¤ ì œëª©']} (Score: {item['ìµœì¢… ì ìˆ˜']}, Reason: {item['ë¶„ì„ ê·¼ê±°']})\n"
    
    full_context = transcript[:30000]
    prompt = f"""
    You are a professional Fact-Check AI Judge.
    
    [Video Info]
    Title: {title}
    Transcript Summary: {full_context[:2000]}...
    
    [Cross-Checked Evidence]
    {news_summary}
    
    [Instruction]
    1. Verify truthfulness based on evidence.
    2. Reliable match -> Truth (0-30).
    3. Mismatch/No evidence -> Fake (70-100).
    4. MUST OUTPUT REASON IN KOREAN.
    
    [Output Format - JSON Only]
    {{ "score": <int>, "reason": "<í•œê¸€ íŒê²°ë¬¸>" }}
    """
    
    result_text, model_used, logs = call_gemini_survivor(GOOGLE_API_KEY_B, prompt, is_json=True)
    
    # ì‚¬ì´ë“œë°”ì— ë¡œê·¸ ì¶œë ¥
    with st.sidebar.expander(f"âš–ï¸ Key B (Judge) Logs", expanded=False):
        for log in logs: st.write(log)
        if result_text: st.success(f"âœ… Used: {model_used}")
        
    if result_text:
        try:
            data = json.loads(result_text)
            return data['score'], f"{data['reason']} (By {model_used})"
        except: return 50, f"JSON Error"
    return 50, f"Judge Failed"

# --- [6. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜] ---
def normalize_korean_word(word):
    word = re.sub(r'[^ê°€-í£0-9]', '', word)
    for j in ['ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ì˜','ì—','ì—ê²Œ','ë¡œ','ìœ¼ë¡œ']:
        if word.endswith(j): return word[:-len(j)]
    return word

def extract_meaningful_tokens(text):
    raw = re.findall(r'[ê°€-í£]{2,}', text)
    noise = ['ì¶©ê²©','ì†ë³´','ê¸´ê¸‰','ì˜¤ëŠ˜','ì§€ê¸ˆ','ê²°êµ­','ë‰´ìŠ¤','ì˜ìƒ']
    return [normalize_korean_word(w) for w in raw if w not in noise]

def train_dynamic_vector_engine():
    try:
        res_t = supabase.table("analysis_history").select("video_title").lt("fake_prob", 40).execute()
        res_f = supabase.table("analysis_history").select("video_title").gt("fake_prob", 60).execute()
        
        dt = [row['video_title'] for row in res_t.data] if res_t.data else []
        df = [row['video_title'] for row in res_f.data] if res_f.data else []
        
        vector_engine.train(STATIC_TRUTH_CORPUS + dt, STATIC_FAKE_CORPUS + df)
        return len(dt) + len(df), len(dt), len(df)
    except: 
        vector_engine.train(STATIC_TRUTH_CORPUS, STATIC_FAKE_CORPUS)
        return 0, 0, 0

def save_analysis(channel, title, prob, url, keywords):
    try: supabase.table("analysis_history").insert({"channel_name": channel, "video_title": title, "fake_prob": prob, "analysis_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'), "video_url": url, "keywords": keywords}).execute()
    except: pass

def render_intelligence_distribution(current_prob):
    try:
        res = supabase.table("analysis_history").select("fake_prob").execute()
        if not res.data: return
        df = pd.DataFrame(res.data)
        base = alt.Chart(df).transform_density('fake_prob', as_=['fake_prob', 'density'], extent=[0, 100], bandwidth=5).mark_area(opacity=0.3, color='#888').encode(x=alt.X('fake_prob:Q', title='ê°€ì§œë‰´ìŠ¤ í™•ë¥  ë¶„í¬'), y=alt.Y('density:Q', title='ë°ì´í„° ë°€ë„'))
        rule = alt.Chart(pd.DataFrame({'x': [current_prob]})).mark_rule(color='blue', size=3).encode(x='x')
        st.altair_chart(base + rule, use_container_width=True)
    except: pass

def colored_progress_bar(label, percent, color):
    st.markdown(f"""<div style="margin-bottom: 10px;"><div style="display: flex; justify-content: space-between; margin-bottom: 3px;"><span style="font-size: 13px; font-weight: 600; color: #555;">{label}</span><span style="font-size: 13px; font-weight: 700; color: {color};">{round(percent * 100, 1)}%</span></div><div style="background-color: #eee; border-radius: 5px; height: 8px; width: 100%;"><div style="background-color: {color}; height: 8px; width: {percent * 100}%; border-radius: 5px;"></div></div></div>""", unsafe_allow_html=True)

def render_score_breakdown(data_list):
    style = """<style>table.score-table { width: 100%; border-collapse: separate; border-spacing: 0; border: 1px solid #e0e0e0; border-radius: 8px; overflow: hidden; font-family: sans-serif; font-size: 14px; margin-top: 10px;} table.score-table th { background-color: #f8f9fa; color: #495057; font-weight: bold; padding: 12px 15px; text-align: left; border-bottom: 1px solid #e0e0e0; } table.score-table td { padding: 12px 15px; border-bottom: 1px solid #f0f0f0; color: #333; } table.score-table tr:last-child td { border-bottom: none; } .badge { padding: 4px 8px; border-radius: 6px; font-weight: 700; font-size: 11px; display: inline-block; text-align: center; min-width: 45px; } .badge-danger { background-color: #ffebee; color: #d32f2f; } .badge-success { background-color: #e8f5e9; color: #2e7d32; } .badge-neutral { background-color: #f5f5f5; color: #757575; border: 1px solid #e0e0e0; }</style>"""
    rows = ""
    for item, score, note in data_list:
        try:
            score_num = int(score)
            badge = f'<span class="badge badge-danger">+{score_num}</span>' if score_num > 0 else f'<span class="badge badge-success">{score_num}</span>' if score_num < 0 else f'<span class="badge badge-neutral">0</span>'
        except: badge = f'<span class="badge badge-neutral">{score}</span>'
        rows += f"<tr><td>{item}<br><span style='color:#888; font-size:11px;'>{note}</span></td><td style='text-align: right;'>{badge}</td></tr>"
    st.markdown(f"{style}<table class='score-table'><thead><tr><th>ë¶„ì„ í•­ëª© (Score Breakdown)</th><th style='text-align: right;'>ë³€ë™</th></tr></thead><tbody>{rows}</tbody></table>", unsafe_allow_html=True)

def summarize_transcript(text, title, max_sentences=3):
    if not text or len(text) < 50: return "âš ï¸ ìš”ì•½í•  ìë§‰ ë‚´ìš©ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    clean_text = re.sub(r'http\S+|#EXTM3U|#EXT-X-VERSION:3', '', text)
    clean_text = re.sub(r'\[.*?\]|[>]+', '', clean_text)
    sentences = re.split(r'(?<=[.?!])\s+', clean_text)
    if len(sentences) <= 3: return clean_text.strip()
    title_nouns = set(extract_meaningful_tokens(title))
    scored_sentences = []
    for i, sent in enumerate(sentences):
        if len(sent) < 15: continue
        score = 0
        sent_tokens = extract_meaningful_tokens(sent)
        score += len(sent_tokens)
        for n in sent_tokens:
            if n in title_nouns: score += 10
        if i < len(sentences) * 0.2: score += 3
        elif i > len(sentences) * 0.8: score += 2
        scored_sentences.append((i, sent, score))
    top_sentences = sorted(scored_sentences, key=lambda x:x[2], reverse=True)[:max_sentences]
    top_sentences.sort(key=lambda x:x[0])
    return " ".join([s[1] for s in top_sentences])

def clean_html_regex(text):
    if not text: return ""
    return re.sub('<.*?>', '', text).strip()

def detect_ai_content(info):
    is_ai, reasons = False, []
    text = (info.get('title', '') + " " + info.get('description', '') + " " + " ".join(info.get('tags', []))).lower()
    for kw in ['ai', 'artificial intelligence', 'chatgpt', 'deepfake', 'synthetic', 'ì¸ê³µì§€ëŠ¥', 'ë”¥í˜ì´í¬', 'ê°€ìƒì¸ê°„']:
        if kw in text: is_ai = True; reasons.append(f"í‚¤ì›Œë“œ ê°ì§€: {kw}"); break
    return is_ai, ", ".join(reasons)

def check_is_official(channel_name):
    norm_name = channel_name.upper().replace(" ", "")
    return any(o in norm_name for o in OFFICIAL_CHANNELS)

def count_sensational_words(text):
    return sum(text.count(w) for w in ['ì¶©ê²©', 'ê²½ì•…', 'ì‹¤ì²´', 'í­ë¡œ', 'ë‚œë¦¬', 'ì†ë³´', 'ê¸´ê¸‰', 'ì†Œë¦„', 'ã„·ã„·', 'ì§„ì§œ', 'ê²°êµ­', 'ê³„ì‹œ', 'ì˜ˆì–¸', 'ìœ„ë…', 'ì‚¬ë§', 'ì¤‘íƒœ'])

def check_tag_abuse(title, hashtags, channel_name):
    if check_is_official(channel_name): return 0, "ê³µì‹ ì±„ë„ ë©´ì œ"
    if not hashtags: return 0, "í•´ì‹œíƒœê·¸ ì—†ìŒ"
    tn = set(extract_meaningful_tokens(title)); tgn = set(h.replace("#", "").split(":")[-1].strip() for h in hashtags)
    if len(tgn) < 2: return 0, "ì–‘í˜¸"
    return (20, "ğŸš¨ ì‹¬ê° (ë¶ˆì¼ì¹˜)") if not tn.intersection(tgn) else (0, "ì–‘í˜¸")

def fetch_real_transcript(info_dict):
    try:
        url = None
        for key in ['subtitles', 'automatic_captions']:
            if key in info_dict and 'ko' in info_dict[key]:
                for fmt in info_dict[key]['ko']:
                    if fmt['ext'] == 'vtt': url = fmt['url']; break
            if url: break
        if url:
            res = requests.get(url)
            if res.status_code == 200:
                content = res.text
                if "#EXTM3U" in content: return None, "ìë§‰ í¬ë§· ì˜¤ë¥˜"
                clean = []
                for line in content.splitlines():
                    if '-->' not in line and 'WEBVTT' not in line and line.strip():
                        t = re.sub(r'<[^>]+>', '', line).strip()
                        if t and t not in clean: clean.append(t)
                full_text = " ".join(clean)
                return full_text, f"âœ… ì „ì²´ ìë§‰ ìˆ˜ì§‘ ì™„ë£Œ (ì´ {len(full_text):,}ì)"
    except: pass
    return None, "ìë§‰ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"

def fetch_comments_via_api(video_id):
    try:
        url = "https://www.googleapis.com/youtube/v3/commentThreads"
        res = requests.get(url, params={'part': 'snippet', 'videoId': video_id, 'key': YOUTUBE_API_KEY, 'maxResults': 50, 'order': 'relevance'})
        if res.status_code == 200:
            items = [i['snippet']['topLevelComment']['snippet']['textDisplay'] for i in res.json().get('items', [])]
            return items, f"âœ… API ìˆ˜ì§‘ ì„±ê³µ (Top {len(items)})"
    except: pass
    return [], "âŒ API í†µì‹  ì‹¤íŒ¨"

def fetch_news_regex(query):
    news_res = []
    try:
        rss = f"https://news.google.com/rss/search?q={requests.utils.quote(query)}&hl=ko&gl=KR"
        raw = requests.get(rss, timeout=5).text
        items = re.findall(r'<item>(.*?)</item>', raw, re.DOTALL)
        for item in items[:10]:
            t = re.search(r'<title>(.*?)</title>', item)
            d = re.search(r'<description>(.*?)</description>', item)
            l = re.search(r'<link>(.*?)</link>', item)
            nt = t.group(1).replace("<![CDATA[", "").replace("]]>", "") if t else "ì œëª© ì—†ìŒ"
            nd = clean_html_regex(d.group(1).replace("<![CDATA[", "").replace("]]>", "")) if d else "ë‚´ìš© ì—†ìŒ"
            nl = l.group(1).strip() if l else ""
            news_res.append({'title': nt, 'desc': nd, 'link': nl})
    except: pass
    return news_res

def extract_top_keywords_from_transcript(text, top_n=5):
    if not text: return []
    tokens = extract_meaningful_tokens(text)
    return Counter(tokens).most_common(top_n)

def analyze_comment_relevance(comments, context_text):
    if not comments: return [], 0, "ë¶„ì„ ë¶ˆê°€"
    cn = extract_meaningful_tokens(" ".join(comments))
    top = Counter(cn).most_common(5)
    ctx = set(extract_meaningful_tokens(context_text))
    match = sum(1 for w,c in top if w in ctx)
    score = int(match/len(top)*100) if top else 0
    msg = "âœ… ì£¼ì œ ì§‘ì¤‘" if score >= 60 else "âš ï¸ ì¼ë¶€ ê´€ë ¨" if score >= 20 else "âŒ ë¬´ê´€"
    return [f"{w}({c})" for w, c in top], score, msg

def check_red_flags(comments):
    detected = [k for c in comments for k in ['ê°€ì§œë‰´ìŠ¤', 'ì£¼ì‘', 'ì‚¬ê¸°', 'ê±°ì§“ë§', 'í—ˆìœ„', 'ì„ ë™'] if k in c]
    return len(detected), list(set(detected))

def witty_loading_sequence(total, t_cnt, f_cnt):
    messages = [f"ğŸ§  [Intelligence: {total}] ì§‘ë‹¨ ì§€ì„± ë¡œë“œ ì¤‘...", f"ğŸ”‘ Twin-Gemini Protocol í™œì„±í™”...", "ğŸš€ ìˆ˜ì‚¬ê´€(Investigator) ë° íŒì‚¬(Judge) ì—”ì§„ ê°€ë™"]
    with st.status("ğŸ•µï¸ Dual-Engine Fact-Check v90.0...", expanded=True) as status:
        for msg in messages: st.write(msg); time.sleep(0.3)
        status.update(label="ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ", state="complete", expanded=False)

def run_forensic_main(url):
    db_count, db_truth, db_fake = train_dynamic_vector_engine()
    witty_loading_sequence(db_count, 0, 0)
    
    vid = re.search(r'(?:v=|\/)([0-9A-Za-z_-]{11}).*', url)
    if vid: vid = vid.group(1)

    with yt_dlp.YoutubeDL({'quiet': True, 'skip_download': True}) as ydl:
        try:
            info = ydl.extract_info(url, download=False)
            title = info.get('title', ''); uploader = info.get('uploader', '')
            tags = info.get('tags', []); desc = info.get('description', '')
            
            # ì‚¬ì´ë“œë°” ì´ˆê¸°í™”
            st.sidebar.title("ğŸ¤– AI Model Status")
            avail_models = get_all_available_models(GOOGLE_API_KEY_A)
            st.sidebar.success(f"Detected {len(avail_models)} Active Models")
            with st.sidebar.expander("Show Available Models"):
                st.write(avail_models)
                
            trans, t_status = fetch_real_transcript(info)
            full_text = trans if trans else desc
            summary = summarize_transcript(full_text, title)
            top_transcript_keywords = extract_top_keywords_from_transcript(full_text)
            
            query, source = get_gemini_search_keywords(title, full_text)

            is_official = check_is_official(uploader)
            is_ai, ai_msg = detect_ai_content(info)
            hashtag_display = ", ".join([f"#{t}" for t in tags]) if tags else "í•´ì‹œíƒœê·¸ ì—†ìŒ"
            abuse_score, abuse_msg = check_tag_abuse(title, tags, uploader)
            agitation = count_sensational_words(full_text + title)
            
            ts, fs = vector_engine.analyze_position(query + " " + title)
            t_impact = int(ts * 30) * -1; f_impact = int(fs * 30)

            news_items = fetch_news_regex(query)
            news_ev = []; max_match = 0
            
            for idx, item in enumerate(news_items[:3]):
                ai_s, ai_r, source_type, evidence_text, real_url = deep_verify_news(summary, item['link'], item['desc'])
                if ai_s > max_match: max_match = ai_s
                
                status_icon = "ğŸŸ¢" if ai_s >= 70 else "ğŸ”´" if ai_s < 30 else "ğŸŸ¡"
                news_ev.append({
                    "ë‰´ìŠ¤ ì œëª©": item['title'],
                    "ì¼ì¹˜ë„": f"{status_icon} {ai_s}%",
                    "ìµœì¢… ì ìˆ˜": f"{ai_s}%",
                    "ë¶„ì„ ê·¼ê±°": ai_r,
                    "ë¹„ê³ ": f"[{source_type}] {len(evidence_text)}ì ë¶„ì„",
                    "ì›ë¬¸": real_url
                })
            
            if not news_ev: news_score = 0
            else:
                if max_match >= 70: news_score = -30 
                elif max_match >= 50: news_score = -10
                else: news_score = 10 

            cmts, c_status = fetch_comments_via_api(vid)
            top_kw, rel_score, rel_msg = analyze_comment_relevance(cmts, title + " " + full_text)
            red_cnt, red_list = check_red_flags(cmts)
            
            silent_penalty = 0; is_silent = (len(news_ev) == 0)
            if is_silent:
                if any(k in title for k in CRITICAL_STATE_KEYWORDS): silent_penalty = 10
                elif agitation >= 3: silent_penalty = 20
            
            if is_official: news_score = -50; silent_penalty = 0
            
            sent_score = 0
            if cmts and red_cnt == 0:
                neg = sum(1 for c in cmts for k in ['ê°€ì§œ','ì„ ë™'] if k in c) / len(cmts)
                sent_score = int(neg * 10)
            
            clickbait = 10 if any(w in title for w in ['ì¶©ê²©','ê²½ì•…','í­ë¡œ']) else -5
            
            algo_base_score = 50 + t_impact + f_impact + news_score + sent_score + clickbait + abuse_score + silent_penalty
            
            ai_judge_score, ai_judge_reason = get_gemini_verdict_final(title, full_text, news_ev)
            
            if t_impact == 0 and f_impact == 0 and is_silent:
                ai_judge_score = int((ai_judge_score + 50) / 2)
            
            final_prob = int((algo_base_score * WEIGHT_ALGO) + (ai_judge_score * WEIGHT_AI))
            final_prob = max(1, min(99, final_prob))
            
            save_analysis(uploader, title, final_prob, url, query)

            st.subheader("ğŸ•µï¸ Dual-Engine Analysis Result")
            col_a, col_b, col_c = st.columns(3)
            with col_a: 
                st.metric("ìµœì¢… ê°€ì§œë‰´ìŠ¤ í™•ë¥ ", f"{final_prob}%", delta=f"AI Judge: {ai_judge_score}pt")
            with col_b:
                icon = "ğŸŸ¢" if final_prob < 30 else "ğŸ”´" if final_prob > 60 else "ğŸŸ "
                verdict = "ì•ˆì „ (Verified)" if final_prob < 30 else "ìœ„í—˜ (Fake/Bias)" if final_prob > 60 else "ì£¼ì˜ (Caution)"
                st.metric("ì¢…í•© AI íŒì •", f"{icon} {verdict}")
            with col_c: 
                st.metric("AI Intelligence Level", f"{db_count} Nodes", delta="Twin-Engine Active")
            
            st.divider()
            st.subheader("ğŸ§  Intelligence Map")
            render_intelligence_distribution(final_prob)

            if is_ai: st.warning(f"ğŸ¤– **AI ìƒì„± ì½˜í…ì¸  ê°ì§€ë¨**: {ai_msg}")
            if is_official: st.success(f"ğŸ›¡ï¸ **ê³µì‹ ì–¸ë¡ ì‚¬ ì±„ë„({uploader})ì…ë‹ˆë‹¤.**")

            st.divider()
            col1, col2 = st.columns([1, 1.4])
            with col1:
                st.write("**[ì˜ìƒ ìƒì„¸ ì •ë³´]**")
                st.table(pd.DataFrame({"í•­ëª©": ["ì˜ìƒ ì œëª©", "ì±„ë„ëª…", "ì¡°íšŒìˆ˜", "í•´ì‹œíƒœê·¸"], "ë‚´ìš©": [title, uploader, f"{info.get('view_count',0):,}íšŒ", hashtag_display]}))
                st.info(f"ğŸ¯ **Investigator (Key A) ì¶”ì¶œ ê²€ìƒ‰ì–´**: {query}")
                with st.container(border=True):
                    st.markdown("ğŸ“ **ì˜ìƒ ë‚´ìš© ìš”ì•½**")
                    st.write(summary)
                
                st.write("**[Score Breakdown]**")
                render_score_breakdown([
                    ["ğŸ ê¸°ë³¸ ì¤‘ë¦½ ì ìˆ˜ (Base Score)", 50, "ëª¨ë“  ë¶„ì„ì€ 50ì (ì¤‘ë¦½)ì—ì„œ ì‹œì‘"],
                    ["ì§„ì‹¤ ë°ì´í„° ë§¥ë½", t_impact, "ë‚´ë¶€ DB ì§„ì‹¤ ë°ì´í„°ì™€ ìœ ì‚¬ì„±"],
                    ["ê°€ì§œ íŒ¨í„´ ë§¥ë½", f_impact, "ë‚´ë¶€ DB ê°€ì§œ ë°ì´í„°ì™€ ìœ ì‚¬ì„±"],
                    ["ë‰´ìŠ¤ ë§¤ì¹­ ìƒíƒœ", news_score, "Deep-Crawler ì •ë°€ ëŒ€ì¡° ê²°ê³¼"],
                    ["ì—¬ë¡ /ì œëª©/íƒœê·¸ ê°€ê°", sent_score + clickbait + abuse_score, ""],
                    ["-----------------", "", ""],
                    ["âš–ï¸ AI Judge Score (40%)", ai_judge_score, "Gemini ì¢…í•© ì¶”ë¡ "]
                ])

            with col2:
                st.subheader("ğŸ“Š 5ëŒ€ ì •ë°€ ë¶„ì„ ì¦ê±°")
                
                st.markdown("**[ì¦ê±° 0] Semantic Vector Space (Internal DB)**")
                colored_progress_bar("âœ… ì§„ì‹¤ ì˜ì—­ ê·¼ì ‘ë„", ts, "#2ecc71")
                colored_progress_bar("ğŸš¨ ê±°ì§“ ì˜ì—­ ê·¼ì ‘ë„", fs, "#e74c3c")
                st.write("---")

                st.markdown(f"**[ì¦ê±° 1] ë‰´ìŠ¤ êµì°¨ ëŒ€ì¡° (Deep-Web Crawler)**")
                if news_ev:
                    st.dataframe(
                        pd.DataFrame(news_ev),
                        column_config={
                            "ì›ë¬¸": st.column_config.LinkColumn(label="ë§í¬", display_text="ğŸ”— ì´ë™")
                        },
                        use_container_width=True,
                        hide_index=True
                    )
                    with st.expander("ğŸ” í¬ë¡¤ë§ëœ ë‰´ìŠ¤ ë³¸ë¬¸ ìƒ˜í”Œ ë³´ê¸°"):
                        for n in news_ev:
                            st.caption(f"**{n['ë‰´ìŠ¤ ì œëª©']}**: {n['ë¹„ê³ ']}")
                else: st.warning("ğŸ” ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (Silent Echo Risk)")
                    
                st.markdown("**[ì¦ê±° 2] ì‹œì²­ì ì—¬ë¡  ì‹¬ì¸µ ë¶„ì„**")
                if cmts: st.table(pd.DataFrame([["ìµœë‹¤ ë¹ˆì¶œ í‚¤ì›Œë“œ", ", ".join(top_kw)], ["ë…¼ë€ ê°ì§€ ì—¬ë¶€", f"{red_cnt}íšŒ"], ["ì£¼ì œ ì¼ì¹˜ë„", f"{rel_score}% ({rel_msg})"]], columns=["í•­ëª©", "ë‚´ìš©"]))
                
                st.markdown("**[ì¦ê±° 3] ìë§‰ ì„¸ë§Œí‹± ì‹¬ì¸µ ëŒ€ì¡°**")
                top_kw_str = ", ".join([f"{w}({c})" for w, c in top_transcript_keywords])
                st.table(pd.DataFrame([["ì˜ìƒ ìµœë‹¤ ì–¸ê¸‰ í‚¤ì›Œë“œ", top_kw_str], ["ì œëª© ë‚šì‹œì–´", "ìˆìŒ" if clickbait > 0 else "ì—†ìŒ"], ["ì„ ë™ì„± ì§€ìˆ˜", f"{agitation}íšŒ"]], columns=["ë¶„ì„ í•­ëª©", "íŒì • ê²°ê³¼"]))
                
                st.markdown("**[ì¦ê±° 4] AI ìµœì¢… ë¶„ì„ íŒë‹¨ (Judge Verdict)**")
                with st.container(border=True):
                    st.write(f"âš–ï¸ **íŒê²°:** {ai_judge_reason}")
                    st.caption(f"* Gemini ë…ë¦½ ì¶”ë¡  ì ìˆ˜: {ai_judge_score}ì  (Key B)")

                reasons = []
                if final_prob >= 60:
                    reasons.append("ğŸš¨ **ìœ„í—˜ ê°ì§€**: AI íŒì‚¬ì™€ ì•Œê³ ë¦¬ì¦˜ ëª¨ë‘ ì´ ì˜ìƒì˜ ì£¼ì¥ì„ ì˜ì‹¬í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
                    if len(news_ev) == 0: reasons.append("ğŸ”‡ **ê·¼ê±° ë¶€ì¬**: ìê·¹ì ì¸ ì£¼ì¥ì— ë¹„í•´ ì–¸ë¡  ë³´ë„ê°€ ì „ë¬´í•©ë‹ˆë‹¤.")
                elif final_prob <= 30:
                    reasons.append("âœ… **ì•ˆì „ íŒì •**: ì˜ìƒ ë‚´ìš©ì´ ì£¼ìš” ë‰´ìŠ¤ ë³´ë„ì™€ ì¼ì¹˜í•˜ë©°, AI ì¶”ë¡  ê²°ê³¼ë„ ê¸ì •ì ì…ë‹ˆë‹¤.")
                else:
                    reasons.append("âš ï¸ **ì£¼ì˜ ìš”ë§**: ì¼ë¶€ ê³¼ì¥ëœ í‘œí˜„ì´ë‚˜ í™•ì¸ë˜ì§€ ì•Šì€ ì‚¬ì‹¤ì´ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                
                st.success(f"ğŸ” ìµœì¢… ë¶„ì„ ê²°ê³¼: **{final_prob}ì **")
                for r in reasons: st.write(r)

        except Exception as e: st.error(f"ì˜¤ë¥˜: {e}")

# --- [UI Layout] ---
st.title("âš–ï¸ Fact-Check Center v90.0 (Real Survivor)")

with st.container(border=True):
    st.markdown("### ğŸ›¡ï¸ ë²•ì  ê³ ì§€ ë° ì±…ì„ í•œê³„ (Disclaimer)\në³¸ ì„œë¹„ìŠ¤ëŠ” **ì¸ê³µì§€ëŠ¥(AI) ë° ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜**ìœ¼ë¡œ ì˜ìƒì˜ ì‹ ë¢°ë„ë¥¼ ë¶„ì„í•˜ëŠ” ë³´ì¡° ë„êµ¬ì…ë‹ˆë‹¤. \në¶„ì„ ê²°ê³¼ëŠ” ë²•ì  íš¨ë ¥ì´ ì—†ìœ¼ë©°, ìµœì¢… íŒë‹¨ì˜ ì±…ì„ì€ ì‚¬ìš©ìì—ê²Œ ìˆìŠµë‹ˆë‹¤.")
    st.markdown("* **Engine A (Investigator)**: ì •ë°€ í‚¤ì›Œë“œ ì¶”ì¶œ (Real Survivor Mode)\n* **Engine B (Judge)**: ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ ë° ì •ë°€ ëŒ€ì¡° (Deep-Web Crawler)")
    agree = st.checkbox("ìœ„ ë‚´ìš©ì„ í™•ì¸í•˜ì˜€ìœ¼ë©°, ì´ì— ë™ì˜í•©ë‹ˆë‹¤. (ë™ì˜ ì‹œ ë¶„ì„ ë²„íŠ¼ í™œì„±í™”)")

url_input = st.text_input("ğŸ”— ë¶„ì„í•  ìœ íŠœë¸Œ URL")
if st.button("ğŸš€ ì •ë°€ ë¶„ì„ ì‹œì‘", use_container_width=True, disabled=not agree):
    if url_input: run_forensic_main(url_input)
    else: st.warning("URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

st.divider()
st.subheader("ğŸ—‚ï¸ í•™ìŠµ ë°ì´í„° ê´€ë¦¬ (Cloud Knowledge Base)")
try:
    response = supabase.table("analysis_history").select("*").order("id", desc=True).execute()
    df = pd.DataFrame(response.data)
except: df = pd.DataFrame()

if not df.empty:
    if st.session_state["is_admin"]:
        df['Delete'] = False
        edited_df = st.data_editor(df[['Delete', 'id', 'analysis_date', 'video_title', 'fake_prob', 'keywords']], hide_index=True, use_container_width=True)
        if st.button("ğŸ—‘ï¸ ì„ íƒ í•­ëª© ì‚­ì œ", type="primary"):
            to_delete = edited_df[edited_df.Delete]
            if not to_delete.empty:
                for index, row in to_delete.iterrows(): supabase.table("analysis_history").delete().eq("id", row['id']).execute()
                st.success("ì‚­ì œ ì™„ë£Œ!"); time.sleep(1); st.rerun()
    else:
        st.dataframe(df[['analysis_date', 'video_title', 'fake_prob', 'keywords']], hide_index=True, use_container_width=True)
else: st.info("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

st.write("")
with st.expander("ğŸ” ê´€ë¦¬ì ì ‘ì† (Admin Access)"):
    if st.session_state["is_admin"]:
        st.success("ê´€ë¦¬ì ê¶Œí•œ í™œì„±í™”ë¨")
        if st.button("ë¡œê·¸ì•„ì›ƒ"):
            st.session_state["is_admin"] = False
            st.rerun()
    else:
        input_pwd = st.text_input("Admin Password", type="password")
        if st.button("Login"):
            if input_pwd == ADMIN_PASSWORD:
                st.session_state["is_admin"] = True
                st.rerun()
            else:
                st.error("Access Denied")
