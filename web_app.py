import streamlit as st
from supabase import create_client, Client
import re
import requests
import time
import json
from collections import Counter
from datetime import datetime
from mistralai import Mistral
import google.generativeai as genai
import yt_dlp
import pandas as pd
import altair as alt
from bs4 import BeautifulSoup

# -----------------------------------------------------------------------------
# 1. ì„¤ì • ë° ì´ˆê¸°í™”
# -----------------------------------------------------------------------------
st.set_page_config(page_title="AI ì˜ìƒ ë¶„ì„ê¸° (Final Fix)", layout="wide", page_icon="ğŸ›¡ï¸")

if "is_admin" not in st.session_state: st.session_state["is_admin"] = False
if "debug_logs" not in st.session_state: st.session_state["debug_logs"] = []

# Secrets ë¡œë“œ
try:
    YOUTUBE_API_KEY = st.secrets["YOUTUBE_API_KEY"]
    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    ADMIN_PASSWORD = st.secrets["ADMIN_PASSWORD"]
    MISTRAL_API_KEY = st.secrets["MISTRAL_API_KEY"]
    GOOGLE_API_KEY_A = st.secrets["GOOGLE_API_KEY_A"]
    GOOGLE_API_KEY_B = st.secrets["GOOGLE_API_KEY_B"]
except:
    st.error("âŒ API í‚¤ ì„¤ì • ì˜¤ë¥˜")
    st.stop()

@st.cache_resource
def init_clients():
    return create_client(SUPABASE_URL, SUPABASE_KEY), Mistral(api_key=MISTRAL_API_KEY)

supabase, mistral_client = init_clients()

# -----------------------------------------------------------------------------
# 2. AI ë° ë²¡í„° ì—”ì§„
# -----------------------------------------------------------------------------
def get_gemini_models_dynamic(api_key):
    genai.configure(api_key=api_key)
    return ["gemini-1.5-flash"]

class VectorEngine:
    def __init__(self):
        self.truth_vectors = []
        self.fake_vectors = []
        self.model_name = "models/text-embedding-004" 

    def get_embedding(self, text):
        try:
            genai.configure(api_key=GOOGLE_API_KEY_A) # í‚¤ ëª…ì‹œ
            result = genai.embed_content(model=self.model_name, content=text[:2000], task_type="retrieval_document")
            return result['embedding']
        except: return [0] * 768

    def load_pretrained_vectors(self, truth_vecs, fake_vecs):
        self.truth_vectors = truth_vecs
        self.fake_vectors = fake_vecs

    def cosine_similarity(self, v1, v2):
        if not v1 or not v2: return 0
        dot = sum(a*b for a,b in zip(v1,v2))
        mag1 = sum(a*a for a in v1)**0.5
        mag2 = sum(b*b for b in v2)**0.5
        return dot / (mag1 * mag2) if mag1 * mag2 != 0 else 0

    def analyze(self, query):
        query_vec = self.get_embedding(query) 
        if not self.truth_vectors: raw_t = 0
        else: raw_t = max([self.cosine_similarity(query_vec, v) for v in self.truth_vectors] or [0])
        
        if not self.fake_vectors: raw_f = 0
        else: raw_f = max([self.cosine_similarity(query_vec, v) for v in self.fake_vectors] or [0])
        
        # ì •ê·œí™” (0.75 ì´ìƒì¼ ë•Œ ì ìˆ˜ ë¶€ì—¬)
        def calibrate(s): return (s - 0.75) / 0.25 if s > 0.75 else 0
        return calibrate(raw_t), calibrate(raw_f)

vector_engine = VectorEngine()

# -----------------------------------------------------------------------------
# 3. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# -----------------------------------------------------------------------------
def parse_llm_json(text):
    try:
        text = re.sub(r'```json\s*', '', text).replace('```', '')
        return json.loads(re.search(r'(\{.*\}|\[.*\])', text, re.DOTALL).group(1))
    except: return None

def call_gemini(prompt, is_json=False):
    genai.configure(api_key=GOOGLE_API_KEY_A)
    model = genai.GenerativeModel("gemini-1.5-flash", generation_config={"response_mime_type": "application/json"} if is_json else {})
    try: return model.generate_content(prompt).text
    except: return None

# -----------------------------------------------------------------------------
# 4. ë¶„ì„ ë¡œì§
# -----------------------------------------------------------------------------
def get_keywords(title, trans):
    prompt = f"""[Input] Title: {title}\nTranscript: {trans[:5000]}\nTask: JSON output with 'queries' (3 strings) and 'vector_context' (summary)."""
    res = call_gemini(prompt, is_json=True)
    parsed = parse_llm_json(res)
    if parsed: return parsed.get('queries', [title]), parsed.get('vector_context', title)
    return [title], title

def verify_news(summary, link, snippet):
    return 50, "ë¶„ì„ ì¤‘...", "Snippet", link # (ì•½ì‹ êµ¬í˜„)

def judge_final(title, trans, evidences):
    prompt = f"Analyze fake probability for {title}. Output JSON: {{'score': 50, 'reason': 'reason'}}"
    res = call_gemini(prompt, is_json=True)
    parsed = parse_llm_json(res)
    return (parsed['score'], parsed['reason']) if parsed else (50, "Error")

def fetch_transcript(info):
    try:
        url = None
        for fmt in (info.get('subtitles') or {}).get('ko', []) + (info.get('automatic_captions') or {}).get('ko', []):
            if fmt['ext'] == 'vtt': url = fmt['url']; break
        if url: return " ".join([l.strip() for l in requests.get(url).text.splitlines() if l.strip() and '-->' not in l and '<' not in l]), "Success"
    except: pass
    return info.get('description', '')[:1000], "Fail"

# -----------------------------------------------------------------------------
# 5. DB ê´€ë¦¬ ë° í•™ìŠµ (ë¬¸ì œ í•´ê²°ì˜ í•µì‹¬)
# -----------------------------------------------------------------------------
@st.cache_data(ttl=60) # 1ë¶„ë§Œ ìºì‹œ (ìì£¼ ê°±ì‹ )
def load_all_data():
    """DBì—ì„œ ëª¨ë“  ë°ì´í„°ë¥¼ ê¸ì–´ì˜µë‹ˆë‹¤. (ì¹´ìš´íŠ¸ ì˜¤ë¥˜ ë°©ì§€)"""
    try:
        # ë²¡í„° ìˆëŠ” ê²ƒ, ì—†ëŠ” ê²ƒ ëª¨ë‘ ê°€ì ¸ì˜´ (ìµœëŒ€ 2000ê°œ)
        res = supabase.table("analysis_history").select("*").order("id", desc=True).limit(2000).execute()
        return res.data
    except: return []

def train_engine_dynamic(all_data):
    """ë©”ëª¨ë¦¬ì— ìˆëŠ” ë°ì´í„°ë¡œ ì—”ì§„ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤."""
    dt_vecs, df_vecs = [], []
    valid_count = 0
    
    for row in all_data:
        # ë²¡í„°ê°€ ì¡´ì¬í•˜ê³  ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ
        if row.get('vector_json') and row.get('vector_json') != "null":
            try:
                vec = json.loads(row['vector_json']) if isinstance(row['vector_json'], str) else row['vector_json']
                if row['fake_prob'] < 40: dt_vecs.append(vec)
                elif row['fake_prob'] > 60: df_vecs.append(vec)
                valid_count += 1
            except: pass
            
    vector_engine.load_pretrained_vectors(dt_vecs, df_vecs)
    return valid_count

def save_db(ch, ti, pr, url, kw, detail, vec_ctx):
    try: 
        embedding = vector_engine.get_embedding(vec_ctx)
        supabase.table("analysis_history").insert({
            "channel_name":ch, "video_title":ti, "fake_prob":pr, "video_url":url, 
            "keywords":kw, "detail_json":json.dumps(detail, ensure_ascii=False),
            "analysis_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "vector_json": json.dumps(embedding)
        }).execute()
        st.cache_data.clear() # ì €ì¥í–ˆìœ¼ë©´ ìºì‹œ ì´ˆê¸°í™”!
    except Exception as e: print(f"DB Error: {e}")

# -----------------------------------------------------------------------------
# 6. ë©”ì¸ ì‹¤í–‰
# -----------------------------------------------------------------------------
st.title("ğŸ›¡ï¸ ìœ íŠœë¸Œ ê°€ì§œë‰´ìŠ¤ íŒë…ê¸° (Fixed)")

# [ë°ì´í„° ë¡œë“œ] - API count ëŒ€ì‹  ì§ì ‘ ì…‰ë‹ˆë‹¤.
all_rows = load_all_data()
total_count = len(all_rows)
valid_vector_count = train_engine_dynamic(all_rows)

url = st.text_input("YouTube URL")
if st.button("ğŸš€ ë¶„ì„ ì‹œì‘", type="primary"):
    if url:
        with st.spinner("ë¶„ì„ ì¤‘..."):
            with yt_dlp.YoutubeDL({'quiet':True}) as ydl:
                try:
                    info = ydl.extract_info(url, download=False)
                    title = info.get('title')
                    trans, _ = fetch_transcript(info)
                    
                    queries, ctx = get_keywords(title, trans)
                    
                    # [ìœ ì‚¬ë„ ë¶„ì„] ì´ì œ ì—”ì§„ì— ë°ì´í„°ê°€ ë“¤ì–´ìˆìœ¼ë¯€ë¡œ ì‘ë™í•¨
                    ts, fs = vector_engine.analyze(ctx)
                    
                    score, reason = judge_final(title, trans, [])
                    
                    report = {"final_summary": reason, "summary": ctx, "ts": ts, "fs": fs}
                    
                    save_db(info.get('uploader'), title, score, url, str(queries), report, ctx)
                    
                    st.success("ì™„ë£Œ!")
                    # ê²°ê³¼ í™”ë©´
                    st.divider()
                    c1, c2, c3 = st.columns(3)
                    c1.metric("ê°€ì§œ í™•ë¥ ", f"{score}%")
                    c2.metric("ì§„ì‹¤ ìœ ì‚¬ë„", f"{int(ts*100)}%")
                    c3.metric("ê°€ì§œ ìœ ì‚¬ë„", f"{int(fs*100)}%")
                    
                    st.info(reason)
                    time.sleep(1)
                    st.rerun()
                    
                except Exception as e: st.error(f"Error: {e}")

# -----------------------------------------------------------------------------
# 7. í•˜ë‹¨ íˆìŠ¤í† ë¦¬ ë° ê´€ë¦¬ì (ë³µêµ¬ ê¸°ëŠ¥ í¬í•¨)
# -----------------------------------------------------------------------------
st.divider()
st.subheader(f"ğŸ—‚ï¸ DB History (ì´ {total_count}ê±´)")
st.caption(f"ğŸ’¡ í˜„ì¬ AI í•™ìŠµì— ì‚¬ìš© ê°€ëŠ¥í•œ(ë²¡í„°ê°€ ìˆëŠ”) ë°ì´í„°: {valid_vector_count}ê±´")

if all_rows:
    df = pd.DataFrame(all_rows)
    st.dataframe(df[['analysis_date', 'video_title', 'fake_prob']], use_container_width=True, hide_index=True)

st.divider()
with st.expander("ğŸ” ê´€ë¦¬ì (ë°ì´í„° ì‹¬íì†Œìƒìˆ )"):
    if st.session_state["is_admin"]:
        st.success("Admin Access")
        
        # [í•µì‹¬] ì£½ì–´ìˆëŠ” 567ê°œ ë°ì´í„° ì‚´ë¦¬ëŠ” ë²„íŠ¼
        st.write("### ğŸš‘ ë°ì´í„° ë³µêµ¬ ì„¼í„°")
        
        # ë²¡í„°ê°€ ì—†ëŠ” ë°ì´í„° ê°œìˆ˜ íŒŒì•…
        missing_vec = total_count - valid_vector_count
        if missing_vec > 0:
            st.warning(f"âš ï¸ í˜„ì¬ {missing_vec}ê°œì˜ ë°ì´í„°ê°€ AI í•™ìŠµì— ë°˜ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ë¹ˆ ê»ë°ê¸°)")
            
            if st.button(f"â™»ï¸ {missing_vec}ê°œ ë°ì´í„° ì¼ê´„ ì—…ë°ì´íŠ¸ (ë²¡í„° ìƒì„±)"):
                bar = st.progress(0)
                success = 0
                
                # ë²¡í„°ê°€ ì—†ëŠ” í–‰ë§Œ ê³¨ë¼ëƒ„
                target_rows = [r for r in all_rows if not r.get('vector_json') or r.get('vector_json') == "null"]
                
                for i, row in enumerate(target_rows):
                    try:
                        # ì œëª© + í‚¤ì›Œë“œë¡œ ë¬¸ë§¥ ìƒì„±
                        txt = f"{row.get('video_title', '')} {row.get('keywords', '')}"
                        vec = vector_engine.get_embedding(txt)
                        
                        # DB ì—…ë°ì´íŠ¸
                        supabase.table("analysis_history").update({"vector_json": json.dumps(vec)}).eq("id", row['id']).execute()
                        success += 1
                    except: pass
                    
                    bar.progress(int(((i+1)/len(target_rows))*100))
                    time.sleep(0.1) # API ì œí•œ ë°©ì§€
                
                st.success(f"âœ… {success}ê±´ ë³µêµ¬ ì™„ë£Œ! ì´ì œ ìœ ì‚¬ë„ ë¶„ì„ì´ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤.")
                st.cache_data.clear()
                time.sleep(2)
                st.rerun()
        else:
            st.success("âœ… ëª¨ë“  ë°ì´í„°ê°€ ìµœì‹  ìƒíƒœ(ë²¡í„° í¬í•¨)ì…ë‹ˆë‹¤.")

        if st.button("Logout"): st.session_state["is_admin"]=False; st.rerun()
    else:
        pwd = st.text_input("Password", type="password")
        if st.button("Login"):
            if pwd == ADMIN_PASSWORD: st.session_state["is_admin"]=True; st.rerun()
