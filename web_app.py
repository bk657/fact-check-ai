import streamlit as st
from supabase import create_client, Client
import re
import requests
import time
import random
import math
from datetime import datetime
from collections import Counter
import yt_dlp
import pandas as pd
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup

# --- [1. ì‹œìŠ¤í…œ ì„¤ì •] ---
st.set_page_config(page_title="Fact-Check Center v48.0 (Hybrid)", layout="wide", page_icon="âš–ï¸")

# ðŸŒŸ Secretsì—ì„œ í‚¤ ê°€ì ¸ì˜¤ê¸°
try:
    YOUTUBE_API_KEY = st.secrets["YOUTUBE_API_KEY"]
    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    ADMIN_PASSWORD = st.secrets["ADMIN_PASSWORD"]
except:
    st.error("âŒ í•„ìˆ˜ í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Streamlit Secretsë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    st.stop()

@st.cache_resource
def init_supabase():
    return create_client(SUPABASE_URL, SUPABASE_KEY)

supabase = init_supabase()

# --- [ê´€ë¦¬ìž ì¸ì¦] ---
if "is_admin" not in st.session_state: st.session_state["is_admin"] = False
with st.sidebar:
    st.header("ðŸ›¡ï¸ ê´€ë¦¬ìž ë©”ë‰´")
    with st.form("login_form"):
        password_input = st.text_input("ê´€ë¦¬ìž ë¹„ë°€ë²ˆí˜¸", type="password")
        if st.form_submit_button("ë¡œê·¸ì¸"):
            if password_input == ADMIN_PASSWORD:
                st.session_state["is_admin"] = True; st.rerun()
            else: st.session_state["is_admin"] = False; st.error("ë¶ˆì¼ì¹˜")
    if st.session_state["is_admin"]:
        st.success("âœ… ê´€ë¦¬ìž ì¸ì¦ë¨"); 
        if st.button("ë¡œê·¸ì•„ì›ƒ"): st.session_state["is_admin"] = False; st.rerun()

# --- [ìƒìˆ˜ ì„¤ì •] ---
WEIGHT_NEWS_DEFAULT = 45       
WEIGHT_VECTOR = 35     
WEIGHT_CONTENT = 15    
WEIGHT_SENTIMENT_DEFAULT = 10  
PENALTY_ABUSE = 20     
PENALTY_MISMATCH = 30
PENALTY_NO_FACT = 25
PENALTY_SILENT_ECHO = 40  

# í•µì‹¬ ìƒíƒœì–´ (ì •ì¹˜/ì‚¬íšŒì  íŒŒê¸‰ë ¥ì´ í° ë‹¨ì–´)
VITAL_KEYWORDS = [
    'ìœ„ë…', 'ì‚¬ë§', 'ë³„ì„¸', 'êµ¬ì†', 'ì²´í¬', 'ê¸°ì†Œ', 'ì‹¤í˜•', 'ì‘ê¸‰ì‹¤', 'ì“°ëŸ¬ì ¸', 
    'ì´í˜¼', 'ë¶ˆí™”', 'íŒŒê²½', 'ì¶©ê²©', 'ê²½ì•…', 'ì†ë³´', 'ê¸´ê¸‰', 'í­ë¡œ', 'ì–‘ì„±', 
    'í™•ì§„', 'ì‹¬ì •ì§€', 'ë‡Œì‚¬', 'ì¤‘íƒœ', 'ì••ìˆ˜ìˆ˜ìƒ‰', 'ì†Œí™˜', 'í‡´ì§„', 'íƒ„í•µ', 'ë‚´ëž€'
]

# VIP ì¸ë¬¼ ì‚¬ì „
VIP_ENTITIES = [
    'ìœ¤ì„ì—´', 'ëŒ€í†µë ¹', 'ì´ìž¬ëª…', 'í•œë™í›ˆ', 'ê¹€ê±´í¬', 'ë¬¸ìž¬ì¸', 'ë°•ê·¼í˜œ', 'ì´ëª…ë°•',
    'íŠ¸ëŸ¼í”„', 'ë°”ì´ë“ ', 'í‘¸í‹´', 'ì ¤ë ŒìŠ¤í‚¤', 'ì‹œì§„í•‘', 'ì •ì€', 
    'ì´ì¤€ì„', 'ì¡°êµ­', 'ì¶”ë¯¸ì• ', 'í™ì¤€í‘œ', 'ìœ ìŠ¹ë¯¼', 'ì•ˆì² ìˆ˜',
    'ì†í¥ë¯¼', 'ì´ê°•ì¸', 'ê¹€ë¯¼ìž¬', 'ë¥˜í˜„ì§„', 'ìž¬ìš©', 'ì •ì˜ì„ ', 'ìµœíƒœì›'
]

# ê³µì‹ ì–¸ë¡ ì‚¬ ë¦¬ìŠ¤íŠ¸ (ë‰´ìŠ¤ê°ë³„ì‚¬ ë“± ìœ ì‚¬ ì–¸ë¡  ì œì™¸)
OFFICIAL_CHANNELS = [
    'MBC', 'KBS', 'SBS', 'EBS', 'YTN', 'JTBC', 'TVCHOSUN', 'MBN', 'CHANNEL A', 'OBS',
    'ì±„ë„A', 'TVì¡°ì„ ', 'ì—°í•©ë‰´ìŠ¤', 'YONHAP',
    'í•œê²¨ë ˆ', 'ê²½í–¥', 'ì¡°ì„ ì¼ë³´', 'ì¤‘ì•™ì¼ë³´', 'ë™ì•„ì¼ë³´', 'í•œêµ­ì¼ë³´', 'êµ­ë¯¼ì¼ë³´', 
    'ì„œìš¸ì‹ ë¬¸', 'ì„¸ê³„ì¼ë³´', 'ë¬¸í™”ì¼ë³´', 'ë§¤ì¼ê²½ì œ', 'í•œêµ­ê²½ì œ', 'ì„œìš¸ê²½ì œ',
    'CHOSUN', 'JOONGANG', 'DONGA', 'HANKYOREH', 'KYUNGHYANG'
]

STATIC_TRUTH_CORPUS = ["ë°•ë‚˜ëž˜ ìœ„ìž¥ì „ìž… ì˜í˜¹ ë¬´í˜ì˜", "ìž„ì˜ì›… ì½˜ì„œíŠ¸ ì•”í‘œ ëŒ€ì‘", "ì •í¬ì› êµìˆ˜ ì €ì†ë…¸í™”", "ëŒ€ì „ ì¶©ë‚¨ í–‰ì • í†µí•©", "ì„ ê±° ì¶œë§ˆ ì„ ì–¸", "ê°•í›ˆì‹ ì˜ì› ì¶œë§ˆì„¤"]
STATIC_FAKE_CORPUS = ["ì¶©ê²© í­ë¡œ ê²½ì•…", "ê¸´ê¸‰ ì†ë³´ ì†Œë¦„", "ì´ìž¬ëª… í•œë™í›ˆ ì¶©ê²© ë°œì–¸", "ê²°êµ­ êµ¬ì† ì˜ìž¥ ë°œë¶€", "ë°©ì†¡ ë¶ˆê°€ ì˜ìƒ ìœ ì¶œ", "ê¿ˆì† ê³„ì‹œ ì˜ˆì–¸", "ì‚¬í˜• ì„ ê³  ì§‘í–‰", "ê±´ê°• ì•…í™” ìœ„ë…ì„¤"]

class VectorEngine:
    def __init__(self):
        self.vocab = set(); self.truth_vectors = []; self.fake_vectors = []
    def tokenize(self, text): return re.findall(r'[ê°€-íž£]{2,}', text)
    def build_vocabulary(self, corpus):
        for text in corpus: self.vocab.update(self.tokenize(text))
        self.vocab = sorted(list(self.vocab))
    def text_to_vector(self, text):
        tokens = self.tokenize(text); token_counts = Counter(tokens); vector = []
        for word in self.vocab: vector.append(token_counts[word])
        return vector
    def cosine_similarity(self, vec1, vec2):
        dot = sum(a * b for a, b in zip(vec1, vec2))
        mag1 = math.sqrt(sum(a * a for a in vec1)); mag2 = math.sqrt(sum(b * b for b in vec2))
        return dot / (mag1 * mag2) if mag1 * mag2 > 0 else 0.0
    def train(self, truth, fake):
        self.build_vocabulary(truth + fake)
        self.truth_vectors = [self.text_to_vector(t) for t in truth]
        self.fake_vectors = [self.text_to_vector(t) for t in fake]
    def analyze_position(self, query):
        q_vec = self.text_to_vector(query)
        max_t = max([self.cosine_similarity(q_vec, v) for v in self.truth_vectors] or [0])
        max_f = max([self.cosine_similarity(q_vec, v) for v in self.fake_vectors] or [0])
        return max_t, max_f

vector_engine = VectorEngine()

def save_analysis(channel, title, prob, url, keywords):
    try: supabase.table("analysis_history").insert({
        "channel_name": channel, "video_title": title, "fake_prob": prob, 
        "analysis_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 
        "video_url": url, "keywords": keywords}).execute()
    except: pass

def train_dynamic_vector_engine():
    try:
        dt = [r['video_title'] for r in supabase.table("analysis_history").select("video_title").lt("fake_prob", 30).execute().data]
        df = [r['video_title'] for r in supabase.table("analysis_history").select("video_title").gt("fake_prob", 70).execute().data]
    except: dt, df = [], []
    vector_engine.train(STATIC_TRUTH_CORPUS + dt, STATIC_FAKE_CORPUS + df)
    return len(STATIC_TRUTH_CORPUS + dt) + len(STATIC_FAKE_CORPUS + df)

# --- [Advanced Logic Functions] ---

# ðŸŒŸ [ê°œì„ ] ë…¸ì´ì¦ˆ ë¦¬ìŠ¤íŠ¸ ëŒ€í­ ê°•í™” (ì¼ë°˜ì ì¸ ë‹¨ì–´ ì œì™¸)
def get_noise_words():
    return ['ì¶©ê²©', 'ê²½ì•…', 'ì‹¤ì²´', 'ë‚œë¦¬', 'ê³µê°œ', 'ë°˜ì‘', 'ëª…ë‹¨', 'ë™ì˜ìƒ', 'ì‚¬ì§„', 'ì§‘ì•ˆ', 'ì†ë³´', 
            'ë‹¨ë…', 'ê²°êµ­', 'MBC', 'ë‰´ìŠ¤', 'ì´ë¯¸ì§€', 'ë„ˆë¬´', 'ë‹¤ë¥¸', 'ì•Œê³ ë³´ë‹ˆ', 'ã„·ã„·', 'ì§„ì§œ', 
            'ì •ë§', 'ì˜ìƒ', 'ì‚¬ëžŒ', 'ìƒê°', 'ì˜¤ëŠ˜ë°¤', 'ì˜¤ëŠ˜', 'ë‚´ì¼', 'ì§€ê¸ˆ', 'ëª»ë„˜ê¸´ë‹¤', 'ë„˜ê¸´ë‹¤', 
            'ì´ìœ ', 'ì™œ', 'ì•ˆ', 'ëŒ€ë¶€ë¶„', 'ëª¨ë¥´ì§€ë§Œ', 'ìžˆëŠ”', 'ì—†ëŠ”', 'í•˜ëŠ”', 'ëª¸ì„', 'ëª¸', 'ê±´ê°•']

def extract_nouns(text):
    noise = get_noise_words()
    nouns = re.findall(r'[ê°€-íž£A-Za-z0-9]{2,}', text)
    return [n for n in nouns if n not in noise]

# ðŸŒŸ [v48.0 ì‹ ê·œ] í•˜ì´ë¸Œë¦¬ë“œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì œëª© + ìžë§‰ ìœµí•©)
def generate_hybrid_query(title, hashtags, transcript):
    # 1. ì†ŒìŠ¤ ì¤€ë¹„
    title_text = title + " " + " ".join([h.replace("#", "") for h in hashtags])
    transcript_text = transcript if transcript else ""
    
    # 2. ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°€ì¤‘ì¹˜ ë†’ìŒ)
    title_nouns = extract_nouns(title_text)
    
    # 3. ìžë§‰ì—ì„œ ë¹ˆì¶œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°€ì¤‘ì¹˜ ë‚®ìŒ, ë³´ì™„ìš©)
    transcript_nouns = extract_nouns(transcript_text)
    transcript_counter = Counter(transcript_nouns)
    # ìƒìœ„ 3ê°œë§Œ ì¶”ì¶œ (ë„ˆë¬´ ë§Žìœ¼ë©´ ê²€ìƒ‰ ê¼¬ìž„)
    top_transcript_nouns = [word for word, count in transcript_counter.most_common(3)]
    
    # 4. VIP/Vital ì²´í¬ (ìµœìš°ì„  ìˆœìœ„)
    vip_found = [vip for vip in VIP_ENTITIES if vip in title_text]
    vital_found = [vital for vital in VITAL_KEYWORDS if vital in title_text]
    
    # 5. ì¿¼ë¦¬ ì¡°í•© ë¡œì§
    final_query = []
    
    if vip_found:
        # VIPê°€ ìžˆìœ¼ë©´: VIP + Vital + (ì œëª© ëª…ì‚¬ ì¤‘ VIP ì•„ë‹Œ ê²ƒ)
        final_query.extend(vip_found)
        if vital_found: final_query.extend(vital_found)
        # VIP ê´€ë ¨ ë¬¸ë§¥ ì¶”ê°€ (ì¡°ì‚¬ ê¸°ë°˜ Chunking ë¡œì§ ê°„ì†Œí™” ì ìš©)
        for t_noun in title_nouns:
            if t_noun not in final_query and t_noun not in VIP_ENTITIES:
                final_query.append(t_noun)
                break # í•˜ë‚˜ë§Œ ì¶”ê°€
    else:
        # VIPê°€ ì—†ìœ¼ë©´: ì œëª© ëª…ì‚¬ + ìžë§‰ ë¹ˆì¶œ ëª…ì‚¬ ê²°í•©
        final_query.extend(title_nouns[:2]) # ì œëª© ì•ž 2ê°œ
        
        # ìžë§‰ì—ì„œ ë³´ì™„ (ì œëª©ì— ì—†ëŠ” ë‚´ìš©ì´ë©´ ì¶”ê°€)
        for tr_noun in top_transcript_nouns:
            if tr_noun not in final_query:
                final_query.append(tr_noun)
                if len(final_query) >= 3: break # ìµœëŒ€ 3ë‹¨ì–´ ì¡°í•©
                
    return " ".join(final_query)

def summarize_transcript(text, max_sentences=3):
    if not text or len(text) < 50: return "âš ï¸ ìš”ì•½í•  ìžë§‰ ë‚´ìš©ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    sentences = re.split(r'(?<=[.?!])\s+', text)
    if len(sentences) <= max_sentences: return text
    nouns = re.findall(r'[ê°€-íž£]{2,}', text); word_freq = Counter(nouns); ranked_sentences = []
    for i, sent in enumerate(sentences):
        sent_nouns = re.findall(r'[ê°€-íž£]{2,}', sent)
        if not sent_nouns: continue
        score = sum(word_freq[w] for w in sent_nouns)
        if 10 < len(sent) < 150: ranked_sentences.append((i, sent, score / len(sent_nouns)))
    top_sentences = sorted(ranked_sentences, key=lambda x: x[2], reverse=True)[:max_sentences]
    top_sentences.sort(key=lambda x: x[0])
    return f"ðŸ“Œ **í•µì‹¬ ìš”ì•½**: {' '.join([s[1] for s in top_sentences])}"

def clean_html(raw_html): return BeautifulSoup(raw_html, "html.parser").get_text()

def detect_ai_content(info):
    is_ai, reasons = False, []
    ai_keywords = ['ai', 'artificial intelligence', 'chatgpt', 'midjourney', 'sora', 'deepfake', 'synthetic', 'ì¸ê³µì§€ëŠ¥', 'ë”¥íŽ˜ì´í¬', 'ê°€ìƒì¸ê°„', 'ë²„ì¶”ì–¼', 'gpt']
    text_to_check = (info.get('title', '') + " " + info.get('description', '') + " " + " ".join(info.get('tags', []))).lower()
    for kw in ai_keywords:
        if re.search(r'\b{}\b'.format(re.escape(kw)), text_to_check): is_ai = True; reasons.append(f"í‚¤ì›Œë“œ ê°ì§€: {kw}"); break
    return is_ai, ", ".join(reasons)

def check_is_official(channel_name):
    norm_name = channel_name.upper().replace(" ", "")
    for official in OFFICIAL_CHANNELS:
        if official in norm_name: return True
    return False

def count_sensational_words(text):
    triggers = ['ì¶©ê²©', 'ê²½ì•…', 'ì‹¤ì²´', 'í­ë¡œ', 'ë‚œë¦¬', 'ì†ë³´', 'ê¸´ê¸‰', 'ì†Œë¦„', 'ã„·ã„·', 'ì§„ì§œ', 'ê²°êµ­', 'ê³„ì‹œ', 'ì˜ˆì–¸', 'ìœ„ë…', 'ì‚¬ë§', 'ì¤‘íƒœ']
    count = 0
    for w in triggers: count += text.count(w)
    return count

def check_tag_abuse(title, hashtags, channel_name):
    if check_is_official(channel_name): return 0, "ê³µì‹ ì±„ë„ ë©´ì œ"
    if not hashtags: return 0, "í•´ì‹œíƒœê·¸ ì—†ìŒ"
    title_nouns = extract_nouns(title); tag_nouns = set()
    for t in hashtags: tag_nouns.add(t.replace("#", "").split(":")[-1].strip())
    if len(tag_nouns) < 2: return 0, "ì–‘í˜¸"
    if not set(title_nouns).intersection(tag_nouns): return PENALTY_ABUSE, "ðŸš¨ ì‹¬ê° (ë¶ˆì¼ì¹˜)"
    return 0, "ì–‘í˜¸"

def fetch_real_transcript(info_dict):
    sub_url = None
    if 'subtitles' in info_dict and 'ko' in info_dict['subtitles']:
        for fmt in info_dict['subtitles']['ko']:
            if fmt['ext'] == 'vtt': sub_url = fmt['url']; break
    if not sub_url and 'automatic_captions' in info_dict and 'ko' in info_dict['automatic_captions']:
        for fmt in info_dict['automatic_captions']['ko']:
            if fmt['ext'] == 'vtt': sub_url = fmt['url']; break
    if not sub_url: return None, "ìžë§‰ ì—†ìŒ (ì„¤ëª…ëž€ ëŒ€ì²´)"
    try:
        response = requests.get(sub_url)
        if response.status_code == 200:
            lines = response.text.splitlines(); clean_lines = []; seen = set()
            for line in lines:
                line = line.strip()
                if '-->' in line or line == 'WEBVTT' or not line: continue
                line = re.sub(r'<[^>]+>', '', line)
                if line and line not in seen: clean_lines.append(line); seen.add(line)
            return " ".join(clean_lines), "âœ… ì‹¤ì œ ìžë§‰ ìˆ˜ì§‘ ì„±ê³µ"
    except: pass
    return None, "ìžë§‰ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"

def fetch_comments_via_api(video_id):
    url = "https://www.googleapis.com/youtube/v3/commentThreads"
    params = {'part': 'snippet', 'videoId': video_id, 'key': YOUTUBE_API_KEY, 'maxResults': 100, 'order': 'relevance'}
    try:
        res = requests.get(url, params=params)
        if res.status_code == 200:
            items = res.json().get('items', [])
            top_comments = [item['snippet']['topLevelComment']['snippet']['textDisplay'] for item in items]
            return top_comments[:50], f"âœ… API ìˆ˜ì§‘ ì„±ê³µ (Top {len(top_comments[:50])} by Likes)"
        elif res.status_code == 403: return [], "âš ï¸ API ê¶Œí•œ ì˜¤ë¥˜"
        elif res.status_code == 404: return [], "âš ï¸ ëŒ“ê¸€ ì‚¬ìš© ì¤‘ì§€ë¨"
        else: return [], f"âš ï¸ API ì˜¤ë¥˜ ({res.status_code})"
    except: return [], f"âŒ API í†µì‹  ì‹¤íŒ¨"

def calculate_dual_match(news_item, query_nouns, transcript):
    if not news_item or not transcript: return 0, 0, 0
    news_title = news_item.get('title', ''); title_nouns = extract_nouns(news_title)
    intersection = len(set(query_nouns).intersection(set(title_nouns)))
    title_score = 1.0 if intersection >= 2 else (0.5 if intersection == 1 else 0)
    news_desc = news_item.get('desc', ''); desc_nouns = extract_nouns(news_desc)
    if not desc_nouns: content_score = 0
    else:
        match_count = 0
        for noun in desc_nouns:
            if noun in transcript: match_count += 1
        content_ratio = match_count / len(desc_nouns)
        content_score = 1.0 if content_ratio >= 0.3 else (0.5 if content_ratio >= 0.15 else 0)
    total_score = (title_score * 0.3) + (content_score * 0.7)
    return int(total_score * 100), int(title_score * 100), int(content_score * 100)

def analyze_comment_relevance(comments, context_text):
    if not comments: return [], 0, "ë¶„ì„ ë¶ˆê°€"
    all_comments_text = " ".join(comments); comment_nouns = extract_nouns(all_comments_text)
    if not comment_nouns: return [], 0, "ìœ íš¨ í‚¤ì›Œë“œ ì—†ìŒ"
    top_keywords = Counter(comment_nouns).most_common(5)
    context_nouns = extract_nouns(context_text); match_count = 0; context_set = set(context_nouns)
    for word, cnt in top_keywords:
        if word in context_set: match_count += 1
    relevance_score = int((match_count / len(top_keywords)) * 100)
    msg = "âœ… ì£¼ì œ ì§‘ì¤‘" if relevance_score >= 60 else "âš ï¸ ì¼ë¶€ ê´€ë ¨" if relevance_score >= 20 else "âŒ ë¬´ê´€/ìž¡ë‹´"
    return [f"{w}({c})" for w, c in top_keywords], relevance_score, msg

def check_red_flags(comments):
    keywords = ['ê°€ì§œë‰´ìŠ¤', 'ê°€ì§œ ë‰´ìŠ¤', 'ì£¼ìž‘', 'ì‚¬ê¸°', 'ê±°ì§“ë§', 'í—ˆìœ„', 'êµ¬ë¼', 'í•©ì„±', 'ì„ ë™', 'ì†Œì„¤']
    count = 0; detected = []
    for c in comments:
        for k in keywords:
            if k in c: count += 1; detected.append(k)
    return count, list(set(detected))

# --- [Main] ---
def run_forensic_main(url):
    total_intelligence = train_dynamic_vector_engine()
    witty_loading_sequence(total_intelligence)
    video_id = re.search(r'(?:v=|\/)([0-9A-Za-z_-]{11}).*', url)
    if video_id: video_id = video_id.group(1)
    
    ydl_opts = {'quiet': True, 'skip_download': True, 'writesubtitles': True, 'subtitleslangs': ['ko'], 'extractor_args': {'youtube': {'skip': ['dash', 'hls']}}}
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        try:
            info = ydl.extract_info(url, download=False)
            title = info.get('title', ''); uploader = info.get('uploader', '')
            tags = info.get('tags', []); desc = info.get('description', '')
            
            transcript_text, transcript_status = fetch_real_transcript(info)
            analysis_text = transcript_text if transcript_text else desc
            
            # ðŸŒŸ [v48.0] í•˜ì´ë¸Œë¦¬ë“œ ì¿¼ë¦¬ ìƒì„±
            refined_query = generate_hybrid_query(title, tags, transcript_text)
            
            is_official = check_is_official(uploader)
            is_ai_content, ai_reason = detect_ai_content(info)
            abuse_score, abuse_status = check_tag_abuse(title, tags, uploader)
            summary_text = summarize_transcript(analysis_text)
            agitation_count = count_sensational_words(analysis_text + title)
            agitation_level = "ë†’ìŒ (ìœ„í—˜)" if agitation_count > 3 else "ë³´í†µ"
            
            # ë²¡í„° ë¶„ì„
            t_sim, f_sim = vector_engine.analyze_position(refined_query + " " + title)
            t_impact = int(t_sim * 35) * -1; f_impact = int(f_sim * 35)
            
            # ë‰´ìŠ¤ ê²€ìƒ‰
            news_ev = []; max_dual_score = 0; news_cnt = 0
            try:
                rss_url = f"https://news.google.com/rss/search?q={requests.utils.quote(refined_query)}&hl=ko&gl=KR"
                root = ET.fromstring(requests.get(rss_url, timeout=5).content)
                items = root.findall('.//item'); news_cnt = len(items)
                for item in items[:3]:
                    nt = item.find('title').text; nd = clean_html(item.find('description').text)
                    total, t_sc, c_sc = calculate_dual_match({'title': nt, 'desc': nd}, extract_nouns(refined_query), analysis_text)
                    if total > max_dual_score: max_dual_score = total
                    news_ev.append({"ë‰´ìŠ¤ ì œëª©": nt, "ì¼ì¹˜ë„": f"{total}%"})
            except: pass
            
            # ëŒ“ê¸€ ë¶„ì„
            comments, c_status = fetch_comments_via_api(video_id)
            top_k, rel_score, rel_msg = analyze_comment_relevance(comments, title + " " + analysis_text)
            red_cnt, red_words = check_red_flags(comments)
            is_controversial = red_cnt > 0
            
            # ì ìˆ˜ ì‚°ì •
            news_score = 0; silent_penalty = 0; mismatch_penalty = 0
            is_effective_silence = (news_cnt == 0) or (news_cnt > 0 and max_dual_score < 20)
            
            if is_effective_silence:
                if agitation_count >= 3: silent_penalty = PENALTY_SILENT_ECHO; t_impact *= 2; f_impact *= 2
                else: mismatch_penalty = 10 
            elif is_controversial:
                if max_dual_score < 60: news_score = PENALTY_NO_FACT 
                else: news_score = int((max_dual_score/100)**2 * 65) * -1
            else:
                news_score = int((max_dual_score/100)**2 * 45) * -1
            
            if is_official: news_score = -50; mismatch_penalty = 0; silent_penalty = 0
            
            total_score = 50 + t_impact + f_impact + news_score + silent_penalty + mismatch_penalty + abuse_score
            final_prob = max(5, min(99, total_score))
            
            save_analysis(uploader, title, final_prob, url, refined_query)
            
            # --- UI Output ---
            st.subheader("ðŸ•µï¸ í•µì‹¬ ë¶„ì„ ì§€í‘œ")
            c1, c2, c3 = st.columns(3)
            c1.metric("ê°€ì§œë‰´ìŠ¤ í™•ë¥ ", f"{final_prob}%", delta=f"{total_score-50}")
            c2.metric("íŒì •", "ðŸš¨ ìœ„í—˜" if final_prob>60 else "ðŸŸ¢ ì•ˆì „" if final_prob<30 else "ðŸŸ  ì£¼ì˜")
            c3.metric("AI ì§€ëŠ¥ ë ˆë²¨", f"{total_intelligence}", "+1")
            
            if is_official: st.success(f"ðŸ›¡ï¸ ê³µì‹ ì–¸ë¡ ì‚¬({uploader})ìž…ë‹ˆë‹¤.")
            if is_ai_content: st.warning(f"ðŸ¤– AI ì½˜í…ì¸  ê°ì§€: {ai_reason}")
            if silent_penalty: st.error("ðŸ”‡ ì¹¨ë¬µì˜ ë©”ì•„ë¦¬: ìžê·¹ì ì´ë‚˜ ê·¼ê±°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            
            st.divider()
            col1, col2 = st.columns([1, 1.4])
            with col1:
                st.info(f"ðŸŽ¯ í•€í¬ì¸íŠ¸ ê²€ìƒ‰ì–´: {refined_query}")
                st.write("**ì˜ìƒ ìš”ì•½**"); st.write(summary_text)
                st.table(pd.DataFrame([["ê¸°ë³¸ ì ìˆ˜", 50], ["ë²¡í„° ë¶„ì„", t_impact+f_impact], ["ë‰´ìŠ¤ ëŒ€ì¡°", news_score], ["ì¹¨ë¬µ/ë¶ˆì¼ì¹˜", silent_penalty+mismatch_penalty]], columns=["í•­ëª©", "ì ìˆ˜"]))
            with col2:
                colored_progress_bar("ì§„ì‹¤ ìœ ì‚¬ë„", t_sim, "green"); colored_progress_bar("ê±°ì§“ ìœ ì‚¬ë„", f_sim, "red")
                st.write(f"**ë‰´ìŠ¤ ê²€ìƒ‰ ({news_cnt}ê±´)**"); st.table(pd.DataFrame(news_ev)) if news_ev else st.warning("ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ")
                st.write("**ëŒ“ê¸€ ë¶„ì„**"); st.write(f"ì—¬ë¡ : {rel_msg}, ë…¼ëž€ í‚¤ì›Œë“œ: {red_cnt}íšŒ")

        except Exception as e: st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")

st.title("âš–ï¸ Triple-Evidence Intelligence Forensic v48.0")
url = st.text_input("ðŸ”— ìœ íŠœë¸Œ URL ìž…ë ¥")
if st.button("ðŸš€ ë¶„ì„ ì‹œìž‘") and url: run_forensic_main(url)

st.divider()
st.subheader("ðŸ—‚ï¸ í•™ìŠµ ë°ì´í„° ê´€ë¦¬ (Cloud)")
try:
    df = pd.DataFrame(supabase.table("analysis_history").select("*").order("id", desc=True).execute().data)
    if not df.empty and st.session_state["is_admin"]:
        ed = st.data_editor(df, num_rows="dynamic", key="editor")
        if st.button("ì‚­ì œ ì ìš©"):
            st.warning("ê¸°ëŠ¥ êµ¬í˜„ ì¤‘ (ì§ì ‘ DB ê´€ë¦¬ ê¶Œìž¥)") 
    elif not df.empty: st.dataframe(df)
    else: st.info("ë°ì´í„° ì—†ìŒ")
except: pass
