import streamlit as st
from supabase import create_client, Client
import re
import requests
import time
import random
import math
from datetime import datetime
from collections import Counter
import yt_dlp
import pandas as pd
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup

# --- [1. ì‹œìŠ¤í…œ ì„¤ì •] ---
st.set_page_config(page_title="Fact-Check Center v48.3", layout="wide", page_icon="âš–ï¸")

# ðŸŒŸ Secrets
try:
    YOUTUBE_API_KEY = st.secrets["YOUTUBE_API_KEY"]
    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    ADMIN_PASSWORD = st.secrets["ADMIN_PASSWORD"]
except:
    st.error("âŒ í•„ìˆ˜ í‚¤ ì„¤ì • ì˜¤ë¥˜ (Secrets)")
    st.stop()

@st.cache_resource
def init_supabase():
    return create_client(SUPABASE_URL, SUPABASE_KEY)

supabase = init_supabase()

# --- [ê´€ë¦¬ìž ì¸ì¦] ---
if "is_admin" not in st.session_state: st.session_state["is_admin"] = False
with st.sidebar:
    st.header("ðŸ›¡ï¸ ê´€ë¦¬ìž ë©”ë‰´")
    with st.form("login_form"):
        password_input = st.text_input("ê´€ë¦¬ìž ë¹„ë°€ë²ˆí˜¸", type="password")
        if st.form_submit_button("ë¡œê·¸ì¸"):
            if password_input == ADMIN_PASSWORD:
                st.session_state["is_admin"] = True; st.rerun()
            else: st.session_state["is_admin"] = False; st.error("ë¶ˆì¼ì¹˜")
    if st.session_state["is_admin"]:
        st.success("âœ… ê´€ë¦¬ìž ì¸ì¦ë¨")
        if st.button("ë¡œê·¸ì•„ì›ƒ"): st.session_state["is_admin"] = False; st.rerun()

# --- [ìƒìˆ˜] ---
VITAL_KEYWORDS = ['ìœ„ë…', 'ì‚¬ë§', 'ë³„ì„¸', 'êµ¬ì†', 'ì²´í¬', 'ê¸°ì†Œ', 'ì‹¤í˜•', 'ì‘ê¸‰ì‹¤', 'ì´í˜¼', 'ë¶ˆí™”', 'íŒŒê²½', 'ì¶©ê²©', 'ê²½ì•…', 'ì†ë³´', 'ê¸´ê¸‰', 'í­ë¡œ', 'ì–‘ì„±', 'í™•ì§„', 'ì‹¬ì •ì§€', 'ë‡Œì‚¬', 'ì¤‘íƒœ', 'ì••ìˆ˜ìˆ˜ìƒ‰', 'ì†Œí™˜', 'í‡´ì§„', 'íƒ„í•µ', 'ë‚´ëž€']
VIP_ENTITIES = ['ìœ¤ì„ì—´', 'ëŒ€í†µë ¹', 'ì´ìž¬ëª…', 'í•œë™í›ˆ', 'ê¹€ê±´í¬', 'ë¬¸ìž¬ì¸', 'ë°•ê·¼í˜œ', 'ì´ëª…ë°•', 'íŠ¸ëŸ¼í”„', 'ë°”ì´ë“ ', 'í‘¸í‹´', 'ì ¤ë ŒìŠ¤í‚¤', 'ì‹œì§„í•‘', 'ì •ì€', 'ì´ì¤€ì„', 'ì¡°êµ­', 'ì¶”ë¯¸ì• ', 'í™ì¤€í‘œ', 'ìœ ìŠ¹ë¯¼', 'ì•ˆì² ìˆ˜', 'ì†í¥ë¯¼', 'ì´ê°•ì¸', 'ê¹€ë¯¼ìž¬', 'ë¥˜í˜„ì§„', 'ìž¬ìš©', 'ì •ì˜ì„ ', 'ìµœíƒœì›']
OFFICIAL_CHANNELS = ['MBC', 'KBS', 'SBS', 'EBS', 'YTN', 'JTBC', 'TVCHOSUN', 'MBN', 'CHANNEL A', 'OBS', 'ì±„ë„A', 'TVì¡°ì„ ', 'ì—°í•©ë‰´ìŠ¤', 'YONHAP', 'í•œê²¨ë ˆ', 'ê²½í–¥', 'ì¡°ì„ ì¼ë³´', 'ì¤‘ì•™ì¼ë³´', 'ë™ì•„ì¼ë³´', 'í•œêµ­ì¼ë³´', 'êµ­ë¯¼ì¼ë³´', 'ì„œìš¸ì‹ ë¬¸', 'ì„¸ê³„ì¼ë³´', 'ë¬¸í™”ì¼ë³´', 'ë§¤ì¼ê²½ì œ', 'í•œêµ­ê²½ì œ', 'ì„œìš¸ê²½ì œ', 'CHOSUN', 'JOONGANG', 'DONGA', 'HANKYOREH', 'KYUNGHYANG']
STATIC_TRUTH = ["ë°•ë‚˜ëž˜ ìœ„ìž¥ì „ìž… ë¬´í˜ì˜", "ìž„ì˜ì›… ì•”í‘œ ëŒ€ì‘", "ì •í¬ì› ì €ì†ë…¸í™”", "ëŒ€ì „ ì¶©ë‚¨ í†µí•©", "ì„ ê±° ì¶œë§ˆ ì„ ì–¸"]
STATIC_FAKE = ["ì¶©ê²© í­ë¡œ ê²½ì•…", "ê¸´ê¸‰ ì†ë³´ ì†Œë¦„", "ì¶©ê²© ë°œì–¸ ë…¼ëž€", "êµ¬ì† ì˜ìž¥ ë°œë¶€", "ì˜ìƒ ìœ ì¶œ", "ê³„ì‹œ ì˜ˆì–¸", "ì‚¬í˜• ì§‘í–‰", "ìœ„ë…ì„¤"]

class VectorEngine:
    def __init__(self): self.vocab = set(); self.truth = []; self.fake = []
    def tokenize(self, t): return re.findall(r'[ê°€-íž£]{2,}', t)
    def train(self, t, f):
        for x in t+f: self.vocab.update(self.tokenize(x))
        self.vocab = sorted(list(self.vocab))
        self.truth = [self.vec(x) for x in t]; self.fake = [self.vec(x) for x in f]
    def vec(self, t):
        c = Counter(self.tokenize(t)); return [c[w] for w in self.vocab]
    def sim(self, v1, v2):
        dot = sum(a*b for a,b in zip(v1,v2)); mag = math.sqrt(sum(a*a for a in v1)) * math.sqrt(sum(b*b for b in v2))
        return dot/mag if mag>0 else 0
    def analyze(self, q):
        qv = self.vec(q); mt = max([self.sim(qv,x) for x in self.truth] or [0]); mf = max([self.sim(qv,x) for x in self.fake] or [0])
        return mt, mf

ve = VectorEngine()

def save_analysis(ch, ti, pr, url, kw):
    try: supabase.table("analysis_history").insert({"channel_name":ch, "video_title":ti, "fake_prob":pr, "analysis_date":datetime.now().strftime('%Y-%m-%d %H:%M:%S'), "video_url":url, "keywords":kw}).execute()
    except: pass

def train_ve():
    try:
        dt = [r['video_title'] for r in supabase.table("analysis_history").select("video_title").lt("fake_prob",30).execute().data]
        df = [r['video_title'] for r in supabase.table("analysis_history").select("video_title").gt("fake_prob",70).execute().data]
    except: dt, df = [], []
    ve.train(STATIC_TRUTH+dt, STATIC_FAKE+df)
    return len(STATIC_TRUTH+dt)+len(STATIC_FAKE+df)

# --- [UI Utils] ---
def colored_bar(label, val, color):
    st.markdown(f"<div style='margin-bottom:5px'><div style='display:flex;justify-content:space-between'><span>{label}</span><span style='color:{color};font-weight:bold'>{int(val*100)}%</span></div><div style='background:#eee;height:8px;border-radius:4px'><div style='background:{color};width:{val*100}%;height:100%;border-radius:4px'></div></div></div>", unsafe_allow_html=True)

def loading_seq(level):
    with st.status("ðŸ•µï¸ Forensic Core v48.3 ê°€ë™...", expanded=True) as s:
        st.write(f"ðŸ§  Intelligence Level: {level}"); time.sleep(0.3)
        st.write("ðŸ›¡ï¸ 1ì°¨ ë¶„ì„: íŒŒì‹± ì˜¤ë¥˜ ë°©ì–´ ë° êµ¬ë¬¸ ë¶„ì„..."); time.sleep(0.3)
        st.write("âœ… ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ!"); s.update(label="ë¶„ì„ ì™„ë£Œ!", state="complete", expanded=False)

# --- [Logic] ---
def get_safe_text(element):
    if element is not None and element.text: return element.text.strip()
    return ""

def clean_html(raw):
    if not raw: return ""
    try: return BeautifulSoup(raw, "html.parser").get_text()
    except: return raw

def extract_nouns(text):
    noise = ['ì¶©ê²©','ê²½ì•…','ì†ë³´','ê¸´ê¸‰','ì˜¤ëŠ˜','ë‚´ì¼','ì§€ê¸ˆ','ê²°êµ­','ë‰´ìŠ¤','ì˜ìƒ','ëŒ€ë¶€ë¶„','ì´ìœ ','ì™œ','ìžˆëŠ”','ì—†ëŠ”','í•˜ëŠ”','ê²ƒ','ìˆ˜','ë“±']
    return [n for n in re.findall(r'[ê°€-íž£A-Za-z0-9]{2,}', text) if n not in noise]

def generate_hybrid_query(title, tags, transcript):
    text = title + " " + " ".join([t.replace("#","") for t in tags])
    tn = extract_nouns(text); trn = extract_nouns(transcript if transcript else "")
    top_trn = [w for w,c in Counter(trn).most_common(3)]
    
    vip = [v for v in VIP_ENTITIES if v in text]
    vital = [v for v in VITAL_KEYWORDS if v in text]
    
    q = []
    if vip:
        q.extend(vip); q.extend(vital)
        for n in tn: 
            if n not in q and n not in VIP_ENTITIES: q.append(n); break
    else:
        q.extend(tn[:2])
        for n in top_trn:
            if n not in q: q.append(n); 
            if len(q)>=3: break
    return " ".join(q)

def summarize(text):
    if not text or len(text)<50: return "ìš”ì•½ ì •ë³´ ì—†ìŒ"
    sents = re.split(r'(?<=[.?!])\s+', text)
    freq = Counter(re.findall(r'[ê°€-íž£]{2,}', text))
    ranked = sorted([(i, s, sum(freq[w] for w in re.findall(r'[ê°€-íž£]{2,}',s))) for i,s in enumerate(sents) if 10<len(s)<150], key=lambda x:x[2], reverse=True)[:3]
    return " ".join([r[1] for r in sorted(ranked, key=lambda x:x[0])])

def check_official(uploader):
    return any(o in uploader.upper().replace(" ","") for o in OFFICIAL_CHANNELS)

def check_tags(title, tags, uploader):
    if check_official(uploader): return 0
    if not tags: return 0
    tn = set(extract_nouns(title)); tgn = set()
    for t in tags: tgn.add(t.replace("#","").split(":")[-1].strip())
    # ì ìˆ˜(int)ë§Œ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •ë¨
    return 20 if len(tgn)>=2 and not tn.intersection(tgn) else 0

def fetch_transcript(info):
    try:
        url = None
        if 'subtitles' in info and 'ko' in info['subtitles']:
            for fmt in info['subtitles']['ko']: 
                if fmt['ext'] == 'vtt': url = fmt['url']; break
        if not url and 'automatic_captions' in info and 'ko' in info['automatic_captions']:
            for fmt in info['automatic_captions']['ko']: 
                if fmt['ext'] == 'vtt': url = fmt['url']; break
        
        if url:
            res = requests.get(url)
            if res.status_code == 200:
                clean = []
                for line in res.text.splitlines():
                    if '-->' not in line and line.strip() and not line.startswith('WEBVTT'):
                        t = re.sub(r'<[^>]+>', '', line).strip()
                        if t and t not in clean: clean.append(t)
                return " ".join(clean), "ì„±ê³µ"
    except: pass
    return None, "ì‹¤íŒ¨"

def fetch_comments(vid):
    try:
        url = "https://www.googleapis.com/youtube/v3/commentThreads"
        res = requests.get(url, params={'part':'snippet', 'videoId':vid, 'key':YOUTUBE_API_KEY, 'maxResults':50, 'order':'relevance'})
        if res.status_code == 200:
            return [i['snippet']['topLevelComment']['snippet']['textDisplay'] for i in res.json().get('items',[])], "ì„±ê³µ"
    except: pass
    return [], "ì‹¤íŒ¨"

def calc_match(news_item, query_nouns, text):
    title_n = set(extract_nouns(news_item['title']))
    desc_n = set(extract_nouns(news_item['desc']))
    query_n = set(query_nouns)
    
    t_score = 1.0 if len(query_n & title_n) >= 2 else 0.5 if len(query_n & title_n) >= 1 else 0
    
    c_cnt = 0
    if desc_n:
        for n in desc_n: 
            if n in text: c_cnt += 1
        c_score = 1.0 if c_cnt/len(desc_n) > 0.3 else 0.5 if c_cnt/len(desc_n) > 0.15 else 0
    else: c_score = 0
    
    return int((t_score*0.3 + c_score*0.7)*100)

def analyze_comments(comments, text):
    if not comments: return [], 0, "ë¶„ì„ ë¶ˆê°€"
    cn = extract_nouns(" ".join(comments))
    top = Counter(cn).most_common(5)
    ctx = set(extract_nouns(text))
    match = sum(1 for w,c in top if w in ctx)
    score = int(match/len(top)*100) if top else 0
    msg = "âœ… ì¼ì¹˜" if score>=60 else "âš ï¸ í˜¼ìž¬" if score>=20 else "âŒ ë¶ˆì¼ì¹˜"
    return [f"{w}({c})" for w,c in top], score, msg

def run_main(url):
    intel = train_ve(); loading_seq(intel)
    vid = re.search(r'(?:v=|\/)([0-9A-Za-z_-]{11}).*', url)
    if vid: vid = vid.group(1)
    
    with yt_dlp.YoutubeDL({'quiet':True, 'skip_download':True}) as ydl:
        try:
            info = ydl.extract_info(url, download=False)
            title = info.get('title',''); uploader = info.get('uploader','')
            tags = info.get('tags',[]); desc = info.get('description','')
            
            trans, t_status = fetch_transcript(info)
            full_text = trans if trans else desc
            query = generate_hybrid_query(title, tags, full_text)
            
            # 1. Vector
            ts, fs = ve.analyze(query + " " + title)
            v_score = int(fs*35) - int(ts*35)
            
            # 2. News
            news_res = []; max_match = 0; news_cnt = 0
            try:
                rss = f"https://news.google.com/rss/search?q={requests.utils.quote(query)}&hl=ko&gl=KR"
                root = ET.fromstring(requests.get(rss, timeout=5).content)
                items = root.findall('.//item'); news_cnt = len(items)
                
                for item in items[:3]:
                    nt = get_safe_text(item.find('title'))
                    nd = clean_html(get_safe_text(item.find('description')))
                    m = calc_match({'title':nt, 'desc':nd}, extract_nouns(query), full_text)
                    if m > max_match: max_match = m
                    news_res.append({"ë‰´ìŠ¤ ì œëª©": nt, "ì¼ì¹˜ë„": f"{m}%"})
            except Exception: pass 
            
            # 3. Comments
            cmts, c_st = fetch_comments(vid)
            top_kw, rel_scr, rel_msg = analyze_comments(cmts, title + " " + full_text)
            red_cnt = sum(1 for c in cmts for k in ['ê°€ì§œ','ì£¼ìž‘','ì„ ë™'] if k in c)
            
            # Scoring
            n_score = 0; silent = 0; mismatch = 0
            is_silent = (news_cnt == 0) or (news_cnt > 0 and max_match < 20)
            agitation = sum(full_text.count(w) for w in ['ì¶©ê²©','ê²½ì•…','ì†ë³´'])
            
            if is_silent:
                if agitation >= 3: silent = 40; v_score *= 2 # ì¹¨ë¬µì˜ ë©”ì•„ë¦¬
                else: mismatch = 10
            elif red_cnt > 0: # ë…¼ëž€
                if max_match < 60: n_score = 25
                else: n_score = int((max_match/100)**2 * 65) * -1
            else:
                n_score = int((max_match/100)**2 * 45) * -1
                
            if check_official(uploader): n_score = -50; silent = 0; mismatch = 0
            
            # ðŸŒŸ [ìˆ˜ì •] check_tags()[0] ì œê±° -> check_tags()
            tag_abuse_score = check_tags(title, tags, uploader)
            total = 50 + v_score + n_score + silent + mismatch + tag_abuse_score
            prob = max(5, min(99, total))
            
            save_analysis(uploader, title, prob, url, query)
            
            # Output
            st.subheader("ðŸ•µï¸ í•µì‹¬ ë¶„ì„ ì§€í‘œ")
            c1,c2,c3 = st.columns(3)
            c1.metric("ê°€ì§œë‰´ìŠ¤ í™•ë¥ ", f"{prob}%", f"{total-50}")
            c2.metric("AI íŒì •", "ðŸš¨ ìœ„í—˜" if prob>60 else "ðŸŸ¢ ì•ˆì „" if prob<30 else "ðŸŸ  ì£¼ì˜")
            c3.metric("ì§€ëŠ¥ ë ˆë²¨", intel)
            
            if silent: st.error("ðŸ”‡ ì¹¨ë¬µì˜ ë©”ì•„ë¦¬: ìžê·¹ì  ë‚´ìš©ì´ë‚˜ ê·¼ê±° ì—†ìŒ")
            if check_official(uploader): st.success(f"ðŸ›¡ï¸ ê³µì‹ ì–¸ë¡ ì‚¬({uploader})")
            
            st.divider()
            c1,c2 = st.columns([1,1])
            with c1:
                st.info(f"ðŸŽ¯ ì¿¼ë¦¬: {query}")
                st.write("**ì˜ìƒ ìš”ì•½**"); st.caption(summarize(full_text))
                st.table(pd.DataFrame([["ê¸°ë³¸",50],["ë²¡í„°",v_score],["ë‰´ìŠ¤",n_score],["íŽ˜ë„í‹°",silent+mismatch],["íƒœê·¸ì˜¤ìš©",tag_abuse_score]], columns=["í•­ëª©","ì ìˆ˜"]))
            with c2:
                colored_bar("ì§„ì‹¤", ts, "green"); colored_bar("ê±°ì§“", fs, "red")
                st.write(f"**ë‰´ìŠ¤ ({news_cnt}ê±´)**"); st.table(news_res) if news_res else st.warning("ë‰´ìŠ¤ ì—†ìŒ")
                st.write("**ì—¬ë¡ **"); st.caption(f"{rel_msg} (ë…¼ëž€ì–´ {red_cnt}íšŒ)")
                
        except Exception as e: st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")

# --- [App] ---
st.title("âš–ï¸ Triple-Evidence Intelligence Forensic v48.3")
url = st.text_input("ðŸ”— ìœ íŠœë¸Œ URL")
if st.button("ðŸš€ ë¶„ì„ ì‹œìž‘") and url: run_main(url)

st.divider()
st.subheader("ðŸ—‚ï¸ í•™ìŠµ ë°ì´í„° (Cloud)")
try:
    df = pd.DataFrame(supabase.table("analysis_history").select("*").order("id", desc=True).execute().data)
    if not df.empty:
        if st.session_state["is_admin"]:
            ed = st.data_editor(df, column_config={"Delete":st.column_config.CheckboxColumn(default=False)}, disabled=["id","video_title"], hide_index=True)
            if "Delete" in ed.columns and st.button("ì‚­ì œ"):
                for i, r in ed[ed.Delete].iterrows(): supabase.table("analysis_history").delete().eq("id", r['id']).execute()
                st.success("ì‚­ì œë¨"); st.rerun()
        else: st.dataframe(df, hide_index=True)
except: pass
