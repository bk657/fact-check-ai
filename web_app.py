import streamlit as st
from supabase import create_client, Client
import re
import requests
import time
import random
import math
import os
import json
from collections import Counter
from datetime import datetime

# --- [ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸] ---
from mistralai import Mistral
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
import yt_dlp
import pandas as pd
import altair as alt
from bs4 import BeautifulSoup

# --- [1. ì‹œìŠ¤í…œ ì„¤ì •] ---
st.set_page_config(page_title="ìœ íŠœë¸Œ ê°€ì§œë‰´ìŠ¤ íŒë…ê¸° (Triple Engine)", layout="wide", page_icon="ğŸ›¡ï¸")

st.markdown("""
    <style>
        .block-container { padding-top: 3.5rem !important; padding-bottom: 5rem; }
        .stMetric { background-color: #f8f9fa; padding: 10px; border-radius: 8px; border: 1px solid #eee; text-align: center; }
        div[data-testid="stMetricValue"] { font-size: 1.3rem !important; }
        h1 { font-size: 1.8rem !important; padding-bottom: 10px; }
        h3 { font-size: 1.2rem !important; margin-top: 20px !important; }
        .risk-badge { padding: 5px 10px; border-radius: 5px; font-weight: bold; color: white; }
    </style>
""", unsafe_allow_html=True)

if "is_admin" not in st.session_state:
    st.session_state["is_admin"] = False
if "debug_logs" not in st.session_state:
    st.session_state["debug_logs"] = []

# ğŸŒŸ Secrets ë¡œë“œ
try:
    YOUTUBE_API_KEY = st.secrets["YOUTUBE_API_KEY"]
    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    ADMIN_PASSWORD = st.secrets["ADMIN_PASSWORD"]
    
    MISTRAL_API_KEY = st.secrets["MISTRAL_API_KEY"]
    GOOGLE_API_KEY_A = st.secrets["GOOGLE_API_KEY_A"]
    GOOGLE_API_KEY_B = st.secrets["GOOGLE_API_KEY_B"]
except:
    st.error("âŒ secrets.toml íŒŒì¼ì— API Key ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤.")
    st.stop()

@st.cache_resource
def init_clients():
    su = create_client(SUPABASE_URL, SUPABASE_KEY)
    mi = Mistral(api_key=MISTRAL_API_KEY)
    return su, mi

supabase, mistral_client = init_clients()

# --- [2. ëª¨ë¸ ì •ì˜] ---
MISTRAL_MODELS = ["mistral-large-latest", "mistral-medium-latest", "mistral-small-latest"]

def get_gemini_models_dynamic(api_key):
    genai.configure(api_key=api_key)
    try:
        models = [m.name.replace("models/", "") for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        models.sort(key=lambda x: 0 if 'flash' in x else 1 if 'pro' in x else 2)
        return models
    except: return ["gemini-2.0-flash", "gemini-1.5-flash"]

# --- [í•µì‹¬ ê¸°ìˆ : ë²¡í„° ì—”ì§„] ---
class VectorEngine:
    def __init__(self):
        self.truth_vectors = []
        self.fake_vectors = []
        self.model_name = "models/text-embedding-004" 

    def get_embedding(self, text):
        try:
            genai.configure(api_key=GOOGLE_API_KEY_A)
            result = genai.embed_content(
                model=self.model_name,
                content=text[:2000],
                task_type="retrieval_document"
            )
            return result['embedding']
        except:
            return [0] * 768

    def load_pretrained_vectors(self, truth_vecs, fake_vecs):
        self.truth_vectors = truth_vecs
        self.fake_vectors = fake_vecs

    def train_static(self, truth_text, fake_text):
        self.truth_vectors.extend([self.get_embedding(t) for t in truth_text])
        self.fake_vectors.extend([self.get_embedding(t) for t in fake_text])

    def cosine_similarity(self, v1, v2):
        if not v1 or not v2: return 0
        dot = sum(a*b for a,b in zip(v1,v2))
        mag1 = math.sqrt(sum(a*a for a in v1))
        mag2 = math.sqrt(sum(b*b for b in v2))
        if mag1 == 0 or mag2 == 0: return 0
        return dot / (mag1 * mag2)

    def analyze(self, query):
        query_vec = self.get_embedding(query)
        def calibrate(score):
            baseline = 0.75 
            if score < baseline: return 0.0
            return (score - baseline) / (1.0 - baseline)
        raw_t = max([self.cosine_similarity(query_vec, v) for v in self.truth_vectors] or [0])
        raw_f = max([self.cosine_similarity(query_vec, v) for v in self.fake_vectors] or [0])
        return calibrate(raw_t), calibrate(raw_f)

vector_engine = VectorEngine()

# --- [3. ìœ í‹¸ë¦¬í‹°] ---
def parse_llm_json(text):
    try: parsed = json.loads(text)
    except:
        try:
            text = re.sub(r'```json\s*', '', text); text = re.sub(r'```', '', text)
            match = re.search(r'(\{.*\}|\[.*\])', text, re.DOTALL)
            if match: parsed = json.loads(match.group(1))
            else: return None
        except: return None
    if isinstance(parsed, list): return parsed[0] if len(parsed) > 0 and isinstance(parsed[0], dict) else None
    if isinstance(parsed, dict): return parsed
    return None

def determine_risk_level(prob):
    if prob >= 70: return "â›” ìœ„í—˜ (High Risk)", "#d32f2f"
    elif prob >= 40: return "âš ï¸ ì£¼ì˜ (Caution)", "#f57c00"
    return "âœ… ì•ˆì „ (Safe)", "#388e3c"

def colored_bar_html(label, score, color):
    pct = min(100, max(0, int(score * 100)))
    return f"""<div style="margin-bottom: 6px;"><div style="display: flex; justify-content: space-between; font-size: 13px; font-weight: 600; color: #444;"><span>{label}</span><span>{pct}%</span></div><div style="width: 100%; background-color: #e0e0e0; border-radius: 6px; height: 8px; margin-top: 2px;"><div style="width: {pct}%; background-color: {color}; height: 8px; border-radius: 6px;"></div></div></div>"""

# --- [4. AI Logic] ---
def call_triple_survivor(prompt, is_json=False):
    logs = []
    response_format = {"type": "json_object"} if is_json else None
    
    # Mistral
    for model_name in MISTRAL_MODELS:
        try:
            resp = mistral_client.chat.complete(
                model=model_name, messages=[{"role": "user", "content": prompt}],
                response_format=response_format, temperature=0.2
            )
            if resp.choices:
                logs.append(f"âœ… Success (Mistral): {model_name}")
                return resp.choices[0].message.content, model_name, logs
        except Exception as e:
            logs.append(f"âŒ Mistral Failed: {str(e)[:20]}")
            continue

    # Gemini Fallback
    generation_config = {"response_mime_type": "application/json"} if is_json else {}
    for key_name, key_val in [("Key A", GOOGLE_API_KEY_A), ("Key B", GOOGLE_API_KEY_B)]:
        logs.append(f"âš ï¸ Mistral Failed -> Gemini {key_name} íˆ¬ì…")
        genai.configure(api_key=key_val)
        models = get_gemini_models_dynamic(key_val)
        for model_name in models:
            try:
                model = genai.GenerativeModel(model_name, generation_config=generation_config)
                resp = model.generate_content(prompt)
                if resp.text:
                    logs.append(f"âœ… Success (Gemini {key_name}): {model_name}")
                    return resp.text, f"{model_name} ({key_name})", logs
            except: continue
    return None, "All Failed", logs

# --- [5. Data Constants] ---
WEIGHT_ALGO = 0.85
WEIGHT_AI = 0.15
OFFICIAL_CHANNELS = ['MBC','KBS','SBS','EBS','YTN','JTBC','TVCHOSUN','MBN','CHANNEL A','ì—°í•©ë‰´ìŠ¤','YONHAP','í•œê²¨ë ˆ','ê²½í–¥','ì¡°ì„ ','ì¤‘ì•™','ë™ì•„']
CRITICAL_STATE_KEYWORDS = ['ë³„ê±°','ì´í˜¼','íŒŒê²½','ì‚¬ë§','ìœ„ë…','êµ¬ì†','ì²´í¬','ì‹¤í˜•','ë¶ˆí™”','í­ë¡œ','ì¶©ê²©','ë…¼ë€','ì‹¬ì •ì§€','ë‡Œì‚¬','ì••ìˆ˜ìˆ˜ìƒ‰','ê°ì˜¥']
STATIC_TRUTH = ["ë°•ë‚˜ë˜ ìœ„ì¥ì „ì… ë¬´í˜ì˜", "ì„ì˜ì›… ì•”í‘œ ëŒ€ì‘", "ì •í¬ì› ì €ì†ë…¸í™”", "ì„ ê±° ì¶œë§ˆ ì„ ì–¸"]
STATIC_FAKE = ["ì¶©ê²© í­ë¡œ ê²½ì•…", "ê¸´ê¸‰ ì†ë³´ ì†Œë¦„", "êµ¬ì† ì˜ì¥ ë°œë¶€", "ì‚¬í˜• ì§‘í–‰", "ìœ„ë…ì„¤"]

def get_keywords(title, trans):
    prompt = f"""You are a Fact-Check Investigator. [Input] {title}, {trans[:10000]}. [Task] Generate 3 diverse Google News queries (Specific, Contextual, Keywords). [Output JSON] {{ "queries": ["query1", "query2", "query3"] }}"""
    res, model, logs = call_triple_survivor(prompt, is_json=True)
    st.session_state["debug_logs"].extend([f"[Key] {l}" for l in logs])
    parsed = parse_llm_json(res)
    if parsed and 'queries' in parsed and isinstance(parsed['queries'], list): return parsed['queries'], model
    return [title, title+" íŒ©íŠ¸ì²´í¬", title+" ë‰´ìŠ¤"], model

def scrape_news(url):
    try:
        res = requests.get(url, timeout=5, headers={'User-Agent': 'Mozilla/5.0'})
        soup = BeautifulSoup(res.text, 'html.parser')
        for t in soup(['script','style','nav','footer']): t.decompose()
        text = " ".join([p.get_text().strip() for p in soup.find_all('p') if len(p.get_text())>30])
        return (text[:3000], res.url) if len(text)>100 else (None, res.url)
    except: return None, url

def verify_news(summary, link, snippet):
    txt, real_url = scrape_news(link)
    ev = txt if txt else snippet
    prompt = f"Compare Video({summary[:1000]}) vs News({ev}). Match(90-100)/Related(40-60)/Mismatch(0-10). Output JSON: {{ \"score\": int, \"reason\": \"korean short\" }}"
    res, _, logs = call_triple_survivor(prompt, is_json=True)
    st.session_state["debug_logs"].extend([f"[Verify] {l}" for l in logs])
    p = parse_llm_json(res)
    return (p['score'], p['reason'], "Full" if txt else "Snippet", real_url) if p else (0, "Err", "Err", link)

def judge_final(title, trans, evidences):
    ev_text = "".join([f"- {e['ë‰´ìŠ¤ ì œëª©']} (Score:{e['ìµœì¢… ì ìˆ˜']}, Reason:{e['ë¶„ì„ ê·¼ê±°']})\n" for e in evidences])
    prompt = f"Judge Video: {title}. Evidence: {ev_text}. Decide Fake Probability (0-100). Output JSON: {{ \"score\": int, \"reason\": \"korean explanation\" }}"
    res, model, logs = call_triple_survivor(prompt, is_json=True)
    st.session_state["debug_logs"].extend([f"[Judge] {l}" for l in logs])
    p = parse_llm_json(res)
    return (p['score'], f"{p['reason']} ({model})") if p else (50, "Failed")

def generate_comprehensive_summary(title, final_prob, news_ev, red_cnt, ai_reason, risk_text):
    prompt = f"""Fact-Check AI Analyst. Video: {title}. Prob: {final_prob}% ({risk_text}). News Evidence: {len(news_ev)}. Red Comments: {red_cnt}. AI Reason: {ai_reason}.
    Task: Write a final summary report in Korean. (Why score, Fact check result, Advice)."""
    res, _, _ = call_triple_survivor(prompt, is_json=False)
    return res if res else "ì¢…í•© ë¶„ì„ ì‹¤íŒ¨"

# --- [6. Helper Functions] ---
def normalize(w): return re.sub(r'ì€$|ëŠ”$|ì´$|ê°€$|ì„$|ë¥¼$|ì˜$|ì—$|ë¡œ$', '', re.sub(r'[^ê°€-í£0-9]', '', w))
def get_tokens(t): return [normalize(w) for w in re.findall(r'[ê°€-í£]{2,}', t) if w not in ['ì¶©ê²©','ì†ë³´','ë‰´ìŠ¤']]
def get_top_kw(t): return Counter(get_tokens(t)).most_common(5)
def check_official(n): return any(o in n.upper().replace(" ","") for o in OFFICIAL_CHANNELS)
def count_agitation(t): return sum(t.count(w) for w in ['ì¶©ê²©','ê²½ì•…','ì‹¤ì²´','í­ë¡œ','ì†ë³´','ì†Œë¦„'])
def check_red_flags(cmts): 
    d = [k for c in cmts for k in ['ê°€ì§œ','ì£¼ì‘','êµ¬ë¼','í—ˆìœ„','ì„ ë™'] if k in c]
    return len(d), list(set(d))

# --- [Data Fetching & DB] ---
def fetch_transcript(info):
    try:
        url = None
        for fmt in (info.get('subtitles') or {}).get('ko', []) + (info.get('automatic_captions') or {}).get('ko', []):
            if fmt['ext'] == 'vtt': url = fmt['url']; break
        if url: return " ".join([l.strip() for l in requests.get(url).text.splitlines() if l.strip() and '-->' not in l and '<' not in l]), "Success"
    except: pass
    return None, "Fail"

def fetch_comments(vid):
    try:
        res = requests.get("https://www.googleapis.com/youtube/v3/commentThreads", params={'part':'snippet','videoId':vid,'key':YOUTUBE_API_KEY,'maxResults':50})
        if res.status_code==200: return [str(i['snippet']['topLevelComment']['snippet']['textDisplay']) for i in res.json().get('items',[])]
    except: pass
    return []

def fetch_news(q):
    try:
        raw = requests.get(f"https://news.google.com/rss/search?q={requests.utils.quote(q)}&hl=ko&gl=KR", timeout=5).text
        items = re.findall(r'<item>(.*?)</item>', raw, re.DOTALL)
        res = []
        for i in items[:10]:
            t = re.search(r'<title>(.*?)</title>', i); l = re.search(r'<link>(.*?)</link>', i)
            if t and l: res.append({'title':t.group(1).replace("<![CDATA[","").replace("]]>",""), 'link':l.group(1).strip()})
        return res
    except: return []

def analyze_comments(cmts, ctx):
    if not cmts: return [], 0, "ë°ì´í„° ë¶€ì¡±"
    safe_cmts = " ".join([str(c) for c in cmts])
    top = Counter(get_tokens(safe_cmts)).most_common(5)
    ctx_set = set(get_tokens(ctx))
    score = int(sum(1 for w,c in top if w in ctx_set)/len(top)*100) if top else 0
    return [f"{w}({c})" for w,c in top], score, "ë†’ìŒ" if score>=60 else "ë³´í†µ" if score>=20 else "ë‚®ìŒ"

# [ìˆ˜ì •] í…Œì´ë¸” ì´ë¦„ì„ 'analysis_archive_v2'ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.
@st.cache_data(ttl=3600)
def fetch_db_vectors():
    try:
        # v2 í…Œì´ë¸” ì¡°íšŒ
        res = supabase.table("analysis_archive_v2").select("video_title, fake_prob, vector_json").execute()
        if not res.data: return [], [], 0
        dt_vecs, df_vecs = [], []
        for row in res.data:
            if row.get('vector_json'):
                vec = json.loads(row['vector_json']) if isinstance(row['vector_json'], str) else row['vector_json']
                if row['fake_prob'] < 40: dt_vecs.append(vec)
                elif row['fake_prob'] > 60: df_vecs.append(vec)
        return dt_vecs, df_vecs, len(res.data)
    except: return [], [], 0

def train_engine_wrapper():
    dt_vecs, df_vecs, count = fetch_db_vectors()
    vector_engine.load_pretrained_vectors(dt_vecs, df_vecs)
    vector_engine.train_static(STATIC_TRUTH, STATIC_FAKE)
    return count, [], []

# [ìˆ˜ì •] í…Œì´ë¸” ì´ë¦„ì„ 'analysis_archive_v2'ë¡œ ë³€ê²½í–ˆìŠµë‹ˆë‹¤.
def save_db(ch, ti, pr, url, kw, detail):
    try: 
        embedding = vector_engine.get_embedding(kw + " " + ti)
        data_to_insert = {
            "channel_name":ch, "video_title":ti, "fake_prob":pr, "video_url":url, 
            "keywords":kw, "detail_json":detail, "analysis_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "vector_json": embedding
        }
        # v2 í…Œì´ë¸”ì— ì €ì¥
        supabase.table("analysis_archive_v2").insert(data_to_insert).execute()
        st.toast("âœ… DB ì €ì¥ ì™„ë£Œ!", icon="ğŸ’¾")
    except Exception as e: 
        st.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")
        print(f"DB Error: {e}")

# --- [UI Components] ---
def render_score_breakdown(data_list):
    style = """<style>table.score-table { width: 100%; border-collapse: separate; border-spacing: 0; border: 1px solid #e0e0e0; border-radius: 8px; overflow: hidden; font-family: sans-serif; font-size: 14px; margin-top: 10px;} table.score-table th { background-color: #f8f9fa; color: #495057; font-weight: bold; padding: 12px 15px; text-align: left; border-bottom: 1px solid #e0e0e0; } table.score-table td { padding: 12px 15px; border-bottom: 1px solid #f0f0f0; color: #333; } table.score-table tr:last-child td { border-bottom: none; } .badge { padding: 4px 8px; border-radius: 6px; font-weight: 700; font-size: 11px; display: inline-block; text-align: center; min-width: 45px; } .badge-danger { background-color: #ffebee; color: #d32f2f; } .badge-success { background-color: #e8f5e9; color: #2e7d32; } .badge-neutral { background-color: #f5f5f5; color: #757575; border: 1px solid #e0e0e0; }</style>"""
    rows = ""
    for item, score, note in data_list:
        try:
            score_num = int(score)
            badge = f'<span class="badge badge-danger">+{score_num}</span>' if score_num > 0 else f'<span class="badge badge-success">{score_num}</span>' if score_num < 0 else f'<span class="badge badge-neutral">0</span>'
        except: badge = f'<span class="badge badge-neutral">{score}</span>'
        rows += f"<tr><td>{item}<br><span style='color:#888; font-size:11px;'>{note}</span></td><td style='text-align: right;'>{badge}</td></tr>"
    st.markdown(f"{style}<table class='score-table'><thead><tr><th>ë¶„ì„ í•­ëª©</th><th style='text-align: right;'>ë³€ë™</th></tr></thead><tbody>{rows}</tbody></table>", unsafe_allow_html=True)

def render_intelligence_distribution(current_prob):
    try:
        res = supabase.table("analysis_history").select("fake_prob").execute()
        if not res.data: return
        df = pd.DataFrame(res.data)
        base = alt.Chart(df).transform_density('fake_prob', as_=['fake_prob', 'density'], extent=[0, 100], bandwidth=5).mark_area(opacity=0.3, color='#888').encode(x=alt.X('fake_prob:Q', title='ê°€ì§œë‰´ìŠ¤ í™•ë¥  ë¶„í¬'), y=alt.Y('density:Q', title='ë°€ë„'))
        rule = alt.Chart(pd.DataFrame({'x': [current_prob]})).mark_rule(color='red', size=3).encode(x='x')
        st.altair_chart(base + rule, use_container_width=True)
    except: pass

def render_report_full_ui(prob, db_count, title, channel, data, is_cached=False):
    st.divider()
    if is_cached: st.info(f"ğŸ’¾ ê³¼ê±° ë¶„ì„ ê¸°ë¡ í˜¸ì¶œë¨ (ì´ DB ë°ì´í„°: {db_count}ê°œ)")
    
    risk_text, risk_color = determine_risk_level(prob)
    
    # 1. Hero
    c1, c2, c3 = st.columns([2, 2, 1])
    with c1: st.metric("ğŸ”¥ ê°€ì§œë‰´ìŠ¤ í™•ë¥ ", f"{prob}%")
    with c2: st.markdown(f"<div style='text-align:center; padding:10px; border-radius:10px; background-color:{risk_color}; color:white; font-weight:bold; font-size:1.1rem; margin-top:5px;'>{risk_text}</div>", unsafe_allow_html=True)
    with c3: st.metric("ğŸ—„ï¸ ëˆ„ì  DB", f"{db_count}ê±´")
    
    # 2. Conclusion First
    st.subheader("ğŸ“ AI ìµœì¢… ì¢…í•© ë¦¬í¬íŠ¸")
    with st.container(border=True):
        st.markdown(f"**ğŸ“¢ AI Analyst Comment:**\n\n{data.get('final_summary', 'ë°ì´í„° ì—†ìŒ')}")

    # 3. Video Info
    st.subheader("â„¹ï¸ ì˜ìƒ ê¸°ë³¸ ì •ë³´")
    with st.container(border=True):
        st.write(f"**ğŸ“º {title}**")
        st.caption(f"ì±„ë„: {channel} | ë¶„ì„ì¼: {datetime.now().strftime('%Y-%m-%d')}")
        st.info(f"**ìš”ì•½:** {data.get('summary', 'ìš”ì•½ ì—†ìŒ')}")
        with st.expander("ìƒì„¸ ë©”íƒ€ë°ì´í„°"):
            st.dataframe(pd.DataFrame([data.get('meta', {})]), use_container_width=True, hide_index=True)

    # 4. Evidence Tabs
    st.subheader("ğŸ” ìƒì„¸ ì¦ê±° ë° ë¶„ì„ ë°ì´í„°")
    tab_news, tab_data, tab_ai = st.tabs(["ğŸ“° ë‰´ìŠ¤ íŒ©íŠ¸ì²´í¬", "ğŸ“Š ë°ì´í„°/ì—¬ë¡ ", "ğŸ¤– AI ê¸°ìˆ ì  íŒë‹¨"])
    
    with tab_news:
        st.markdown("###### ğŸ—ï¸ AI ê²€ìƒ‰ í‚¤ì›Œë“œ (3-Way Strategy)")
        if data.get('query_list'):
            st.caption("AIê°€ ì¶”ì¶œí•œ 3ê°€ì§€ ì „ëµ í‚¤ì›Œë“œ: " + " | ".join([f"`{q}`" for q in data['query_list']]))
        if data.get('query'):
            st.success(f"âœ… ë‰´ìŠ¤ ë§¤ì¹­ ì„±ê³µ í‚¤ì›Œë“œ: **{data['query']}**")
        
        st.divider()
        st.write("###### [ì¦ê±° 2] ë‰´ìŠ¤ ëŒ€ì¡° ê²°ê³¼ (Top 5)")
        if data.get('news_evidence'):
            for news in data['news_evidence']:
                with st.expander(f"{news['ì¼ì¹˜ë„']} {news['ë‰´ìŠ¤ ì œëª©']}"):
                    st.write(f"**ğŸ•µï¸ ë¶„ì„:** {news['ë¶„ì„ ê·¼ê±°']}")
                    st.caption(f"ì¶œì²˜: {news['ë¹„ê³ ']}")
                    st.link_button("ğŸ”— ì›ë¬¸ ë³´ê¸°", news['ì›ë¬¸'])
        else: st.warning("ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ")

    with tab_data:
        st.write("###### [ì¦ê±° 1] ë°ì´í„° ìœ ì‚¬ë„ ë¶„ì„")
        c1, c2 = st.columns(2)
        with c1: st.markdown(colored_bar_html("ì§„ì‹¤ ë°ì´í„° ìœ ì‚¬ë„", data.get('ts', 0), "#4CAF50"), unsafe_allow_html=True)
        with c2: st.markdown(colored_bar_html("ê°€ì§œ ë°ì´í„° ìœ ì‚¬ë„", data.get('fs', 0), "#F44336"), unsafe_allow_html=True)
        render_intelligence_distribution(prob)
        st.divider()
        st.write("###### [ì¦ê±° 3] ëŒ“ê¸€ ì—¬ë¡  ë¶„ì„")
        c1, c2, c3 = st.columns(3)
        with c1: st.metric("ëŒ“ê¸€ ìˆ˜", f"{data.get('cmt_count', 0)}")
        with c2: st.metric("ì£¼ì œ ì—°ê´€", data.get('cmt_rel', '-'))
        with c3: st.metric("ì„ ë™ ì˜ì‹¬", f"{data.get('red_cnt', 0)}")
        if data.get('top_cmt_kw'): st.write(f"ğŸ—£ï¸ **ì£¼ìš” í‚¤ì›Œë“œ:** {', '.join(data['top_cmt_kw'])}")

    with tab_ai:
        st.write("###### [ì¦ê±° 4] AI ê¸°ìˆ ì  íŒë‹¨")
        st.info(f"**ğŸ¤– Logic:**\n{data.get('ai_reason', 'íŒë‹¨ ë³´ë¥˜')}")
        st.write("###### ğŸ”¢ ì ìˆ˜ ì‚°ì • ë‚´ì—­")
        if data.get('score_breakdown'): render_score_breakdown(data['score_breakdown'])

# --- [Execution Logic] ---
def run_forensic_main(url):
    st.session_state["debug_logs"] = []
    my_bar = st.progress(0, text="ë¶„ì„ ì—”ì§„ ê°€ë™ ì¤‘...")
    db_count, _, _ = train_engine_wrapper()
    
    vid = re.search(r'(?:v=|\/)([0-9A-Za-z_-]{11}).*', url)
    if not vid: st.error("URL ì˜¤ë¥˜"); return
    vid = vid.group(1)

    with yt_dlp.YoutubeDL({'quiet':True, 'skip_download':True}) as ydl:
        try:
            info = ydl.extract_info(url, download=False)
            meta = {"ì œëª©":info.get('title'),"ì±„ë„ëª…":info.get('uploader'),"ì¡°íšŒìˆ˜":info.get('view_count',0),"ëŒ“ê¸€ìˆ˜":info.get('comment_count',0),"ì¹´í…Œê³ ë¦¬":", ".join(info.get('categories',[])),"í•´ì‹œíƒœê·¸":", ".join(info.get('tags',[])[:5])}
            
            my_bar.progress(20, "ì˜ìƒ/ìë§‰ ë¶„ì„ ì¤‘...")
            trans, _ = fetch_transcript(info)
            full_text = trans if trans else info.get('description', '')
            summary = full_text[:800] + "..."
            
            my_bar.progress(40, "í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë‰´ìŠ¤ ê²€ìƒ‰...")
            queries, _ = get_keywords(meta['ì œëª©'], full_text)
            news_items, final_query = [], queries[0]
            for q in queries:
                items = fetch_news(q)
                if items: news_items=items; final_query=q; break
            
            my_bar.progress(60, "íŒ©íŠ¸ì²´í¬ ëŒ€ì¡° ë¶„ì„...")
            news_ev, max_match = [], 0
            for item in news_items[:5]:
                s, r, src, r_url = verify_news(summary, item['link'], item['title'])
                if s > max_match: max_match = s
                icon = "ğŸŸ¢" if s>=80 else "ğŸŸ¡" if s>=60 else "ğŸ”´"
                news_ev.append({"ë‰´ìŠ¤ ì œëª©":item['title'],"ì¼ì¹˜ë„":f"{icon} {s}%","ìµœì¢… ì ìˆ˜":s,"ë¶„ì„ ê·¼ê±°":r,"ë¹„ê³ ":src,"ì›ë¬¸":r_url})
            
            cmts = fetch_comments(vid)
            top_kw, rel_score, rel_msg = analyze_comments(cmts, full_text)
            red_cnt, _ = check_red_flags(cmts)
            
            ts, fs = vector_engine.analyze(final_query+" "+meta['ì œëª©'])
            t_impact, f_impact = int(ts*30)*-1, int(fs*30)
            
            news_score = -40 if max_match>=80 else -15 if max_match>=70 else 10 if max_match>=60 else 30
            if not news_ev: news_score = 0
            if check_official(meta['ì±„ë„ëª…']): news_score = -50
            
            bait = 10 if count_agitation(meta['ì œëª©']) > 0 else -5
            base = 50 + t_impact + f_impact + news_score + min(20, red_cnt*3) + bait
            
            my_bar.progress(80, "AI ìµœì¢… íŒê²°...")
            ai_score, ai_reason = judge_final(meta['ì œëª©'], full_text, news_ev)
            final_prob = max(1, min(99, int(base*WEIGHT_ALGO + ai_score*WEIGHT_AI)))
            
            risk_text, _ = determine_risk_level(final_prob)
            final_summary = generate_comprehensive_summary(meta['ì œëª©'], final_prob, news_ev, red_cnt, ai_reason, risk_text)
            
            score_bd = [["ê¸°ë³¸ ì ìˆ˜",50,"Base"],["ì§„ì‹¤ ë°ì´í„° ìœ ì‚¬ë„",t_impact,"Vector"],["ê°€ì§œ ë°ì´í„° ìœ ì‚¬ë„",f_impact,"Vector"],["ë‰´ìŠ¤ íŒ©íŠ¸ì²´í¬",news_score,"Fact"],["ì—¬ë¡ /ì–´ê·¸ë¡œ",min(20,red_cnt*3)+bait,"Sent"],["AI íŒê²°",ai_score,"LLM"]]
            
            report = {"meta":meta,"summary":summary,"query_list":queries,"query":final_query,"score_breakdown":score_bd,"news_evidence":news_ev,"cmt_count":len(cmts),"cmt_rel":f"{rel_score}% ({rel_msg})","red_cnt":red_cnt,"top_cmt_kw":top_kw,"ai_reason":ai_reason,"ts":ts,"fs":fs,"final_summary":final_summary}
            
            save_db(meta['ì±„ë„ëª…'], meta['ì œëª©'], final_prob, url, final_query, report)
            my_bar.empty()
            render_report_full_ui(final_prob, db_count, meta['ì œëª©'], meta['ì±„ë„ëª…'], report, is_cached=False)
            
        except Exception as e: st.error(f"Error: {e}")

# --- [B2B Report] ---
def generate_b2b_report(df):
    if df.empty: return pd.DataFrame()
    df['fake_prob'] = pd.to_numeric(df['fake_prob'], errors='coerce').fillna(0)
    res = []
    for ch, g in df.groupby('channel_name'):
        avg = g['fake_prob'].mean()
        kws = []
        for k in g['keywords']:
            if isinstance(k, list): kws.extend([str(x) for x in k])
            elif k: kws.append(str(k))
        tokens = re.findall(r'[ê°€-í£]{2,}', " ".join(kws))
        target = ", ".join([t[0] for t in Counter(tokens).most_common(3)])
        grade = "â›” BLACKLIST" if avg>=60 else "âš ï¸ CAUTION" if avg>=40 else "âœ… SAFE"
        res.append({"Channel":ch, "Grade":grade, "Avg Risk":f"{int(avg)}%", "Videos":len(g), "Target":target})
    return pd.DataFrame(res).sort_values("Avg Risk", ascending=False)

# --- [Layout Main] ---
st.title("âš–ï¸ìœ íŠœë¸Œ ê°€ì§œë‰´ìŠ¤ íŒë…ê¸° (Triple Engine)")

with st.container(border=True):
    with st.expander("â„¹ï¸ ì„œë¹„ìŠ¤ ì´ìš© ì•ˆë‚´ ë° ë©´ì±… ì¡°í•­ (Disclaimer)"):
        st.markdown("""
        ë³¸ ì„œë¹„ìŠ¤ëŠ” **ì¸ê³µì§€ëŠ¥(AI) ë° ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜**ìœ¼ë¡œ ì˜ìƒì˜ ì‹ ë¢°ë„ë¥¼ ë¶„ì„í•˜ëŠ” ë³´ì¡° ë„êµ¬ì…ë‹ˆë‹¤. 
        **ë¶„ì„ ê²°ê³¼ëŠ” ì–´ë– í•œ ë²•ì  íš¨ë ¥ë„ ì—†ìœ¼ë©°, ìµœì¢… íŒë‹¨ê³¼ ì±…ì„ì€ ì „ì ìœ¼ë¡œ ì‚¬ìš©ì(ë‹¹ì‚¬ì)ì—ê²Œ ìˆìŠµë‹ˆë‹¤.**
        """)
    agree = st.checkbox("ìœ„ ê³ ì§€ ë‚´ìš©ì„ í™•ì¸í•˜ì˜€ìœ¼ë©°, ê²°ê³¼ì— ëŒ€í•œ ìµœì¢… ì±…ì„ì´ ë³¸ì¸ì—ê²Œ ìˆìŒì„ ë™ì˜í•©ë‹ˆë‹¤.")

url = st.text_input("ğŸ”— YouTube URL ì…ë ¥")
if st.button("ğŸš€ ì •ë°€ ë¶„ì„ ì‹œì‘", use_container_width=True, disabled=not agree):
    if url: run_forensic_main(url)
    else: st.warning("URLì„ ì…ë ¥í•˜ì„¸ìš”.")

# --- [Bottom Section] ---
st.divider()
st.subheader("ğŸ—‚ï¸ DB History")

if st.session_state["is_admin"]:
    st.caption("âœ… ê´€ë¦¬ì ëª¨ë“œ: ì‚­ì œ ê°€ëŠ¥")
else:
    st.caption("ğŸ”’ ë·°ì–´ ëª¨ë“œ: ì¡°íšŒë§Œ ê°€ëŠ¥")

try:
    # [ìˆ˜ì •] í…Œì´ë¸” ì´ë¦„ì„ 'analysis_history'ë¡œ ì›ìƒë³µêµ¬
    response = supabase.table("analysis_history").select("*").order("id", desc=True).execute()
    data = response.data
    if not data: st.info("ğŸ“­ ì €ì¥ëœ ë¶„ì„ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        df_hist = pd.DataFrame(data)
        if st.session_state["is_admin"]:
            if "Delete" not in df_hist.columns: df_hist.insert(0, "Delete", False)
            edited_df = st.data_editor(df_hist, hide_index=True, use_container_width=True, column_config={"Delete": st.column_config.CheckboxColumn("ì‚­ì œ", default=False), "fake_prob": st.column_config.NumberColumn("ê°€ì§œ í™•ë¥ ", format="%d%%"), "video_url": st.column_config.LinkColumn("URL"), "detail_json": None, "vector_json": None}, disabled=["id", "analysis_date", "channel_name", "video_title", "fake_prob", "keywords", "video_url"])
            if st.button("ğŸ—‘ï¸ ì„ íƒ í•­ëª© ì˜êµ¬ ì‚­ì œ", type="primary"):
                to_delete = edited_df[edited_df['Delete'] == True]
                if not to_delete.empty:
                    # [ìˆ˜ì •] í…Œì´ë¸” ì´ë¦„ì„ 'analysis_history'ë¡œ ì›ìƒë³µêµ¬
                    for index, row in to_delete.iterrows(): supabase.table("analysis_history").delete().eq("id", row['id']).execute()
                    st.success("ì‚­ì œ ì™„ë£Œ!"); time.sleep(1); st.rerun()
        else: st.dataframe(df_hist[['analysis_date','channel_name','video_title','fake_prob']], use_container_width=True, hide_index=True)
except Exception as e: st.error(f"âŒ DB Error: {e}")

st.divider()
with st.expander("ğŸ” ê´€ë¦¬ì (Admin & B2B Report)"):
    if st.session_state["is_admin"]:
        st.success("Admin Logged In (Target: analysis_archive_v2)")
        
        st.write("### ğŸš‘ ë°ì´í„° ì´ì‚¬ (ìµœì¢…_ì§„ì§œ_ë§ˆì§€ë§‰.ver)")
        uploaded_file = st.file_uploader("ë°±ì—… íŒŒì¼(export.csv)ì„ ì—¬ê¸°ì— ì˜¬ë¦¬ì„¸ìš”", type="csv")
        
        if uploaded_file is not None:
            if st.button("ğŸš¨ ìƒˆ DBë¡œ ë³µêµ¬ ì‹œì‘", type="primary"):
                
                # 1. íŒŒì¼ ì½ê¸°
                try:
                    df_restore = pd.read_csv(uploaded_file)
                    st.info(f"ğŸ“‚ íŒŒì¼ ì½ê¸° ì„±ê³µ: {len(df_restore)}ê°œ ë°ì´í„° ëŒ€ê¸° ì¤‘...")
                except Exception as e:
                    st.error(f"íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
                    st.stop()

                restore_bar = st.progress(0)
                success_cnt = 0
                fail_cnt = 0
                
                # 2. ë°”ë¡œ ë°ì´í„° ì£¼ì… ì‹œì‘ (ì“¸ë°ì—†ëŠ” ì¡°íšŒ ì½”ë“œ ì‚­ì œ)
                for i, row in df_restore.iterrows():
                    # ì œëª© ì—†ëŠ” ë°ì´í„° íŒ¨ìŠ¤
                    title = str(row.get('video_title', ''))
                    if title == 'nan' or not title: continue
                    
                    # ë°ì´í„° ë§¤í•‘
                    restore_data = {
                        "analysis_date": str(row.get('analysis_date', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))),
                        "channel_name": str(row.get('channel_name', 'Unknown')),
                        "video_title": title,
                        "fake_prob": int(row['fake_prob']) if pd.notna(row.get('fake_prob')) else 0,
                        "video_url": str(row.get('video_url', '')),
                        "keywords": str(row.get('keywords', '')),
                        "detail_json": {"final_summary": "ë³µêµ¬ëœ ë°ì´í„°"},
                        "vector_json": None 
                    }
                    
                    try:
                        # ì €ì¥ ì‹œë„
                        supabase.table("analysis_archive_v2").insert(restore_data).execute()
                        success_cnt += 1
                        
                    except Exception as e:
                        fail_cnt += 1
                        # ì—ëŸ¬ ë°œìƒ ì‹œ ì¦‰ì‹œ í™”ë©´ì— ì¶œë ¥í•˜ê³  ë©ˆì¶¤
                        if fail_cnt == 1:
                            st.error(f"ğŸš¨ ì €ì¥ ì‹¤íŒ¨! (ì²« ë²ˆì§¸ ë°ì´í„°)")
                            st.error(f"ì—ëŸ¬ ë©”ì‹œì§€: {e}")
                            st.write("ë„£ìœ¼ë ¤ê³  í–ˆë˜ ë°ì´í„°:")
                            st.json(restore_data)
                            st.stop()
                    
                    restore_bar.progress(int(((i + 1) / len(df_restore)) * 100))
                
                # ê²°ê³¼
                st.write("---")
                if success_cnt > 0:
                    st.success(f"âœ… {success_cnt}ê±´ ì™„ë²½í•˜ê²Œ ë³µêµ¬ ì„±ê³µ!")
                    st.balloons()
                    st.info("ì´ì œ ì•„ë˜ [ë°ì´í„° ì—…ë°ì´íŠ¸] ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”!")
                else:
                    st.error("âŒ 0ê±´ ì €ì¥ë¨.")

        st.write("---")

        # 3. ë°ì´í„° ì—…ë°ì´íŠ¸ (ìƒˆ í…Œì´ë¸” ê¸°ì¤€)
        st.write("### ğŸ”§ ì‹œìŠ¤í…œ ê´€ë¦¬")
        try:
            # ì—¬ê¸°ë„ .head() ê°™ì€ ê±° ì•ˆ ì“°ê³  ì•ˆì „í•˜ê²Œ ì¡°íšŒ
            res = supabase.table("analysis_archive_v2").select("id", count='exact').is_("vector_json", "null").execute()
            missing_count = res.count
        except: missing_count = 0

        if missing_count > 0:
            st.warning(f"âš ï¸ í•™ìŠµ ë¯¸ë°˜ì˜ ë°ì´í„° {missing_count}ê±´")
            if st.button(f"â™»ï¸ ë°ì´í„° ì—…ë°ì´íŠ¸ ({missing_count}ê±´)"):
                prog_text = st.empty()
                bar = st.progress(0)
                old_rows = supabase.table("analysis_archive_v2").select("*").is_("vector_json", "null").execute().data
                
                for i, row in enumerate(old_rows):
                    txt = f"{row.get('keywords','')} {row.get('video_title','')}"
                    try:
                        vec = vector_engine.get_embedding(txt)
                        supabase.table("analysis_archive_v2").update({"vector_json": vec}).eq("id", row['id']).execute()
                    except: continue
                    bar.progress(int(((i+1)/missing_count)*100))
                    prog_text.text(f"ì²˜ë¦¬ ì¤‘... {i+1}/{missing_count}")
                    time.sleep(0.5)
                st.success("ì™„ë£Œ!")
                time.sleep(1)
                st.rerun()
        else:
            st.success("âœ… ëª¨ë“  ë°ì´í„°ê°€ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤.")

        if st.button("Logout"): st.session_state["is_admin"]=False; st.rerun()
    else:
        pwd = st.text_input("Password", type="password")
        if st.button("Login"):
            if pwd == ADMIN_PASSWORD: st.session_state["is_admin"]=True; st.rerun()
            else: st.error("Wrong Password")


