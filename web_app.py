import streamlit as st
from supabase import create_client, Client
import re
import requests
import time
import random
import math
import os
import json
from collections import Counter
from datetime import datetime

# --- [ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸] ---
from mistralai import Mistral
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
import yt_dlp
import pandas as pd
import altair as alt
from bs4 import BeautifulSoup

# --- [1. ì‹œìŠ¤í…œ ì„¤ì •] ---
st.set_page_config(page_title="ìœ íŠœë¸Œ ê°€ì§œë‰´ìŠ¤ íŒë…ê¸° (Triple Engine)", layout="wide", page_icon="ğŸ›¡ï¸")

if "is_admin" not in st.session_state:
Â  Â  st.session_state["is_admin"] = False
if "debug_logs" not in st.session_state:
Â  Â  st.session_state["debug_logs"] = []

# ğŸŒŸ Secrets ë¡œë“œ (3ì¤‘ í‚¤ ë¡œë“œ)
try:
Â  Â  YOUTUBE_API_KEY = st.secrets["YOUTUBE_API_KEY"]
Â  Â  SUPABASE_URL = st.secrets["SUPABASE_URL"]
Â  Â  SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
Â  Â  ADMIN_PASSWORD = st.secrets["ADMIN_PASSWORD"]
Â  Â Â 
Â  Â  MISTRAL_API_KEY = st.secrets["MISTRAL_API_KEY"]
Â  Â  GOOGLE_API_KEY_A = st.secrets["GOOGLE_API_KEY_A"]
Â  Â  GOOGLE_API_KEY_B = st.secrets["GOOGLE_API_KEY_B"]
except:
Â  Â  st.error("âŒ secrets.toml íŒŒì¼ì— API Key ì„¤ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. (Mistral, Google A, Google B)")
Â  Â  st.stop()

@st.cache_resource
def init_clients():
Â  Â  # Supabase & Mistral (GeminiëŠ” í˜¸ì¶œ ì‹œë§ˆë‹¤ í‚¤ ë³€ê²½)
Â  Â  su = create_client(SUPABASE_URL, SUPABASE_KEY)
Â  Â  mi = Mistral(api_key=MISTRAL_API_KEY)
Â  Â  return su, mi

supabase, mistral_client = init_clients()

# --- [2. ëª¨ë¸ ì •ì˜] ---
# Mistral ìš°ì„ ìˆœìœ„ ë¦¬ìŠ¤íŠ¸
MISTRAL_MODELS = [
Â  Â  "mistral-large-latest",
Â  Â  "mistral-medium-latest",
Â  Â  "mistral-small-latest",
Â  Â  "open-mixtral-8x22b"
]

# Gemini ëª¨ë¸ íƒìƒ‰ í•¨ìˆ˜ (í‚¤ ë³„ë¡œ ë™ì‘)
def get_gemini_models_dynamic(api_key):
Â  Â  """íŠ¹ì • API Keyë¡œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´"""
Â  Â  genai.configure(api_key=api_key)
Â  Â  try:
Â  Â  Â  Â  models = []
Â  Â  Â  Â  for m in genai.list_models():
Â  Â  Â  Â  Â  Â  if 'generateContent' in m.supported_generation_methods:
Â  Â  Â  Â  Â  Â  Â  Â  name = m.name.replace("models/", "")
Â  Â  Â  Â  Â  Â  Â  Â  models.append(name)
Â  Â  Â  Â  # ì„±ëŠ¥ìˆœ ì •ë ¬
Â  Â  Â  Â  models.sort(key=lambda x: 0 if 'flash' in x else 1 if 'pro' in x else 2)
Â  Â  Â  Â  return models
Â  Â  except:
Â  Â  Â  Â  return ["gemini-2.0-flash", "gemini-1.5-flash", "gemini-1.5-pro"] # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’

# --- [3. ìœ í‹¸ë¦¬í‹°] ---
def parse_llm_json(text):
Â  Â  try:
Â  Â  Â  Â  parsed = json.loads(text)
Â  Â  except:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  text = re.sub(r'```json\s*', '', text)
Â  Â  Â  Â  Â  Â  text = re.sub(r'```', '', text)
Â  Â  Â  Â  Â  Â  match = re.search(r'(\{.*\}|\[.*\])', text, re.DOTALL)
Â  Â  Â  Â  Â  Â  if match: parsed = json.loads(match.group(1))
Â  Â  Â  Â  Â  Â  else: return None
Â  Â  Â  Â  except: return None
Â  Â  if isinstance(parsed, list): return parsed[0] if len(parsed) > 0 and isinstance(parsed[0], dict) else None
Â  Â  if isinstance(parsed, dict): return parsed
Â  Â  return None

# --- [4. â­ Triple Hybrid Survivor Logic] ---
def call_triple_survivor(prompt, is_json=False):
Â  Â  logs = []
Â  Â Â 
Â  Â  # === [Phase 1: Mistral AI (1ì„ ë°œ)] ===
Â  Â  response_format = {"type": "json_object"} if is_json else None
Â  Â  for model_name in MISTRAL_MODELS:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  resp = mistral_client.chat.complete(
Â  Â  Â  Â  Â  Â  Â  Â  model=model_name,
Â  Â  Â  Â  Â  Â  Â  Â  messages=[{"role": "user", "content": prompt}],
Â  Â  Â  Â  Â  Â  Â  Â  response_format=response_format,
Â  Â  Â  Â  Â  Â  Â  Â  temperature=0.2
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  if resp.choices:
Â  Â  Â  Â  Â  Â  Â  Â  content = resp.choices[0].message.content
Â  Â  Â  Â  Â  Â  Â  Â  logs.append(f"âœ… Success (Mistral): {model_name}")
Â  Â  Â  Â  Â  Â  Â  Â  return content, f"{model_name}", logs
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  logs.append(f"âŒ Mistral Failed ({model_name}): {str(e)[:30]}...")
Â  Â  Â  Â  Â  Â  time.sleep(0.2)
Â  Â  Â  Â  Â  Â  continue

Â  Â  # === [Phase 2: Google Gemini Key A (2ì„ ë°œ)] ===
Â  Â  logs.append("âš ï¸ Mistral ì „ë©¸ -> Gemini Key A íˆ¬ì…")
Â  Â  models_a = get_gemini_models_dynamic(GOOGLE_API_KEY_A)
Â  Â Â 
Â  Â  generation_config = {"response_mime_type": "application/json"} if is_json else {}
Â  Â  safety_settings = {
Â  Â  Â  Â  HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
Â  Â  Â  Â  HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
Â  Â  Â  Â  HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
Â  Â  Â  Â  HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
Â  Â  }

Â  Â  # í‚¤ A ì„¤ì • (ì¤‘ìš”: ì¬ì„¤ì •)
Â  Â  genai.configure(api_key=GOOGLE_API_KEY_A)
Â  Â Â 
Â  Â  for model_name in models_a:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  model = genai.GenerativeModel(model_name, generation_config=generation_config)
Â  Â  Â  Â  Â  Â  resp = model.generate_content(prompt, safety_settings=safety_settings)
Â  Â  Â  Â  Â  Â  if resp.text:
Â  Â  Â  Â  Â  Â  Â  Â  logs.append(f"âœ… Success (Gemini Key A): {model_name}")
Â  Â  Â  Â  Â  Â  Â  Â  return resp.text, f"{model_name} (Key A)", logs
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  continue

Â  Â  # === [Phase 3: Google Gemini Key B (ìµœí›„ì˜ ë³´ë£¨)] ===
Â  Â  logs.append("âš ï¸ Key A ì „ë©¸ -> Gemini Key B íˆ¬ì… (Final Stand)")
Â  Â Â 
Â  Â  # í‚¤ B ì„¤ì • (ì¤‘ìš”: ì¬ì„¤ì •)
Â  Â  genai.configure(api_key=GOOGLE_API_KEY_B)
Â  Â  models_b = get_gemini_models_dynamic(GOOGLE_API_KEY_B) # ëª¨ë¸ ë¦¬ìŠ¤íŠ¸ ë‹¤ì‹œ í™•ë³´
Â  Â Â 
Â  Â  for model_name in models_b:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  model = genai.GenerativeModel(model_name, generation_config=generation_config)
Â  Â  Â  Â  Â  Â  resp = model.generate_content(prompt, safety_settings=safety_settings)
Â  Â  Â  Â  Â  Â  if resp.text:
Â  Â  Â  Â  Â  Â  Â  Â  logs.append(f"âœ… Success (Gemini Key B): {model_name}")
Â  Â  Â  Â  Â  Â  Â  Â  return resp.text, f"{model_name} (Key B)", logs
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  continue

Â  Â  return None, "All Failed (Mistral + Key A + Key B)", logs

# --- [5. ìƒìˆ˜ ë° ë°ì´í„°] ---
# ë°¸ëŸ°ìŠ¤: Algo 85% : AI 15%
WEIGHT_ALGO = 0.85
WEIGHT_AI = 0.15

OFFICIAL_CHANNELS = ['MBC', 'KBS', 'SBS', 'EBS', 'YTN', 'JTBC', 'TVCHOSUN', 'MBN', 'CHANNEL A', 'OBS', 'ì±„ë„A', 'TVì¡°ì„ ', 'ì—°í•©ë‰´ìŠ¤', 'YONHAP', 'í•œê²¨ë ˆ', 'ê²½í–¥', 'ì¡°ì„ ', 'ì¤‘ì•™', 'ë™ì•„']
CRITICAL_STATE_KEYWORDS = ['ë³„ê±°', 'ì´í˜¼', 'íŒŒê²½', 'ì‚¬ë§', 'ìœ„ë…', 'êµ¬ì†', 'ì²´í¬', 'ì‹¤í˜•', 'ë¶ˆí™”', 'í­ë¡œ', 'ì¶©ê²©', 'ë…¼ë€', 'ì¤‘íƒœ', 'ì‹¬ì •ì§€', 'ë‡Œì‚¬', 'ì••ìˆ˜ìˆ˜ìƒ‰', 'ì†Œí™˜', 'íŒŒì‚°', 'ë¹šë”ë¯¸', 'ì „ê³¼', 'ê°ì˜¥', 'ê°„ì²©']

STATIC_TRUTH_CORPUS = ["ë°•ë‚˜ë˜ ìœ„ì¥ì „ì… ë¬´í˜ì˜", "ì„ì˜ì›… ì•”í‘œ ëŒ€ì‘", "ì •í¬ì› ì €ì†ë…¸í™”", "ëŒ€ì „ ì¶©ë‚¨ í†µí•©", "ì„ ê±° ì¶œë§ˆ ì„ ì–¸"]
STATIC_FAKE_CORPUS = ["ì¶©ê²© í­ë¡œ ê²½ì•…", "ê¸´ê¸‰ ì†ë³´ ì†Œë¦„", "ì¶©ê²© ë°œì–¸ ë…¼ë€", "êµ¬ì† ì˜ì¥ ë°œë¶€", "ì˜ìƒ ìœ ì¶œ", "ê³„ì‹œ ì˜ˆì–¸", "ì‚¬í˜• ì§‘í–‰", "ìœ„ë…ì„¤"]

class VectorEngine:
Â  Â  def __init__(self):
Â  Â  Â  Â  self.vocab = set(); self.truth_vectors = []; self.fake_vectors = []
Â  Â  def tokenize(self, text): return re.findall(r'[ê°€-í£]{2,}', text)
Â  Â  def train(self, truth, fake):
Â  Â  Â  Â  for t in truth + fake: self.vocab.update(self.tokenize(t))
Â  Â  Â  Â  self.vocab = sorted(list(self.vocab))
Â  Â  Â  Â  self.truth_vectors = [self.text_to_vector(t) for t in truth]
Â  Â  Â  Â  self.fake_vectors = [self.text_to_vector(t) for t in fake]
Â  Â  def text_to_vector(self, text, vocabulary=None):
Â  Â  Â  Â  target_vocab = vocabulary if vocabulary else self.vocab
Â  Â  Â  Â  c = Counter(self.tokenize(text))
Â  Â  Â  Â  return [c[w] for w in target_vocab]
Â  Â  def cosine_similarity(self, v1, v2):
Â  Â  Â  Â  dot = sum(a*b for a,b in zip(v1,v2))
Â  Â  Â  Â  mag = math.sqrt(sum(a*a for a in v1)) * math.sqrt(sum(b*b for b in v2))
Â  Â  Â  Â  return dot/mag if mag>0 else 0
Â  Â  def analyze_position(self, query):
Â  Â  Â  Â  qv = self.text_to_vector(query)
Â  Â  Â  Â  mt = max([self.cosine_similarity(qv, v) for v in self.truth_vectors] or [0])
Â  Â  Â  Â  mf = max([self.cosine_similarity(qv, v) for v in self.fake_vectors] or [0])
Â  Â  Â  Â  return mt, mf

vector_engine = VectorEngine()

# [Engine A] ìˆ˜ì‚¬ê´€
def get_hybrid_search_keywords(title, transcript):
Â  Â  context_data = transcript[:15000]Â 
Â  Â  prompt = f"""
Â  Â  You are a Fact-Check Investigator.
Â  Â  [Input] Title: {title}, Transcript: {context_data}
Â  Â  [Task] Extract ONE precise Google News search query.
Â  Â  [Rules] Focus on Proper Nouns (Person, Drug, Event). Ignore Generic Verbs.
Â  Â  [Output] ONLY the Korean search query string (2-4 words). Do not add quotes.
Â  Â  """
Â  Â  result_text, model_used, logs = call_triple_survivor(prompt)
Â  Â  st.session_state["debug_logs"].extend([f"[Key A] {l}" for l in logs])
Â  Â  return (result_text.strip(), f"âœ¨ {model_used}") if result_text else (title, "âŒ Error")

# [í¬ë¡¤ëŸ¬] ë‰´ìŠ¤ ë³¸ë¬¸ ìˆ˜ì§‘
def scrape_news_content_robust(google_url):
Â  Â  try:
Â  Â  Â  Â  session = requests.Session()
Â  Â  Â  Â  session.headers.update({'User-Agent': 'Mozilla/5.0'})
Â  Â  Â  Â  response = session.get(google_url, timeout=5, allow_redirects=True)
Â  Â  Â  Â  final_url = response.url
Â  Â  Â  Â  soup = BeautifulSoup(response.text, 'html.parser')
Â  Â  Â  Â  for tag in soup(['script', 'style', 'nav', 'footer', 'iframe']): tag.decompose()
Â  Â  Â  Â  text = " ".join([p.get_text().strip() for p in soup.find_all('p') if len(p.get_text().strip()) > 30])
Â  Â  Â  Â  return (text[:4000], final_url) if len(text) > 100 else (None, final_url)
Â  Â  except: return None, google_url

# [Engine B] ë‰´ìŠ¤ ì •ë°€ ëŒ€ì¡°
def deep_verify_news(video_summary, news_url, news_snippet):
Â  Â  scraped_text, real_url = scrape_news_content_robust(news_url)
Â  Â  evidence_text = scraped_text if scraped_text else news_snippet
Â  Â  source_type = "Full Article" if scraped_text else "Snippet Only"
Â  Â Â 
Â  Â  prompt = f"""
Â  Â  Compare Video Summary vs News Evidence.
Â  Â  [Video] {video_summary[:2000]}
Â  Â  [News ({source_type})] {evidence_text}
Â  Â  [Task] Does news confirm video claim? Match(90-100), Related(40-60), Mismatch(0-10).
Â  Â  [Output JSON] {{ "score": <int>, "reason": "<short korean reason>" }}
Â  Â  """
Â  Â  result_text, model_used, logs = call_triple_survivor(prompt, is_json=True)
Â  Â  st.session_state["debug_logs"].extend([f"[Verify] {l}" for l in logs])
Â  Â Â 
Â  Â  res = parse_llm_json(result_text)
Â  Â  if res: return res.get('score', 0), res.get('reason', 'N/A'), source_type, evidence_text, real_url
Â  Â  return 0, "Error", "Error", "", news_url

# [Engine B] ìµœì¢… íŒê²°
def get_hybrid_verdict_final(title, transcript, verified_news_list):
Â  Â  news_summary = ""
Â  Â  for item in verified_news_list:
Â  Â  Â  Â  news_summary += f"- News: {item['ë‰´ìŠ¤ ì œëª©']} (Score: {item['ìµœì¢… ì ìˆ˜']}, Reason: {item['ë¶„ì„ ê·¼ê±°']})\n"
Â  Â Â 
Â  Â  full_context = transcript[:30000]
Â  Â  prompt = f"""
Â  Â  You are a Fact-Check Judge.
Â  Â  [Video] {title} / {full_context[:2000]}...
Â  Â  [Evidence] {news_summary}
Â  Â  [Instruction] Verify truth. Match->Truth(0-30), Mismatch->Fake(70-100).Â 
Â  Â  Output JSON format only: {{ "score": <int>, "reason": "<korean explanation>" }}
Â  Â  """
Â  Â  result_text, model_used, logs = call_triple_survivor(prompt, is_json=True)
Â  Â  st.session_state["debug_logs"].extend([f"[Judge] {l}" for l in logs])
Â  Â Â 
Â  Â  res = parse_llm_json(result_text)
Â  Â  if res: return res.get('score', 50), f"{res.get('reason')} (By {model_used})"
Â  Â  return 50, "Judge Failed"

# --- [6. ìœ í‹¸ë¦¬í‹°] ---
def normalize_korean_word(word):
Â  Â  word = re.sub(r'[^ê°€-í£0-9]', '', word)
Â  Â  for j in ['ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ì˜','ì—','ì—ê²Œ','ë¡œ','ìœ¼ë¡œ']:
Â  Â  Â  Â  if word.endswith(j): return word[:-len(j)]
Â  Â  return word

def extract_meaningful_tokens(text):
Â  Â  raw = re.findall(r'[ê°€-í£]{2,}', text)
Â  Â  noise = ['ì¶©ê²©','ì†ë³´','ê¸´ê¸‰','ì˜¤ëŠ˜','ì§€ê¸ˆ','ê²°êµ­','ë‰´ìŠ¤','ì˜ìƒ']
Â  Â  return [normalize_korean_word(w) for w in raw if w not in noise]

def extract_top_keywords_from_transcript(text, top_n=5):
Â  Â  if not text: return []
Â  Â  tokens = extract_meaningful_tokens(text)
Â  Â  return Counter(tokens).most_common(top_n)

def train_dynamic_vector_engine():
Â  Â  try:
Â  Â  Â  Â  res_t = supabase.table("analysis_history").select("video_title").lt("fake_prob", 40).execute()
Â  Â  Â  Â  res_f = supabase.table("analysis_history").select("video_title").gt("fake_prob", 60).execute()
Â  Â  Â  Â  dt = [row['video_title'] for row in res_t.data] if res_t.data else []
Â  Â  Â  Â  df = [row['video_title'] for row in res_f.data] if res_f.data else []
Â  Â  Â  Â  vector_engine.train(STATIC_TRUTH_CORPUS + dt, STATIC_FAKE_CORPUS + df)
Â  Â  Â  Â  return len(dt)+len(df), dt, df
Â  Â  except:Â 
Â  Â  Â  Â  vector_engine.train(STATIC_TRUTH_CORPUS, STATIC_FAKE_CORPUS)
Â  Â  Â  Â  return 0, [], []

def save_analysis(channel, title, prob, url, keywords):
Â  Â  try: supabase.table("analysis_history").insert({"channel_name": channel, "video_title": title, "fake_prob": prob, "analysis_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'), "video_url": url, "keywords": keywords}).execute()
Â  Â  except: pass

def render_intelligence_distribution(current_prob):
Â  Â  try:
Â  Â  Â  Â  res = supabase.table("analysis_history").select("fake_prob").execute()
Â  Â  Â  Â  if not res.data: return
Â  Â  Â  Â  df = pd.DataFrame(res.data)
Â  Â  Â  Â  base = alt.Chart(df).transform_density('fake_prob', as_=['fake_prob', 'density'], extent=[0, 100], bandwidth=5).mark_area(opacity=0.3, color='#888').encode(x=alt.X('fake_prob:Q', title='ê°€ì§œë‰´ìŠ¤ í™•ë¥  ë¶„í¬'), y=alt.Y('density:Q', title='ë°ì´í„° ë°€ë„'))
Â  Â  Â  Â  rule = alt.Chart(pd.DataFrame({'x': [current_prob]})).mark_rule(color='blue', size=3).encode(x='x')
Â  Â  Â  Â  st.altair_chart(base + rule, use_container_width=True)
Â  Â  except: pass

def colored_progress_bar(label, percent, color):
Â  Â  st.markdown(f"""<div style="margin-bottom: 10px;"><div style="display: flex; justify-content: space-between; margin-bottom: 3px;"><span style="font-size: 13px; font-weight: 600; color: #555;">{label}</span><span style="font-size: 13px; font-weight: 700; color: {color};">{round(percent * 100, 1)}%</span></div><div style="background-color: #eee; border-radius: 5px; height: 8px; width: 100%;"><div style="background-color: {color}; height: 8px; width: {percent * 100}%; border-radius: 5px;"></div></div></div>""", unsafe_allow_html=True)

def render_score_breakdown(data_list):
Â  Â  style = """<style>table.score-table { width: 100%; border-collapse: separate; border-spacing: 0; border: 1px solid #e0e0e0; border-radius: 8px; overflow: hidden; font-family: sans-serif; font-size: 14px; margin-top: 10px;} table.score-table th { background-color: #f8f9fa; color: #495057; font-weight: bold; padding: 12px 15px; text-align: left; border-bottom: 1px solid #e0e0e0; } table.score-table td { padding: 12px 15px; border-bottom: 1px solid #f0f0f0; color: #333; } table.score-table tr:last-child td { border-bottom: none; } .badge { padding: 4px 8px; border-radius: 6px; font-weight: 700; font-size: 11px; display: inline-block; text-align: center; min-width: 45px; } .badge-danger { background-color: #ffebee; color: #d32f2f; } .badge-success { background-color: #e8f5e9; color: #2e7d32; } .badge-neutral { background-color: #f5f5f5; color: #757575; border: 1px solid #e0e0e0; }</style>"""
Â  Â  rows = ""
Â  Â  for item, score, note in data_list:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  score_num = int(score)
Â  Â  Â  Â  Â  Â  badge = f'<span class="badge badge-danger">+{score_num}</span>' if score_num > 0 else f'<span class="badge badge-success">{score_num}</span>' if score_num < 0 else f'<span class="badge badge-neutral">0</span>'
Â  Â  Â  Â  except: badge = f'<span class="badge badge-neutral">{score}</span>'
Â  Â  Â  Â  rows += f"<tr><td>{item}<br><span style='color:#888; font-size:11px;'>{note}</span></td><td style='text-align: right;'>{badge}</td></tr>"
Â  Â  st.markdown(f"{style}<table class='score-table'><thead><tr><th>ë¶„ì„ í•­ëª© (Score Breakdown)</th><th style='text-align: right;'>ë³€ë™</th></tr></thead><tbody>{rows}</tbody></table>", unsafe_allow_html=True)

def summarize_transcript(text, title):
Â  Â  return text[:800] + "..." if len(text) > 800 else text

def clean_html_regex(text):
Â  Â  return re.sub('<.*?>', '', text).strip()

def detect_ai_content(info):
Â  Â  is_ai, reasons = False, []
Â  Â  text = (info.get('title', '') + " " + info.get('description', '') + " " + " ".join(info.get('tags', []))).lower()
Â  Â  for kw in ['ai', 'artificial intelligence', 'chatgpt', 'deepfake', 'synthetic', 'ì¸ê³µì§€ëŠ¥', 'ë”¥í˜ì´í¬']:
Â  Â  Â  Â  if kw in text: is_ai = True; reasons.append(f"í‚¤ì›Œë“œ ê°ì§€: {kw}"); break
Â  Â  return is_ai, ", ".join(reasons)

def check_is_official(channel_name):
Â  Â  norm_name = channel_name.upper().replace(" ", "")
Â  Â  return any(o in norm_name for o in OFFICIAL_CHANNELS)

def count_sensational_words(text):
Â  Â  return sum(text.count(w) for w in ['ì¶©ê²©', 'ê²½ì•…', 'ì‹¤ì²´', 'í­ë¡œ', 'ë‚œë¦¬', 'ì†ë³´', 'ê¸´ê¸‰', 'ì†Œë¦„', 'ã„·ã„·'])

def check_tag_abuse(title, hashtags, channel_name):
Â  Â  if check_is_official(channel_name): return 0, "ê³µì‹ ì±„ë„ ë©´ì œ"
Â  Â  if not hashtags: return 0, "í•´ì‹œíƒœê·¸ ì—†ìŒ"
Â  Â  return 0, "ì–‘í˜¸"

def fetch_real_transcript(info_dict):
Â  Â  try:
Â  Â  Â  Â  url = None
Â  Â  Â  Â  subs = info_dict.get('subtitles') or {}
Â  Â  Â  Â  auto = info_dict.get('automatic_captions') or {}
Â  Â  Â  Â  merged = {**subs, **auto}
Â  Â  Â  Â  if 'ko' in merged:
Â  Â  Â  Â  Â  Â  for fmt in merged['ko']:
Â  Â  Â  Â  Â  Â  Â  Â  if fmt['ext'] == 'vtt': url = fmt['url']; break
Â  Â  Â  Â  if url:
Â  Â  Â  Â  Â  Â  res = requests.get(url)
Â  Â  Â  Â  Â  Â  if res.status_code == 200:
Â  Â  Â  Â  Â  Â  Â  Â  lines = [l.strip() for l in res.text.splitlines() if l.strip() and '-->' not in l and '<' not in l]
Â  Â  Â  Â  Â  Â  Â  Â  return " ".join(lines), "Success"
Â  Â  except: pass
Â  Â  return None, "Fail"

def fetch_comments_via_api(video_id):
Â  Â  try:
Â  Â  Â  Â  url = "https://www.googleapis.com/youtube/v3/commentThreads"
Â  Â  Â  Â  res = requests.get(url, params={'part': 'snippet', 'videoId': video_id, 'key': YOUTUBE_API_KEY, 'maxResults': 50})
Â  Â  Â  Â  if res.status_code == 200:
Â  Â  Â  Â  Â  Â  data = res.json()
Â  Â  Â  Â  Â  Â  items = []
Â  Â  Â  Â  Â  Â  for i in data.get('items', []):
Â  Â  Â  Â  Â  Â  Â  Â  snippet = i.get('snippet', {}).get('topLevelComment', {}).get('snippet', {})
Â  Â  Â  Â  Â  Â  Â  Â  if 'textDisplay' in snippet: items.append(snippet['textDisplay'])
Â  Â  Â  Â  Â  Â  return items, "Success"
Â  Â  except: pass
Â  Â  return [], "Fail"

def fetch_news_regex(query):
Â  Â  news_res = []
Â  Â  try:
Â  Â  Â  Â  rss = f"https://news.google.com/rss/search?q={requests.utils.quote(query)}&hl=ko&gl=KR"
Â  Â  Â  Â  raw = requests.get(rss, timeout=5).text
Â  Â  Â  Â  items = re.findall(r'<item>(.*?)</item>', raw, re.DOTALL)
Â  Â  Â  Â  for item in items[:10]:
Â  Â  Â  Â  Â  Â  t = re.search(r'<title>(.*?)</title>', item)
Â  Â  Â  Â  Â  Â  d = re.search(r'<description>(.*?)</description>', item)
Â  Â  Â  Â  Â  Â  l = re.search(r'<link>(.*?)</link>', item)
Â  Â  Â  Â  Â  Â  if t and l:
Â  Â  Â  Â  Â  Â  Â  Â  nt = t.group(1).replace("<![CDATA[", "").replace("]]>", "")
Â  Â  Â  Â  Â  Â  Â  Â  nl = l.group(1).strip()
Â  Â  Â  Â  Â  Â  Â  Â  nd = clean_html_regex(d.group(1)) if d else ""
Â  Â  Â  Â  Â  Â  Â  Â  news_res.append({'title': nt, 'desc': nd, 'link': nl})
Â  Â  except: pass
Â  Â  return news_res

def analyze_comment_relevance(comments, context_text):
Â  Â  if not comments: return [], 0, "ë¶„ì„ ë¶ˆê°€"
Â  Â  cn = extract_meaningful_tokens(" ".join(comments))
Â  Â  top = Counter(cn).most_common(5)
Â  Â  ctx = set(extract_meaningful_tokens(context_text))
Â  Â  match = sum(1 for w,c in top if w in ctx)
Â  Â  score = int(match/len(top)*100) if top else 0
Â  Â  msg = "âœ… ì£¼ì œ ì§‘ì¤‘" if score >= 60 else "âš ï¸ ì¼ë¶€ ê´€ë ¨" if score >= 20 else "âŒ ë¬´ê´€"
Â  Â  return [f"{w}({c})" for w, c in top], score, msg

def check_red_flags(comments):
Â  Â  detected = [k for c in comments for k in ['ê°€ì§œë‰´ìŠ¤', 'ì£¼ì‘', 'ì‚¬ê¸°', 'ê±°ì§“ë§', 'í—ˆìœ„', 'ì„ ë™'] if k in c]
Â  Â  return len(detected), list(set(detected))

def run_forensic_main(url):
Â  Â  st.session_state["debug_logs"] = []
Â  Â  progress_text = "íŠ¸ë¦¬í”Œ ì—”ì§„(Mistral + Gemini A/B) ê°€ë™ ì¤‘..."
Â  Â  my_bar = st.progress(0, text=progress_text)
Â  Â Â 
Â  Â  db_count, db_truth, db_fake = train_dynamic_vector_engine()
Â  Â Â 
Â  Â  my_bar.progress(10, text="1ë‹¨ê³„: ì˜ìƒ ìë§‰ ë° ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘...")
Â  Â  vid = re.search(r'(?:v=|\/)([0-9A-Za-z_-]{11}).*', url)
Â  Â  if vid: vid = vid.group(1)

Â  Â  with yt_dlp.YoutubeDL({'quiet': True, 'skip_download': True}) as ydl:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  info = ydl.extract_info(url, download=False)
Â  Â  Â  Â  Â  Â  title = info.get('title', ''); uploader = info.get('uploader', '')
Â  Â  Â  Â  Â  Â  tags = info.get('tags', []); desc = info.get('description', '')
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  trans, t_status = fetch_real_transcript(info)
Â  Â  Â  Â  Â  Â  full_text = trans if trans else desc
Â  Â  Â  Â  Â  Â  summary = summarize_transcript(full_text, title)
Â  Â  Â  Â  Â  Â  top_transcript_keywords = extract_top_keywords_from_transcript(full_text)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  my_bar.progress(30, text="2ë‹¨ê³„: AI ìˆ˜ì‚¬ê´€(Triple)ì´ ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
Â  Â  Â  Â  Â  Â  query, source = get_hybrid_search_keywords(title, full_text)

Â  Â  Â  Â  Â  Â  my_bar.progress(50, text="3ë‹¨ê³„: ë‰´ìŠ¤ í¬ë¡¤ë§ ë° ë”¥ ì›¹ íƒìƒ‰ ì¤‘...")
Â  Â  Â  Â  Â  Â  is_official = check_is_official(uploader)
Â  Â  Â  Â  Â  Â  is_ai, ai_msg = detect_ai_content(info)
Â  Â  Â  Â  Â  Â  hashtag_display = ", ".join([f"#{t}" for t in tags]) if tags else "í•´ì‹œíƒœê·¸ ì—†ìŒ"
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  agitation = count_sensational_words(full_text + title)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  ts, fs = vector_engine.analyze_position(query + " " + title)
Â  Â  Â  Â  Â  Â  t_impact = int(ts * 30) * -1; f_impact = int(fs * 30)

Â  Â  Â  Â  Â  Â  news_items = fetch_news_regex(query)
Â  Â  Â  Â  Â  Â  news_ev = []; max_match = 0
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  my_bar.progress(70, text="4ë‹¨ê³„: ë‰´ìŠ¤ ë³¸ë¬¸ ì •ë°€ ëŒ€ì¡° ì¤‘...")
Â  Â  Â  Â  Â  Â  for idx, item in enumerate(news_items[:3]):
Â  Â  Â  Â  Â  Â  Â  Â  ai_s, ai_r, source_type, evidence_text, real_url = deep_verify_news(summary, item['link'], item['desc'])
Â  Â  Â  Â  Â  Â  Â  Â  if ai_s > max_match: max_match = ai_s
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  status_icon = "ğŸŸ¢" if ai_s >= 80 else "ğŸŸ¡" if ai_s >= 60 else "ğŸ”´"
Â  Â  Â  Â  Â  Â  Â  Â  news_ev.append({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "ë‰´ìŠ¤ ì œëª©": item['title'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "ì¼ì¹˜ë„": f"{status_icon} {ai_s}%",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "ìµœì¢… ì ìˆ˜": f"{ai_s}%",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "ë¶„ì„ ê·¼ê±°": ai_r,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "ë¹„ê³ ": f"[{source_type}] {len(evidence_text)}ì ë¶„ì„",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "ì›ë¬¸": real_url
Â  Â  Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # [ìˆ˜ì •ë¨: ë‰´ìŠ¤ ìœ ì‚¬ë„ ì—„ê²© ëª¨ë“œ (Strict Mode) - 60% ì´ìƒì€ ì˜ì‹¬]
Â  Â  Â  Â  Â  Â  if not news_ev: news_score = 0
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  if max_match >= 80: news_score = -40
Â  Â  Â  Â  Â  Â  Â  Â  elif max_match >= 70: news_score = -15
Â  Â  Â  Â  Â  Â  Â  Â  elif max_match >= 60: news_score = 10Â 
Â  Â  Â  Â  Â  Â  Â  Â  else: news_score = 30

Â  Â  Â  Â  Â  Â  cmts, c_status = fetch_comments_via_api(vid)
Â  Â  Â  Â  Â  Â  top_kw, rel_score, rel_msg = analyze_comment_relevance(cmts, title + " " + full_text)
Â  Â  Â  Â  Â  Â  red_cnt, red_list = check_red_flags(cmts)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  silent_penalty = 0; is_silent = (len(news_ev) == 0)
Â  Â  Â  Â  Â  Â  if is_silent:
Â  Â  Â  Â  Â  Â  Â  Â  if any(k in title for k in CRITICAL_STATE_KEYWORDS): silent_penalty = 10
Â  Â  Â  Â  Â  Â  Â  Â  elif agitation >= 3: silent_penalty = 20
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if is_official: news_score = -50; silent_penalty = 0
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # ------------------------------------------------------------------
Â  Â  Â  Â  Â  Â  # [ğŸš¨ ê¸´ê¸‰ ìˆ˜ì •: ì—¬ë¡ /ì œëª©/íƒœê·¸ ì ìˆ˜ ë™ì  í™œì„±í™”]
Â  Â  Â  Â  Â  Â  # ------------------------------------------------------------------
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 1. ì—¬ë¡  ì ìˆ˜ (Sentiment Score) - ëŒ“ê¸€ì˜ 'ê°€ì§œë‰´ìŠ¤' ì–¸ê¸‰ íšŸìˆ˜ ë°˜ì˜
Â  Â  Â  Â  Â  Â  sent_score = min(20, red_cnt * 3)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 2. ë‚šì‹œì„± ì œëª© (Clickbait) - í‚¤ì›Œë“œ ëŒ€í­ í™•ì¥
Â  Â  Â  Â  Â  Â  bait_keywords = ['ì¶©ê²©', 'ê²½ì•…', 'í­ë¡œ', 'ì†ë³´', 'ê¸´ê¸‰', 'ë‚˜ë½', 'ì‹¤ì²´', 'ì†Œë¦„', 'ê²°êµ­', 'ã„·ã„·', '??', 'ì§„ì‹¤', 'ì´ìœ ']
Â  Â  Â  Â  Â  Â  if any(w in title for w in bait_keywords):
Â  Â  Â  Â  Â  Â  Â  Â  clickbait = 10Â  # ë‚šì‹œì„± ì œëª©ì´ë©´ ê°€ì§œ ì˜ì‹¬ (+10)
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  clickbait = -5Â  # ë‹´ë°±í•œ ì œëª©ì´ë©´ ì‹ ë¢°ë„ ìƒìŠ¹ (-5)

Â  Â  Â  Â  Â  Â  # 3. íƒœê·¸ ë‚¨ìš© ì ìˆ˜
Â  Â  Â  Â  Â  Â  if len(tags) == 0: abuse_score = 5 # íƒœê·¸ ìˆ¨ê¹€ ì˜ì‹¬
Â  Â  Â  Â  Â  Â  elif len(tags) > 30: abuse_score = 5 # íƒœê·¸ ìŠ¤íŒ¸ ì˜ì‹¬
Â  Â  Â  Â  Â  Â  else: abuse_score = 0
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # 4. ì¢…í•© ì•Œê³ ë¦¬ì¦˜ ì ìˆ˜ í•©ì‚°
Â  Â  Â  Â  Â  Â  algo_base_score = 50 + t_impact + f_impact + news_score + sent_score + clickbait + abuse_score + silent_penalty
Â  Â  Â  Â  Â  Â  # ------------------------------------------------------------------
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  my_bar.progress(90, text="5ë‹¨ê³„: AI íŒì‚¬(Triple) ìµœì¢… íŒê²° ì¤‘...")
Â  Â  Â  Â  Â  Â  ai_judge_score, ai_judge_reason = get_hybrid_verdict_final(title, full_text, news_ev)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # [Silent Echo Neutralizer]
Â  Â  Â  Â  Â  Â  neutralizer_applied = False
Â  Â  Â  Â  Â  Â  if t_impact == 0 and f_impact == 0 and is_silent:
Â  Â  Â  Â  Â  Â  Â  Â  neutralizer_applied = True
Â  Â  Â  Â  Â  Â  Â  Â  ai_judge_score = int((ai_judge_score + 50) / 2)
Â  Â  Â  Â  Â  Â  Â  Â  algo_base_score = int((algo_base_score + 50) / 2)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  final_prob = int((algo_base_score * WEIGHT_ALGO) + (ai_judge_score * WEIGHT_AI))
Â  Â  Â  Â  Â  Â  final_prob = max(1, min(99, final_prob))
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  save_analysis(uploader, title, final_prob, url, query)
Â  Â  Â  Â  Â  Â  my_bar.empty()

Â  Â  Â  Â  Â  Â  st.subheader(f"ğŸ•µï¸ Triple-Engine Analysis Result")
Â  Â  Â  Â  Â  Â  col_a, col_b, col_c = st.columns(3)
Â  Â  Â  Â  Â  Â  with col_a:Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.metric("ìµœì¢… ê°€ì§œë‰´ìŠ¤ í™•ë¥ ", f"{final_prob}%", delta=f"AI Judge: {ai_judge_score}pt")
Â  Â  Â  Â  Â  Â  with col_b:
Â  Â  Â  Â  Â  Â  Â  Â  icon = "ğŸŸ¢" if final_prob < 30 else "ğŸ”´" if final_prob > 60 else "ğŸŸ "
Â  Â  Â  Â  Â  Â  Â  Â  verdict = "ì•ˆì „ (Verified)" if final_prob < 30 else "ìœ„í—˜ (Fake/Bias)" if final_prob > 60 else "ì£¼ì˜ (Caution)"
Â  Â  Â  Â  Â  Â  Â  Â  if neutralizer_applied: verdict += " (ì¦ê±° ë¶€ì¡±ìœ¼ë¡œ ë³´ì •ë¨)"
Â  Â  Â  Â  Â  Â  Â  Â  st.metric("ì¢…í•© AI íŒì •", f"{icon} {verdict}")
Â  Â  Â  Â  Â  Â  with col_c:Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.metric("AI Intelligence Level", f"{db_count} Nodes", delta="Triple Active")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.divider()
Â  Â  Â  Â  Â  Â  st.subheader("ğŸ§  Intelligence Map")
Â  Â  Â  Â  Â  Â  render_intelligence_distribution(final_prob)

Â  Â  Â  Â  Â  Â  if is_ai: st.warning(f"ğŸ¤– **AI ìƒì„± ì½˜í…ì¸  ê°ì§€ë¨**: {ai_msg}")
Â  Â  Â  Â  Â  Â  if is_official: st.success(f"ğŸ›¡ï¸ **ê³µì‹ ì–¸ë¡ ì‚¬ ì±„ë„({uploader})ì…ë‹ˆë‹¤.**")
Â  Â  Â  Â  Â  Â  if neutralizer_applied:
Â  Â  Â  Â  Â  Â  Â  Â  st.info("ğŸ’¡ **Silent Echo ê°ì§€**: ë‰´ìŠ¤ ê¸°ì‚¬ì™€ DB ë°ì´í„°ê°€ ë°œê²¬ë˜ì§€ ì•Šì•„, AI íŒë‹¨ ì ìˆ˜ë¥¼ 'ì¤‘ë¦½(50ì )' ë°©í–¥ìœ¼ë¡œ ê°•ì œ ë³´ì •í–ˆìŠµë‹ˆë‹¤.")

Â  Â  Â  Â  Â  Â  st.divider()
Â  Â  Â  Â  Â  Â  col1, col2 = st.columns([1, 1.4])
Â  Â  Â  Â  Â  Â  with col1:
Â  Â  Â  Â  Â  Â  Â  Â  st.write("**[ì˜ìƒ ìƒì„¸ ì •ë³´]**")
Â  Â  Â  Â  Â  Â  Â  Â  st.table(pd.DataFrame({"í•­ëª©": ["ì˜ìƒ ì œëª©", "ì±„ë„ëª…", "ì¡°íšŒìˆ˜", "í•´ì‹œíƒœê·¸"], "ë‚´ìš©": [title, uploader, f"{info.get('view_count',0):,}íšŒ", hashtag_display]}))
Â  Â  Â  Â  Â  Â  Â  Â  st.info(f"ğŸ¯ **Investigator (Triple) ì¶”ì¶œ ê²€ìƒ‰ì–´**: {query}")
Â  Â  Â  Â  Â  Â  Â  Â  with st.container(border=True):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("ğŸ“ **ì˜ìƒ ë‚´ìš© ìš”ì•½**")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write(summary)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.write("**[Score Breakdown]**")
Â  Â  Â  Â  Â  Â  Â  Â  render_score_breakdown([
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ["ğŸ ê¸°ë³¸ ì¤‘ë¦½ ì ìˆ˜ (Base Score)", 50, "ëª¨ë“  ë¶„ì„ì€ 50ì (ì¤‘ë¦½)ì—ì„œ ì‹œì‘"],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ["ì§„ì‹¤ ë°ì´í„° ë§¥ë½", t_impact, "ë‚´ë¶€ DB ì§„ì‹¤ ë°ì´í„°ì™€ ìœ ì‚¬ì„±"],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ["ê°€ì§œ íŒ¨í„´ ë§¥ë½", f_impact, "ë‚´ë¶€ DB ê°€ì§œ ë°ì´í„°ì™€ ìœ ì‚¬ì„±"],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ["ë‰´ìŠ¤ ë§¤ì¹­ ìƒíƒœ", news_score, "Deep-Crawler ì •ë°€ ëŒ€ì¡° ê²°ê³¼ (Strict)"],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ["ì—¬ë¡ /ì œëª©/íƒœê·¸ ê°€ê°", sent_score + clickbait + abuse_score, ""],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ["* ì¦ê±° ë¶€ì¡± ë³´ì •", "ì ìš©ë¨" if neutralizer_applied else "ë¯¸ì ìš©", "ë°ì´í„° ì—†ì„ ì‹œ ê°•ì œ ì¤‘ë¦½í™”"],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ["-----------------", "", ""],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ["âš–ï¸ AI Judge Score (15%)", ai_judge_score, "Triple ì¢…í•© ì¶”ë¡  (ì°¸ê³ ìš©)"]
Â  Â  Â  Â  Â  Â  Â  Â  ])

Â  Â  Â  Â  Â  Â  with col2:
Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("ğŸ“Š 5ëŒ€ ì •ë°€ ë¶„ì„ ì¦ê±°")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**[ì¦ê±° 0] Semantic Vector Space (Internal DB)**")
Â  Â  Â  Â  Â  Â  Â  Â  colored_progress_bar("âœ… ì§„ì‹¤ ì˜ì—­ ê·¼ì ‘ë„", ts, "#2ecc71")
Â  Â  Â  Â  Â  Â  Â  Â  colored_progress_bar("ğŸš¨ ê±°ì§“ ì˜ì—­ ê·¼ì ‘ë„", fs, "#e74c3c")
Â  Â  Â  Â  Â  Â  Â  Â  st.write("---")

Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(f"**[ì¦ê±° 1] ë‰´ìŠ¤ êµì°¨ ëŒ€ì¡° (Deep-Web Crawler)**")
Â  Â  Â  Â  Â  Â  Â  Â  if news_ev:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pd.DataFrame(news_ev),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  column_config={
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "ì›ë¬¸": st.column_config.LinkColumn(label="ë§í¬", display_text="ğŸ”— ì´ë™")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  },
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  use_container_width=True,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  hide_index=True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.expander("ğŸ” í¬ë¡¤ë§ëœ ë‰´ìŠ¤ ë³¸ë¬¸ ìƒ˜í”Œ ë³´ê¸°"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for n in news_ev:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.caption(f"**{n['ë‰´ìŠ¤ ì œëª©']}**: {n['ë¹„ê³ ']}")
Â  Â  Â  Â  Â  Â  Â  Â  else: st.warning("ğŸ” ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (Silent Echo Risk)")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**[ì¦ê±° 2] ì‹œì²­ì ì—¬ë¡  ì‹¬ì¸µ ë¶„ì„**")
Â  Â  Â  Â  Â  Â  Â  Â  if cmts: st.table(pd.DataFrame([["ìµœë‹¤ ë¹ˆì¶œ í‚¤ì›Œë“œ", ", ".join(top_kw)], ["ë…¼ë€ ê°ì§€ ì—¬ë¶€", f"{red_cnt}íšŒ"], ["ì£¼ì œ ì¼ì¹˜ë„", f"{rel_score}% ({rel_msg})"]], columns=["í•­ëª©", "ë‚´ìš©"]))
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**[ì¦ê±° 3] ìë§‰ ì„¸ë§Œí‹± ì‹¬ì¸µ ëŒ€ì¡°**")
Â  Â  Â  Â  Â  Â  Â  Â  top_kw_str = ", ".join([f"{w}({c})" for w, c in top_transcript_keywords])
Â  Â  Â  Â  Â  Â  Â  Â  st.table(pd.DataFrame([["ì˜ìƒ ìµœë‹¤ ì–¸ê¸‰ í‚¤ì›Œë“œ", top_kw_str], ["ì œëª© ë‚šì‹œì–´", "ìˆìŒ" if clickbait > 0 else "ì—†ìŒ"], ["ì„ ë™ì„± ì§€ìˆ˜", f"{agitation}íšŒ"]], columns=["ë¶„ì„ í•­ëª©", "íŒì • ê²°ê³¼"]))
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("**[ì¦ê±° 4] AI ìµœì¢… ë¶„ì„ íŒë‹¨ (Judge Verdict)**")
Â  Â  Â  Â  Â  Â  Â  Â  with st.container(border=True):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write(f"âš–ï¸ **íŒê²°:** {ai_judge_reason}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.caption(f"* Triple ë…ë¦½ ì¶”ë¡  ì ìˆ˜: {ai_judge_score}ì ")

Â  Â  Â  Â  Â  Â  Â  Â  reasons = []
Â  Â  Â  Â  Â  Â  Â  Â  if final_prob >= 60:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  reasons.append("ğŸš¨ **ìœ„í—˜ ê°ì§€**: AI íŒì‚¬ì™€ ì•Œê³ ë¦¬ì¦˜ ëª¨ë‘ ì´ ì˜ìƒì˜ ì£¼ì¥ì„ ì˜ì‹¬í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if len(news_ev) == 0: reasons.append("ğŸ”‡ **ê·¼ê±° ë¶€ì¬**: ìê·¹ì ì¸ ì£¼ì¥ì— ë¹„í•´ ì–¸ë¡  ë³´ë„ê°€ ì „ë¬´í•©ë‹ˆë‹¤.")
Â  Â  Â  Â  Â  Â  Â  Â  elif final_prob <= 30:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  reasons.append("âœ… **ì•ˆì „ íŒì •**: ì˜ìƒ ë‚´ìš©ì´ ì£¼ìš” ë‰´ìŠ¤ ë³´ë„ì™€ ì¼ì¹˜í•˜ë©°, AI ì¶”ë¡  ê²°ê³¼ë„ ê¸ì •ì ì…ë‹ˆë‹¤.")
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  reasons.append("âš ï¸ **ì£¼ì˜ ìš”ë§**: ì¼ë¶€ ê³¼ì¥ëœ í‘œí˜„ì´ë‚˜ í™•ì¸ë˜ì§€ ì•Šì€ ì‚¬ì‹¤ì´ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.success(f"ğŸ” ìµœì¢… ë¶„ì„ ê²°ê³¼: **{final_prob}ì **")
Â  Â  Â  Â  Â  Â  Â  Â  for r in reasons: st.write(r)

Â  Â  Â  Â  except Exception as e: st.error(f"ì˜¤ë¥˜: {e}")

# --- [NEW] B2B ë¦¬í¬íŠ¸ ìƒì„± ì—”ì§„ (ì—¬ê¸°ì— ë¶™ì—¬ë„£ìœ¼ì„¸ìš”) ---
def generate_b2b_report_logic(df_history):
Â  Â  if df_history.empty: return pd.DataFrame()
Â  Â Â 
Â  Â  # 1. ë°ì´í„° ê°•ì œ í˜•ë³€í™˜ (ë¬¸ìì—´ -> ìˆ«ì) [í•µì‹¬ ìˆ˜ì •]
Â  Â  # ì—ëŸ¬ ì›ì¸ì´ì—ˆë˜ ë¬¸ìì—´ ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë°”ê¾¸ê³ , ë¹ˆ ê°’ì€ 0ìœ¼ë¡œ ì±„ì›ë‹ˆë‹¤.
Â  Â  df_history['fake_prob'] = pd.to_numeric(df_history['fake_prob'], errors='coerce').fillna(0)
Â  Â Â 
Â  Â  # 2. ì•ˆì „í•œ ì§ì ‘ ê³„ì‚° ë°©ì‹ (MultiIndex ë¯¸ì‚¬ìš©)
Â  Â  grouped = df_history.groupby('channel_name')
Â  Â Â 
Â  Â  # ì»¬ëŸ¼ë³„ë¡œ ë”°ë¡œ ê³„ì‚°í•´ì„œ í•©ì¹©ë‹ˆë‹¤ (ê°€ì¥ ì•ˆì „í•œ ë°©ë²•)
Â  Â  report = pd.DataFrame({
Â  Â  Â  Â  'analyzed_count': grouped['fake_prob'].count(),
Â  Â  Â  Â  'avg_risk': grouped['fake_prob'].mean(),
Â  Â  Â  Â  'max_risk': grouped['fake_prob'].max(),
Â  Â  Â  Â  'all_keywords': grouped['keywords'].apply(lambda x: ' '.join([str(k) for k in x if k]))
Â  Â  }).reset_index()
Â  Â Â 
Â  Â  results = []
Â  Â  for _, row in report.iterrows():
Â  Â  Â  Â  avg_score = row['avg_risk']
Â  Â  Â  Â Â 
Â  Â  Â  Â  if avg_score >= 60: grade = "â›” BLACKLIST (ì‹¬ê°)"
Â  Â  Â  Â  elif avg_score >= 40: grade = "âš ï¸ CAUTION (ì£¼ì˜)"
Â  Â  Â  Â  else: grade = "âœ… SAFE (ì–‘í˜¸)"
Â  Â  Â  Â Â 
Â  Â  Â  Â  # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
Â  Â  Â  Â  tokens = re.findall(r'[ê°€-í£]{2,}', str(row['all_keywords']))
Â  Â  Â  Â  targets = ", ".join([t[0] for t in Counter(tokens).most_common(3)])
Â  Â  Â  Â Â 
Â  Â  Â  Â  results.append({
Â  Â  Â  Â  Â  Â  "ì±„ë„ëª…": row['channel_name'],
Â  Â  Â  Â  Â  Â  "ìœ„í—˜ ë“±ê¸‰": grade,
Â  Â  Â  Â  Â  Â  "í‰ê·  ê°€ì§œ í™•ë¥ ": f"{int(avg_score)}%",
Â  Â  Â  Â  Â  Â  "ìµœê³  ê°€ì§œ í™•ë¥ ": f"{int(row['max_risk'])}%",
Â  Â  Â  Â  Â  Â  "ë¶„ì„ ì˜ìƒ ìˆ˜": f"{int(row['analyzed_count'])}ê°œ",
Â  Â  Â  Â  Â  Â  "ì£¼ìš” íƒ€ê²Ÿ": targets
Â  Â  Â  Â  })
Â  Â  Â  Â Â 
Â  Â  return pd.DataFrame(results).sort_values(by='í‰ê·  ê°€ì§œ í™•ë¥ ', ascending=False)

# --- [UI Layout] ---
st.title("âš–ï¸ìœ íŠœë¸Œ ê°€ì§œë‰´ìŠ¤ íŒë…ê¸° (Triple Engine)")

with st.container(border=True):
Â  Â  st.markdown("### ğŸ›¡ï¸ ë²•ì  ê³ ì§€ ë° ì±…ì„ í•œê³„ (Disclaimer)\në³¸ ì„œë¹„ìŠ¤ëŠ” **ì¸ê³µì§€ëŠ¥(AI) ë° ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜**ìœ¼ë¡œ ì˜ìƒì˜ ì‹ ë¢°ë„ë¥¼ ë¶„ì„í•˜ëŠ” ë³´ì¡° ë„êµ¬ì…ë‹ˆë‹¤. \në¶„ì„ ê²°ê³¼ëŠ” ë²•ì  íš¨ë ¥ì´ ì—†ìœ¼ë©°, ìµœì¢… íŒë‹¨ì˜ ì±…ì„ì€ ì‚¬ìš©ìì—ê²Œ ìˆìŠµë‹ˆë‹¤.")
Â  Â  st.markdown("* **1st Line**: Mistral AI\n* **2nd Line**: Google Gemini Key A\n* **3rd Line**: Google Gemini Key B (Final Backup)")
Â  Â  agree = st.checkbox("ìœ„ ë‚´ìš©ì„ í™•ì¸í•˜ì˜€ìœ¼ë©°, ì´ì— ë™ì˜í•©ë‹ˆë‹¤. (ë™ì˜ ì‹œ ë¶„ì„ ë²„íŠ¼ í™œì„±í™”)")

url_input = st.text_input("ğŸ”— ë¶„ì„í•  ìœ íŠœë¸Œ URL")
if st.button("ğŸš€ ì •ë°€ ë¶„ì„ ì‹œì‘", use_container_width=True, disabled=not agree):
Â  Â  if url_input: run_forensic_main(url_input)
Â  Â  else: st.warning("URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

st.divider()
st.subheader("ğŸ—‚ï¸ í•™ìŠµ ë°ì´í„° ê´€ë¦¬ (Cloud Knowledge Base)")
try:
Â  Â  response = supabase.table("analysis_history").select("*").order("id", desc=True).execute()
Â  Â  df = pd.DataFrame(response.data)
except: df = pd.DataFrame()

if not df.empty:
Â  Â  if st.session_state["is_admin"]:
Â  Â  Â  Â  df['Delete'] = False
Â  Â  Â  Â  edited_df = st.data_editor(df[['Delete', 'id', 'analysis_date', 'video_title', 'fake_prob', 'keywords']], hide_index=True, use_container_width=True)
Â  Â  Â  Â  if st.button("ğŸ—‘ï¸ ì„ íƒ í•­ëª© ì‚­ì œ", type="primary"):
Â  Â  Â  Â  Â  Â  to_delete = edited_df[edited_df.Delete]
Â  Â  Â  Â  Â  Â  if not to_delete.empty:
Â  Â  Â  Â  Â  Â  Â  Â  for index, row in to_delete.iterrows(): supabase.table("analysis_history").delete().eq("id", row['id']).execute()
Â  Â  Â  Â  Â  Â  Â  Â  st.success("ì‚­ì œ ì™„ë£Œ!"); time.sleep(1); st.rerun()
Â  Â  else:
Â  Â  Â  Â  st.dataframe(df[['analysis_date', 'video_title', 'fake_prob', 'keywords']], hide_index=True, use_container_width=True)
else: st.info("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

st.write("")
# [ê´€ë¦¬ì ì „ìš© ì„¹ì…˜]
with st.expander("ğŸ” ê´€ë¦¬ì ì ‘ì† (Admin Access)"):
Â  Â  if st.session_state["is_admin"]:
Â  Â  Â  Â  st.success("ê´€ë¦¬ì ê¶Œí•œ í™œì„±í™”ë¨")
Â  Â  Â  Â 
Â  Â  Â  Â  st.divider()
Â  Â  Â  Â  st.subheader("ğŸ¢ B2B ë¸Œëœë“œ ì„¸ì´í”„í‹° ë¦¬í¬íŠ¸ (Business Intelligence)")
Â  Â  Â  Â  if st.button("ğŸ“Š ë¦¬í¬íŠ¸ ìƒì„± ë° ë¶„ì„"):
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  # ìœ„ì—ì„œ ì •ì˜í•œ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤. (dfëŠ” ë°”ë¡œ ìœ„ íˆìŠ¤í† ë¦¬ ì˜ì—­ì—ì„œ ì´ë¯¸ ì •ì˜ë¨)
Â  Â  Â  Â  Â  Â  Â  Â  rpt = generate_b2b_report_logic(df)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if not rpt.empty:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  rpt,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  column_config={
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "ìœ„í—˜ ë“±ê¸‰": st.column_config.TextColumn("Risk Level", help="í‰ê·  ê°€ì§œë‰´ìŠ¤ í™•ë¥  ê¸°ë°˜ ë“±ê¸‰"),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "í‰ê·  ê°€ì§œ í™•ë¥ ": st.column_config.ProgressColumn("Avg Risk", format="%s", min_value=0, max_value=100),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  },
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  use_container_width=True, hide_index=True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  csv = rpt.to_csv(index=False).encode('utf-8-sig')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.download_button("ğŸ“¥ ë¦¬í¬íŠ¸ ì—‘ì…€(CSV) ë‹¤ìš´ë¡œë“œ", csv, "b2b_report.csv", "text/csv")
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info("ë¶„ì„í•  ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
Â  Â  Â  Â  Â  Â  except Exception as e: st.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
Â  Â  Â  Â Â 
Â  Â  Â  Â  st.divider()
Â  Â  Â  Â  st.subheader("ğŸ› ï¸ ì‹œìŠ¤í…œ ìƒíƒœ ë° ë””ë²„ê·¸ ë¡œê·¸")
Â  Â  Â  Â Â 
Â  Â  Â  Â  st.write("**ğŸ¤– Triple Defense System Status:**")
Â  Â  Â  Â Â 
Â  Â  Â  Â  st.caption("1ï¸âƒ£ Mistral Priority Chain")
Â  Â  Â  Â  st.code(", ".join(MISTRAL_MODELS))
Â  Â  Â  Â Â 
Â  Â  Â  Â  st.caption("2ï¸âƒ£ Gemini Key A (Dynamic Scan)")
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  st.code(", ".join(get_gemini_models_dynamic(GOOGLE_API_KEY_A)))
Â  Â  Â  Â  except: st.error("Key A ì—°ê²° ì‹¤íŒ¨")

Â  Â  Â  Â  st.caption("3ï¸âƒ£ Gemini Key B (Dynamic Scan)")
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  st.code(", ".join(get_gemini_models_dynamic(GOOGLE_API_KEY_B)))
Â  Â  Â  Â  except: st.error("Key B ì—°ê²° ì‹¤íŒ¨")

Â  Â  Â  Â  if "debug_logs" in st.session_state and st.session_state["debug_logs"]:
Â  Â  Â  Â  Â  Â  st.write(f"**ğŸ“œ ìµœê·¼ ì‹¤í–‰ ë¡œê·¸ ({len(st.session_state['debug_logs'])}ê±´):**")
Â  Â  Â  Â  Â  Â  log_text = "\n".join(st.session_state["debug_logs"])
Â  Â  Â  Â  Â  Â  st.text_area("Logs", log_text, height=300)
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.info("ì‹¤í–‰ëœ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")

Â  Â  Â  Â  if st.button("ë¡œê·¸ì•„ì›ƒ"):
Â  Â  Â  Â  Â  Â  st.session_state["is_admin"] = False
Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  else:
Â  Â  Â  Â  input_pwd = st.text_input("Admin Password", type="password")
Â  Â  Â  Â  if st.button("Login"):
Â  Â  Â  Â  Â  Â  if input_pwd == ADMIN_PASSWORD:
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state["is_admin"] = True
Â  Â  Â  Â  Â  Â  Â  Â  st.rerun()
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.error("Access Denied")
