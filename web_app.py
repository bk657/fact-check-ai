import streamlit as st
import google.generativeai as genai
import re
import requests
import time
import json
import yt_dlp
import pandas as pd
import altair as alt
from datetime import datetime

# --- [1. ì‹œìŠ¤í…œ ì„¤ì •] ---
st.set_page_config(page_title="Fact-Check Center v62.0 (Final)", layout="wide", page_icon="âš–ï¸")

# ğŸŒŸ Secrets ë¡œë“œ
try:
    YOUTUBE_API_KEY = st.secrets["YOUTUBE_API_KEY"]
    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    ADMIN_PASSWORD = st.secrets["ADMIN_PASSWORD"]
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
except KeyError as e:
    st.error(f"âŒ í•„ìˆ˜ í‚¤ ì„¤ì • ëˆ„ë½: {e}")
    st.stop()

# ğŸŒŸ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” (ê²€ì¦ëœ gemini-pro ì—°ê²°)
@st.cache_resource
def init_services():
    try:
        from supabase import create_client
        sb = create_client(SUPABASE_URL, SUPABASE_KEY)
        genai.configure(api_key=GOOGLE_API_KEY)
        
        # ğŸš¨ ì„±ê³µí•œ ëª¨ë¸: gemini-pro
        model = genai.GenerativeModel('gemini-pro')
        return sb, model
    except Exception as e:
        return None, None

supabase, gemini_model = init_services()

# --- [2. Gemini AI ì—ì´ì „íŠ¸] ---
class GeminiAgent:
    def __init__(self, model):
        self.model = model

    def extract_keywords(self, title, transcript):
        """ë‰´ìŠ¤ ê²€ìƒ‰ì„ ìœ„í•œ ìµœì ì˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        if not self.model: return title
        prompt = f"""
        Extract the single most important search query to fact-check this video.
        - Input: {title}
        - Context: {transcript[:500]}
        - Output: ONLY the keyword string (e.g., 'Jay Lee Divorce')
        """
        try:
            response = self.model.generate_content(prompt)
            return response.text.strip()
        except: return title

    def analyze_content(self, title, channel, transcript, news_context, comments):
        """ë‰´ìŠ¤, ìë§‰, ëŒ“ê¸€ì„ ì¢…í•©í•˜ì—¬ íŒ©íŠ¸ì²´í¬ ìˆ˜í–‰"""
        if not self.model:
            return {"fake_prob": 50, "verdict": "ì˜¤ë¥˜", "summary": "AI ì—°ê²° ëŠê¹€", "clickbait_score": 0}

        prompt = f"""
        You are a professional Fact-Checker. Analyze the video claims against the news facts.
        Respond in JSON format ONLY.

        [Input Data]
        - Video Title: {title}
        - Video Transcript: {transcript[:4000]}
        - Related News: {news_context}
        - User Comments: {comments}

        [Tasks]
        1. Compare Video Claims vs News Facts.
        2. If News matches claims -> Low fake_prob (0-30).
        3. If News contradicts or No News -> High fake_prob (70-100).
        4. Translate all output values to Korean.

        [JSON Output Format]
        {{
            "summary": "3 bullet points summary in Korean",
            "fake_prob": Integer (0-100),
            "verdict": "ìœ„í—˜/ì£¼ì˜/ì•ˆì „",
            "reasoning": "Detailed reasoning in Korean (citing news results)",
            "fact_check_status": "Short status in Korean (e.g., 'ë‰´ìŠ¤ êµì°¨ ê²€ì¦ ì™„ë£Œ')",
            "clickbait_score": Integer (0-100)
        }}
        """
        try:
            response = self.model.generate_content(prompt)
            text = response.text.replace("```json", "").replace("```", "").strip()
            return json.loads(text)
        except Exception as e:
            return {
                "summary": "ë¶„ì„ ì‹¤íŒ¨", "fake_prob": 50, "verdict": "ì˜¤ë¥˜",
                "reasoning": "ë°ì´í„° ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "fact_check_status": "ë¶„ì„ ë¶ˆê°€", "clickbait_score": 0
            }

gemini_agent = GeminiAgent(gemini_model)

# --- [3. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜] ---
def fetch_youtube_info(url):
    ydl_opts = {'quiet': True, 'skip_download': True}
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        try:
            info = ydl.extract_info(url, download=False)
            transcript = ""
            for sub_type in ['subtitles', 'automatic_captions']:
                if sub_type in info and 'ko' in info[sub_type]:
                    url = next((x['url'] for x in info[sub_type]['ko'] if x['ext'] == 'vtt'), None)
                    if url: 
                        transcript = requests.get(url).text
                        break
            
            clean_text = ""
            if transcript:
                lines = [line.strip() for line in transcript.splitlines() if '-->' not in line and line.strip() and not line.startswith(('WEBVTT', 'NOTE'))]
                clean_text = " ".join(list(dict.fromkeys(lines)))
            else:
                clean_text = info.get('description', '')

            return {
                "id": info['id'], "title": info.get('title', ''), 
                "channel": info.get('uploader', ''), "transcript": clean_text
            }
        except: return None

def fetch_google_news(query):
    try:
        rss_url = f"https://news.google.com/rss/search?q={requests.utils.quote(query)}&hl=ko&gl=KR&ceid=KR:ko"
        res = requests.get(rss_url, timeout=5)
        items = re.findall(r'<item>(.*?)</item>', res.text, re.DOTALL)
        news_list = []
        for item in items[:5]:
            t = re.search(r'<title>(.*?)</title>', item)
            if t: news_list.append(t.group(1).replace("<![CDATA[", "").replace("]]>", ""))
        return " | ".join(news_list) if news_list else "ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ ì—†ìŒ"
    except: return "ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨"

def fetch_comments(video_id):
    try:
        url = "https://www.googleapis.com/youtube/v3/commentThreads"
        params = {'part': 'snippet', 'videoId': video_id, 'key': YOUTUBE_API_KEY, 'maxResults': 10, 'order': 'relevance'}
        res = requests.get(url, params=params)
        if res.status_code == 200:
            return " | ".join([i['snippet']['topLevelComment']['snippet']['textDisplay'] for i in res.json().get('items', [])])
    except: pass
    return "ëŒ“ê¸€ ì—†ìŒ"

def save_history(data):
    try:
        supabase.table("analysis_history").insert({
            "channel_name": data['channel'], "video_title": data['title'],
            "fake_prob": data['fake_prob'], "analysis_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "video_url": f"https://youtu.be/{data['id']}", "keywords": data['fact_check_status']
        }).execute()
    except: pass

# --- [4. UI êµ¬ì„±] ---
with st.sidebar:
    st.header("ğŸ›¡ï¸ ê´€ë¦¬ì ë©”ë‰´")
    if not st.session_state.get("is_admin"):
        if st.button("ë¡œê·¸ì¸"):
            st.session_state["is_admin"] = True
            st.rerun()
    else:
        st.success("ê´€ë¦¬ì ë¡œê·¸ì¸ë¨")
        if st.button("ë¡œê·¸ì•„ì›ƒ"):
            st.session_state["is_admin"] = False
            st.rerun()

st.title("âš–ï¸ Fact-Check Center v62.0")
st.caption("Powered by Google Gemini Pro")

with st.container(border=True):
    st.info("ğŸ’¡ **Gemini AI**ê°€ ì˜ìƒ ë‚´ìš©ì„ ë¶„ì„í•˜ê³ , ì‹¤ì‹œê°„ ë‰´ìŠ¤ì™€ ëŒ€ì¡°í•˜ì—¬ ì§„ìœ„ ì—¬ë¶€ë¥¼ íŒë³„í•©ë‹ˆë‹¤.")
    url_input = st.text_input("ë¶„ì„í•  ìœ íŠœë¸Œ URLì„ ì…ë ¥í•˜ì„¸ìš”")
    
    if st.button("ğŸš€ ì •ë°€ ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True):
        if url_input:
            if not gemini_model:
                st.error("âš ï¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì˜¤ë¥˜: ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            else:
                with st.status("ğŸ•µï¸ Gemini AIê°€ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...", expanded=True) as status:
                    
                    st.write("ğŸ“¥ ì˜ìƒ ë°ì´í„°(ìë§‰/ë©”íƒ€ì •ë³´) ë‹¤ìš´ë¡œë“œ...")
                    v_info = fetch_youtube_info(url_input)
                    if not v_info:
                        st.error("ì˜ìƒ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. URLì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                        st.stop()
                    
                    st.write("ğŸ§  ë¬¸ë§¥ ë¶„ì„ ë° ê²€ìƒ‰ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
                    search_keyword = gemini_agent.extract_keywords(v_info['title'], v_info['transcript'])
                    st.info(f"ğŸ‘‰ ìƒì„±ëœ ê²€ìƒ‰ì–´: **{search_keyword}**")
                    
                    st.write("ğŸ“° ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ ë° íŒ©íŠ¸ íƒìƒ‰ ì¤‘...")
                    news_result = fetch_google_news(search_keyword)
                    
                    st.write("âš–ï¸ ì£¼ì¥ vs ì‚¬ì‹¤ êµì°¨ ê²€ì¦ ìˆ˜í–‰ ì¤‘...")
                    comments = fetch_comments(v_info['id'])
                    result = gemini_agent.analyze_content(
                        v_info['title'], v_info['channel'], v_info['transcript'], news_result, comments
                    )
                    
                    save_data = {**v_info, **result}
                    save_history(save_data)
                    
                    status.update(label="âœ… ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!", state="complete", expanded=False)

                # --- ê²°ê³¼ ë¦¬í¬íŠ¸ ---
                st.divider()
                
                # 1. ìƒë‹¨ í•µì‹¬ ì§€í‘œ
                c1, c2, c3 = st.columns(3)
                c1.metric("ê°€ì§œë‰´ìŠ¤ ìœ„í—˜ë„", f"{result['fake_prob']}%", delta="High Risk" if result['fake_prob']>50 else "-Safe")
                c2.metric("AI ìµœì¢… íŒì •", result['verdict'])
                c3.metric("ë‚šì‹œì„± ì§€ìˆ˜", f"{result['clickbait_score']}ì ")
                
                # 2. ìƒì„¸ íŒì • ì´ìœ 
                if result['fake_prob'] > 60:
                    st.error(f"ğŸš¨ **ìœ„í—˜ ê°ì§€**: {result['reasoning']}")
                elif result['fake_prob'] < 40:
                    st.success(f"âœ… **ì•ˆì „**: {result['reasoning']}")
                else:
                    st.warning(f"âš ï¸ **ì£¼ì˜**: {result['reasoning']}")
                    
                # 3. ìƒì„¸ ë¶„ì„ ë‚´ìš©
                col_l, col_r = st.columns([1.2, 1])
                with col_l:
                    st.subheader("ğŸ“ AI ë¶„ì„ ë¦¬í¬íŠ¸")
                    st.caption(f"ê²€ì¦ ìƒíƒœ: {result['fact_check_status']}")
                    st.write(f"**í•µì‹¬ ìš”ì•½**:\n{result['summary']}")
                    
                    with st.expander("ğŸ” ì°¸ì¡°ëœ ë‰´ìŠ¤ ë°ì´í„° ë³´ê¸°"):
                        st.write(news_result if news_result else "ê´€ë ¨ ê¸°ì‚¬ ì—†ìŒ")

                with col_r:
                    st.subheader("ğŸ“º ì˜ìƒ ì •ë³´")
                    st.table(pd.DataFrame({
                        "í•­ëª©": ["ì±„ë„ëª…", "ì˜ìƒ ì œëª©", "ìë§‰ ê¸¸ì´"],
                        "ë‚´ìš©": [v_info['channel'], v_info['title'], f"{len(v_info['transcript']):,}ì"]
                    }))

st.divider()
st.subheader("ğŸ—‚ï¸ ìµœê·¼ ë¶„ì„ ê¸°ë¡")
try:
    response = supabase.table("analysis_history").select("*").order("id", desc=True).limit(5).execute()
    if response.data:
        st.dataframe(pd.DataFrame(response.data)[['video_title', 'fake_prob', 'keywords', 'analysis_date']], use_container_width=True, hide_index=True)
except: pass
