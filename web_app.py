import streamlit as st
from supabase import create_client, Client
import re
import requests
import time
import random
import math
from datetime import datetime
from collections import Counter
import yt_dlp
import pandas as pd
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup

# --- [1. ì‹œìŠ¤í…œ ì„¤ì •] ---
st.set_page_config(page_title="Fact-Check Center v47.2 (Transcript+)", layout="wide", page_icon="âš–ï¸")

# ğŸŒŸ Secrets
try:
    YOUTUBE_API_KEY = st.secrets["YOUTUBE_API_KEY"]
    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    ADMIN_PASSWORD = st.secrets["ADMIN_PASSWORD"]
except:
    st.error("âŒ í•„ìˆ˜ í‚¤(API Key, DB Key, Password)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

@st.cache_resource
def init_supabase():
    return create_client(SUPABASE_URL, SUPABASE_KEY)

supabase = init_supabase()

# --- [ê´€ë¦¬ì ì¸ì¦] ---
if "is_admin" not in st.session_state: st.session_state["is_admin"] = False
with st.sidebar:
    st.header("ğŸ›¡ï¸ ê´€ë¦¬ì ë©”ë‰´")
    with st.form("login_form"):
        password_input = st.text_input("ê´€ë¦¬ì ë¹„ë°€ë²ˆí˜¸", type="password")
        if st.form_submit_button("ë¡œê·¸ì¸"):
            if password_input == ADMIN_PASSWORD:
                st.session_state["is_admin"] = True; st.rerun()
            else: st.session_state["is_admin"] = False; st.error("ë¶ˆì¼ì¹˜")
    if st.session_state["is_admin"]:
        st.success("âœ… ê´€ë¦¬ì ì¸ì¦ë¨")
        if st.button("ë¡œê·¸ì•„ì›ƒ"): st.session_state["is_admin"] = False; st.rerun()

# --- [ìƒìˆ˜] ---
WEIGHT_NEWS_DEFAULT = 45; WEIGHT_VECTOR = 35; WEIGHT_CONTENT = 15; WEIGHT_SENTIMENT_DEFAULT = 10
PENALTY_ABUSE = 20; PENALTY_MISMATCH = 30; PENALTY_NO_FACT = 25; PENALTY_SILENT_ECHO = 40

VITAL_KEYWORDS = ['ìœ„ë…', 'ì‚¬ë§', 'ë³„ì„¸', 'êµ¬ì†', 'ì²´í¬', 'ê¸°ì†Œ', 'ì‹¤í˜•', 'ì‘ê¸‰ì‹¤', 'ì´í˜¼', 'ë¶ˆí™”', 'íŒŒê²½', 'ì¶©ê²©', 'ê²½ì•…', 'ì†ë³´', 'ê¸´ê¸‰', 'í­ë¡œ', 'ì–‘ì„±', 'í™•ì§„', 'ì‹¬ì •ì§€', 'ë‡Œì‚¬', 'ì¤‘íƒœ', 'ì••ìˆ˜ìˆ˜ìƒ‰', 'ì†Œí™˜', 'í‡´ì§„', 'íƒ„í•µ', 'ë‚´ë€']
VIP_ENTITIES = ['ìœ¤ì„ì—´', 'ëŒ€í†µë ¹', 'ì´ì¬ëª…', 'í•œë™í›ˆ', 'ê¹€ê±´í¬', 'ë¬¸ì¬ì¸', 'ë°•ê·¼í˜œ', 'ì´ëª…ë°•', 'íŠ¸ëŸ¼í”„', 'ë°”ì´ë“ ', 'í‘¸í‹´', 'ì ¤ë ŒìŠ¤í‚¤', 'ì‹œì§„í•‘', 'ì •ì€', 'ì´ì¤€ì„', 'ì¡°êµ­', 'ì¶”ë¯¸ì• ', 'í™ì¤€í‘œ', 'ìœ ìŠ¹ë¯¼', 'ì•ˆì² ìˆ˜', 'ì†í¥ë¯¼', 'ì´ê°•ì¸', 'ê¹€ë¯¼ì¬', 'ë¥˜í˜„ì§„', 'ì¬ìš©', 'ì •ì˜ì„ ', 'ìµœíƒœì›']
OFFICIAL_CHANNELS = ['MBC', 'KBS', 'SBS', 'EBS', 'YTN', 'JTBC', 'TVCHOSUN', 'MBN', 'CHANNEL A', 'OBS', 'ì±„ë„A', 'TVì¡°ì„ ', 'ì—°í•©ë‰´ìŠ¤', 'YONHAP', 'í•œê²¨ë ˆ', 'ê²½í–¥', 'ì¡°ì„ ', 'ì¤‘ì•™', 'ë™ì•„']

STATIC_TRUTH_CORPUS = ["ë°•ë‚˜ë˜ ìœ„ì¥ì „ì… ë¬´í˜ì˜", "ì„ì˜ì›… ì•”í‘œ ëŒ€ì‘", "ì •í¬ì› ì €ì†ë…¸í™”", "ëŒ€ì „ ì¶©ë‚¨ í†µí•©", "ì„ ê±° ì¶œë§ˆ ì„ ì–¸"]
STATIC_FAKE_CORPUS = ["ì¶©ê²© í­ë¡œ ê²½ì•…", "ê¸´ê¸‰ ì†ë³´ ì†Œë¦„", "ì¶©ê²© ë°œì–¸ ë…¼ë€", "êµ¬ì† ì˜ì¥ ë°œë¶€", "ì˜ìƒ ìœ ì¶œ", "ê³„ì‹œ ì˜ˆì–¸", "ì‚¬í˜• ì§‘í–‰", "ìœ„ë…ì„¤"]

class VectorEngine:
    def __init__(self): self.vocab = set(); self.truth_vectors = []; self.fake_vectors = []
    def tokenize(self, t): return re.findall(r'[ê°€-í£]{2,}', t)
    def train(self, t_corpus, f_corpus):
        for t in t_corpus + f_corpus: self.vocab.update(self.tokenize(t))
        self.vocab = sorted(list(self.vocab))
        self.truth_vectors = [self.text_to_vector(t) for t in t_corpus]
        self.fake_vectors = [self.text_to_vector(t) for t in f_corpus]
    def text_to_vector(self, text):
        c = Counter(self.tokenize(text)); return [c[w] for w in self.vocab]
    def cosine_similarity(self, v1, v2):
        dot = sum(a*b for a,b in zip(v1,v2)); mag = math.sqrt(sum(a*a for a in v1)) * math.sqrt(sum(b*b for b in v2))
        return dot/mag if mag>0 else 0
    def analyze_position(self, query):
        qv = self.text_to_vector(query)
        mt = max([self.cosine_similarity(qv, v) for v in self.truth_vectors] or [0])
        mf = max([self.cosine_similarity(qv, v) for v in self.fake_vectors] or [0])
        return mt, mf

ve = VectorEngine()

def save_analysis(ch, ti, pr, url, kw):
    try: supabase.table("analysis_history").insert({"channel_name":ch, "video_title":ti, "fake_prob":pr, "analysis_date":datetime.now().strftime('%Y-%m-%d %H:%M:%S'), "video_url":url, "keywords":kw}).execute()
    except: pass

def train_ve():
    try:
        dt = [r['video_title'] for r in supabase.table("analysis_history").select("video_title").lt("fake_prob",30).execute().data]
        df = [r['video_title'] for r in supabase.table("analysis_history").select("video_title").gt("fake_prob",70).execute().data]
    except: dt, df = [], []
    ve.train(STATIC_TRUTH_CORPUS+dt, STATIC_FAKE_CORPUS+df)
    return len(STATIC_TRUTH_CORPUS+dt)+len(STATIC_FAKE_CORPUS+df)

# --- [UI Utils] ---
def colored_bar(label, val, color):
    st.markdown(f"<div style='margin-bottom:5px'><div style='display:flex;justify-content:space-between'><span>{label}</span><span style='color:{color};font-weight:bold'>{int(val*100)}%</span></div><div style='background:#eee;height:8px;border-radius:4px'><div style='background:{color};width:{val*100}%;height:100%;border-radius:4px'></div></div></div>", unsafe_allow_html=True)

def loading_seq(count):
    with st.status("ğŸ•µï¸ Semantic Core v47.2 ê°€ë™...", expanded=True) as s:
        st.write(f"ğŸ§  Intelligence Level: {count}")
        st.write("ğŸ“ ìë§‰ ì „ì²´ ë°ì´í„° ìˆ˜ì§‘ ë° ì‹¬ì¸µ ë¶„ì„ ì¤‘...")
        time.sleep(0.5)
        st.write("âœ… ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ!"); s.update(label="ë¶„ì„ ì™„ë£Œ!", state="complete", expanded=False)

# --- [Logic] ---
def extract_nouns(text):
    noise = ['ì¶©ê²©','ê²½ì•…','ì†ë³´','ê¸´ê¸‰','ì˜¤ëŠ˜','ë‚´ì¼','ì§€ê¸ˆ','ê²°êµ­','ë‰´ìŠ¤','ì˜ìƒ','ëŒ€ë¶€ë¶„','ì´ìœ ','ì™œ','ìˆëŠ”','ì—†ëŠ”','í•˜ëŠ”','ê²ƒ','ìˆ˜','ë“±','ì§„ì§œ','ì •ë§','ì•Œê³ ë³´ë‹ˆ','ë„ˆë¬´']
    return [n for n in re.findall(r'[ê°€-í£A-Za-z0-9]{2,}', text) if n not in noise]

# ğŸŒŸ [v47.2 Upgrade] ìë§‰ì—ì„œ ë¹ˆì¶œ í‚¤ì›Œë“œ ë½‘ê¸°
def extract_top_keywords(text, top_n=3):
    nouns = extract_nouns(text)
    if not nouns: return []
    return [w for w, c in Counter(nouns).most_common(top_n)]

def generate_query(title, tags, transcript_keywords=[]):
    # ì œëª© + íƒœê·¸ + ìë§‰í‚¤ì›Œë“œ ê²°í•©
    base_text = title + " " + " ".join([t.replace("#","") for t in tags])
    words = base_text.split()
    
    q_parts = []
    # 1. VIP/Vital ìš°ì„ 
    for w in words:
        if any(v in w for v in VITAL_KEYWORDS + VIP_ENTITIES): q_parts.append(w)
    
    # 2. ìë§‰ì—ì„œ ë½‘ì€ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ê°€ (ì¤‘ë³µ ë°©ì§€)
    for kw in transcript_keywords:
        if kw not in q_parts and kw not in title:
            q_parts.append(kw)
    
    # 3. ì—†ìœ¼ë©´ ì œëª© ëª…ì‚¬ ì‚¬ìš©
    if not q_parts: q_parts = extract_nouns(title)[:3]
    
    return " ".join(list(dict.fromkeys(q_parts))[:4]) # ìµœëŒ€ 4ë‹¨ì–´

def summarize(text):
    if not text or len(text)<50: return "ìš”ì•½ ì •ë³´ ì—†ìŒ"
    sents = re.split(r'(?<=[.?!])\s+', text)
    freq = Counter(re.findall(r'[ê°€-í£]{2,}', text))
    ranked = sorted([(i, s, sum(freq[w] for w in re.findall(r'[ê°€-í£]{2,}',s))) for i,s in enumerate(sents) if 10<len(s)<150], key=lambda x:x[2], reverse=True)[:3]
    return " ".join([r[1] for r in sorted(ranked, key=lambda x:x[0])])

def check_official(uploader):
    return any(o in uploader.upper().replace(" ","") for o in OFFICIAL_CHANNELS)

def check_tags(title, tags, uploader):
    if check_official(uploader): return 0
    if not tags: return 0
    tn = set(extract_nouns(title)); tgn = set(t.replace("#","").split(":")[-1].strip() for t in tags)
    return 20 if len(tgn)>=2 and not tn.intersection(tgn) else 0

# ğŸŒŸ [v47.2 Upgrade] ìë§‰ ì™„ì „ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±° ë¡œì§ ì™„í™”)
def fetch_transcript(info):
    try:
        url = None
        for k in ['subtitles','automatic_captions']:
            if k in info and 'ko' in info[k]:
                for f in info[k]['ko']:
                    if f['ext'] == 'vtt': url = f['url']; break
            if url: break
        
        if url:
            res = requests.get(url)
            if res.status_code == 200:
                clean = []
                for line in res.text.splitlines():
                    if '-->' not in line and 'WEBVTT' not in line and line.strip():
                        t = re.sub(r'<[^>]+>', '', line).strip()
                        # ì¤‘ë³µ ì™„í™”: ë°”ë¡œ ì• ë¬¸ì¥ê³¼ ê°™ì„ ë•Œë§Œ ìƒëµ (ë¬¸ë§¥ ìœ ì§€)
                        if t and (not clean or clean[-1] != t): 
                            clean.append(t)
                return " ".join(clean), "âœ… ìë§‰ ì „ì²´ ìˆ˜ì§‘ ì™„ë£Œ"
    except: pass
    return None, "ìë§‰ ì—†ìŒ"

def fetch_comments(vid):
    try:
        url = "https://www.googleapis.com/youtube/v3/commentThreads"
        res = requests.get(url, params={'part':'snippet', 'videoId':vid, 'key':YOUTUBE_API_KEY, 'maxResults':50, 'order':'relevance'})
        if res.status_code == 200:
            return [i['snippet']['topLevelComment']['snippet']['textDisplay'] for i in res.json().get('items',[])], "ì„±ê³µ"
    except: pass
    return [], "ì‹¤íŒ¨"

def calc_match(news_item, query_nouns, text):
    tn = set(extract_nouns(news_item['title'])); dn = set(extract_nouns(news_item['desc']))
    qn = set(query_nouns)
    t_score = 1.0 if len(qn & tn) >= 2 else 0.5 if len(qn & tn) >= 1 else 0
    c_cnt = sum(1 for n in dn if n in text)
    c_score = 1.0 if (len(dn)>0 and c_cnt/len(dn)>=0.3) else 0.5 if (len(dn)>0 and c_cnt/len(dn)>=0.15) else 0
    return int((t_score*0.3 + c_score*0.7)*100)

def analyze_comments(comments, text):
    if not comments: return [], 0, "ë¶„ì„ ë¶ˆê°€"
    cn = extract_nouns(" ".join(comments))
    top = Counter(cn).most_common(5)
    ctx = set(extract_nouns(text))
    match = sum(1 for w,c in top if w in ctx)
    score = int(match/len(top)*100) if top else 0
    msg = "âœ… ì¼ì¹˜" if score>=60 else "âš ï¸ í˜¼ì¬" if score>=20 else "âŒ ë¶ˆì¼ì¹˜"
    return [f"{w}({c})" for w,c in top], score, msg

def clean_html(raw): return BeautifulSoup(raw, "html.parser").get_text()

def run_main(url):
    intel = train_ve(); loading_seq(intel)
    vid = re.search(r'(?:v=|\/)([0-9A-Za-z_-]{11}).*', url)
    if vid: vid = vid.group(1)
    
    with yt_dlp.YoutubeDL({'quiet':True, 'skip_download':True}) as ydl:
        try:
            info = ydl.extract_info(url, download=False)
            title = info.get('title',''); uploader = info.get('uploader','')
            tags = info.get('tags',[]); desc = info.get('description','')
            
            # 1. ìë§‰ ìˆ˜ì§‘ (ì—…ê·¸ë ˆì´ë“œë¨)
            trans, t_status = fetch_transcript(info)
            full_text = trans if trans else desc
            
            # 2. ìë§‰ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì‹ ê·œ)
            trans_keywords = extract_top_keywords(full_text)
            
            # 3. ì¿¼ë¦¬ ìƒì„± (ìë§‰ í‚¤ì›Œë“œ ë°˜ì˜)
            query = generate_query(title, tags, trans_keywords)
            
            ts, fs = ve.analyze_position(query + " " + title)
            v_score = int(fs*10) - int(ts*10) # ê°€ì¤‘ì¹˜ ì¡°ì •
            
            # ë‰´ìŠ¤ ê²€ìƒ‰ (XML ë°©ì‹ ìœ ì§€ - v47.1 ê¸°ì¤€)
            news_res = []; max_match = 0
            try:
                rss = f"https://news.google.com/rss/search?q={requests.utils.quote(query)}&hl=ko&gl=KR"
                r = requests.get(rss, timeout=5)
                root = ET.fromstring(r.content)
                items = root.findall('.//item')
                for item in items[:3]:
                    nt = item.find('title').text
                    nd = clean_html(item.find('description').text) if item.find('description') is not None else ""
                    m = calc_match({'title':nt, 'desc':nd}, extract_nouns(query), full_text)
                    if m > max_match: max_match = m
                    news_res.append({"ë‰´ìŠ¤ ì œëª©": nt, "ì¼ì¹˜ë„": f"{m}%"})
            except: pass
            
            cmts, c_st = fetch_comments(vid)
            top_kw, rel_scr, rel_msg = analyze_comments(cmts, title + " " + full_text)
            red_cnt = sum(1 for c in cmts for k in ['ê°€ì§œ','ì£¼ì‘','ì„ ë™'] if k in c)
            
            n_score = 0; silent = 0; mismatch = 0
            is_silent = (len(news_res) == 0) or (len(news_res) > 0 and max_match < 20)
            agitation = full_text.count('ì¶©ê²©') + full_text.count('ê²½ì•…')
            
            if is_silent:
                if agitation >= 3: silent = 40; v_score *= 2
                else: mismatch = 10
            elif red_cnt > 0:
                n_score = 25 if max_match < 60 else int((max_match/100)**2 * 65) * -1
            else:
                n_score = int((max_match/100)**2 * 45) * -1
                
            if check_official(uploader): n_score = -50; silent = 0; mismatch = 0
            
            tag_abuse = check_tags(title, tags, uploader)
            total = 50 + v_score + n_score + silent + mismatch + tag_abuse
            prob = max(5, min(99, total))
            
            save_analysis(uploader, title, prob, url, query)
            
            st.subheader("ğŸ•µï¸ í•µì‹¬ ë¶„ì„ ì§€í‘œ")
            c1,c2,c3 = st.columns(3)
            c1.metric("ê°€ì§œë‰´ìŠ¤ í™•ë¥ ", f"{prob}%", f"{total-50}")
            c2.metric("AI íŒì •", "ğŸš¨ ìœ„í—˜" if prob>60 else "ğŸŸ¢ ì•ˆì „" if prob<30 else "ğŸŸ  ì£¼ì˜")
            c3.metric("ì§€ëŠ¥ ë ˆë²¨", intel)
            
            if silent: st.error("ğŸ”‡ ì¹¨ë¬µì˜ ë©”ì•„ë¦¬: ê·¼ê±° ë¶€ì¡±")
            if check_official(uploader): st.success(f"ğŸ›¡ï¸ ê³µì‹ ì–¸ë¡ ì‚¬({uploader})")
            
            st.divider()
            c1,c2 = st.columns([1,1])
            with c1:
                st.info(f"ğŸ¯ ì¿¼ë¦¬: {query}")
                st.caption(f"ì¶”ì¶œ í‚¤ì›Œë“œ: {', '.join(trans_keywords)}")
                st.write("**ì˜ìƒ ìš”ì•½**"); st.caption(summarize(full_text))
                st.table(pd.DataFrame([["ê¸°ë³¸",50],["ë²¡í„°",v_score],["ë‰´ìŠ¤",n_score],["í˜ë„í‹°",silent+mismatch],["íƒœê·¸ì˜¤ìš©",tag_abuse]], columns=["í•­ëª©","ì ìˆ˜"]))
            with c2:
                colored_bar("ì§„ì‹¤", ts, "green"); colored_bar("ê±°ì§“", fs, "red")
                st.write(f"**ë‰´ìŠ¤ ({len(news_res)}ê±´)**"); st.table(news_res) if news_res else st.warning("ë‰´ìŠ¤ ì—†ìŒ")
                st.write("**ì—¬ë¡ **"); st.caption(f"{rel_msg} (ë…¼ë€ì–´ {red_cnt}íšŒ)")
                
        except Exception as e: st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}")

# --- [App] ---
st.title("âš–ï¸ Triple-Evidence Intelligence Forensic v47.2")
url = st.text_input("ğŸ”— ìœ íŠœë¸Œ URL")
if st.button("ğŸš€ ë¶„ì„ ì‹œì‘") and url: run_main(url)

st.divider()
st.subheader("ğŸ—‚ï¸ í•™ìŠµ ë°ì´í„° (Cloud)")
try:
    df = pd.DataFrame(supabase.table("analysis_history").select("*").order("id", desc=True).execute().data)
    if not df.empty:
        if st.session_state["is_admin"]:
            df['Delete'] = False
            cols = ['Delete'] + [c for c in df.columns if c != 'Delete']
            df = df[cols]
            ed = st.data_editor(df, column_config={"Delete": st.column_config.CheckboxColumn("ì‚­ì œ", default=False)}, disabled=["id","video_title","fake_prob"], hide_index=True, use_container_width=True)
            to_del = ed[ed.Delete]
            if not to_del.empty:
                if st.button(f"ğŸ—‘ï¸ {len(to_del)}ê±´ ì‚­ì œ"):
                    for i, r in to_del.iterrows(): supabase.table("analysis_history").delete().eq("id", r['id']).execute()
                    st.success("ì‚­ì œë¨"); time.sleep(1); st.rerun()
        else: st.dataframe(df, hide_index=True)
    else: st.info("ë°ì´í„° ì—†ìŒ")
except: pass
