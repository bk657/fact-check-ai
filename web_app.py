import streamlit as st
from supabase import create_client, Client
import re
import requests
import time
import random
import math
from datetime import datetime
from collections import Counter
import yt_dlp
import pandas as pd
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup

# --- [1. ÏãúÏä§ÌÖú ÏÑ§Ï†ï] ---
st.set_page_config(page_title="Fact-Check Center v48.1", layout="wide", page_icon="‚öñÔ∏è")

# üåü SecretsÏóêÏÑú ÌÇ§ Í∞ÄÏ†∏Ïò§Í∏∞
try:
    YOUTUBE_API_KEY = st.secrets["YOUTUBE_API_KEY"]
    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    ADMIN_PASSWORD = st.secrets["ADMIN_PASSWORD"]
except:
    st.error("‚ùå ÌïÑÏàò ÌÇ§Í∞Ä ÏÑ§Ï†ïÎêòÏßÄ ÏïäÏïòÏäµÎãàÎã§. Streamlit SecretsÎ•º ÌôïÏù∏Ìï¥Ï£ºÏÑ∏Ïöî.")
    st.stop()

@st.cache_resource
def init_supabase():
    return create_client(SUPABASE_URL, SUPABASE_KEY)

supabase = init_supabase()

# --- [Í¥ÄÎ¶¨Ïûê Ïù∏Ï¶ù] ---
if "is_admin" not in st.session_state: st.session_state["is_admin"] = False
with st.sidebar:
    st.header("üõ°Ô∏è Í¥ÄÎ¶¨Ïûê Î©îÎâ¥")
    with st.form("login_form"):
        password_input = st.text_input("Í¥ÄÎ¶¨Ïûê ÎπÑÎ∞ÄÎ≤àÌò∏", type="password")
        if st.form_submit_button("Î°úÍ∑∏Ïù∏"):
            if password_input == ADMIN_PASSWORD:
                st.session_state["is_admin"] = True; st.rerun()
            else: st.session_state["is_admin"] = False; st.error("Î∂àÏùºÏπò")
    if st.session_state["is_admin"]:
        st.success("‚úÖ Í¥ÄÎ¶¨Ïûê Ïù∏Ï¶ùÎê®"); 
        if st.button("Î°úÍ∑∏ÏïÑÏõÉ"): st.session_state["is_admin"] = False; st.rerun()

# --- [ÏÉÅÏàò ÏÑ§Ï†ï] ---
WEIGHT_NEWS_DEFAULT = 45       
WEIGHT_VECTOR = 35     
WEIGHT_CONTENT = 15    
WEIGHT_SENTIMENT_DEFAULT = 10  
PENALTY_ABUSE = 20     
PENALTY_MISMATCH = 30
PENALTY_NO_FACT = 25
PENALTY_SILENT_ECHO = 40  

VITAL_KEYWORDS = [
    'ÏúÑÎèÖ', 'ÏÇ¨Îßù', 'Î≥ÑÏÑ∏', 'Íµ¨ÏÜç', 'Ï≤¥Ìè¨', 'Í∏∞ÏÜå', 'Ïã§Ìòï', 'ÏùëÍ∏âÏã§', 'Ïì∞Îü¨Ï†∏', 
    'Ïù¥Ìòº', 'Î∂àÌôî', 'ÌååÍ≤Ω', 'Ï∂©Í≤©', 'Í≤ΩÏïÖ', 'ÏÜçÎ≥¥', 'Í∏¥Í∏â', 'Ìè≠Î°ú', 'ÏñëÏÑ±', 
    'ÌôïÏßÑ', 'Ïã¨Ï†ïÏßÄ', 'ÎáåÏÇ¨', 'Ï§ëÌÉú', 'ÏïïÏàòÏàòÏÉâ', 'ÏÜåÌôò', 'Ìá¥ÏßÑ', 'ÌÉÑÌïµ', 'ÎÇ¥ÎûÄ'
]

VIP_ENTITIES = [
    'Ïú§ÏÑùÏó¥', 'ÎåÄÌÜµÎ†π', 'Ïù¥Ïû¨Î™Ö', 'ÌïúÎèôÌõà', 'ÍπÄÍ±¥Ìù¨', 'Î¨∏Ïû¨Ïù∏', 'Î∞ïÍ∑ºÌòú', 'Ïù¥Î™ÖÎ∞ï',
    'Ìä∏ÎüºÌîÑ', 'Î∞îÏù¥Îì†', 'Ìë∏Ìã¥', 'Ï†§Î†åÏä§ÌÇ§', 'ÏãúÏßÑÌïë', 'Ï†ïÏùÄ', 
    'Ïù¥Ï§ÄÏÑù', 'Ï°∞Íµ≠', 'Ï∂îÎØ∏Ïï†', 'ÌôçÏ§ÄÌëú', 'Ïú†ÏäπÎØº', 'ÏïàÏ≤†Ïàò',
    'ÏÜêÌù•ÎØº', 'Ïù¥Í∞ïÏù∏', 'ÍπÄÎØºÏû¨', 'Î•òÌòÑÏßÑ', 'Ïû¨Ïö©', 'Ï†ïÏùòÏÑ†', 'ÏµúÌÉúÏõê'
]

OFFICIAL_CHANNELS = [
    'MBC', 'KBS', 'SBS', 'EBS', 'YTN', 'JTBC', 'TVCHOSUN', 'MBN', 'CHANNEL A', 'OBS',
    'Ï±ÑÎÑêA', 'TVÏ°∞ÏÑ†', 'Ïó∞Ìï©Îâ¥Ïä§', 'YONHAP',
    'ÌïúÍ≤®Î†à', 'Í≤ΩÌñ•', 'Ï°∞ÏÑ†ÏùºÎ≥¥', 'Ï§ëÏïôÏùºÎ≥¥', 'ÎèôÏïÑÏùºÎ≥¥', 'ÌïúÍµ≠ÏùºÎ≥¥', 'Íµ≠ÎØºÏùºÎ≥¥', 
    'ÏÑúÏö∏Ïã†Î¨∏', 'ÏÑ∏Í≥ÑÏùºÎ≥¥', 'Î¨∏ÌôîÏùºÎ≥¥', 'Îß§ÏùºÍ≤ΩÏ†ú', 'ÌïúÍµ≠Í≤ΩÏ†ú', 'ÏÑúÏö∏Í≤ΩÏ†ú',
    'CHOSUN', 'JOONGANG', 'DONGA', 'HANKYOREH', 'KYUNGHYANG'
]

STATIC_TRUTH_CORPUS = ["Î∞ïÎÇòÎûò ÏúÑÏû•Ï†ÑÏûÖ ÏùòÌòπ Î¨¥ÌòêÏùò", "ÏûÑÏòÅÏõÖ ÏΩòÏÑúÌä∏ ÏïîÌëú ÎåÄÏùë", "Ï†ïÌù¨Ïõê ÍµêÏàò Ï†ÄÏÜçÎÖ∏Ìôî", "ÎåÄÏ†Ñ Ï∂©ÎÇ® ÌñâÏ†ï ÌÜµÌï©", "ÏÑ†Í±∞ Ï∂úÎßà ÏÑ†Ïñ∏", "Í∞ïÌõàÏãù ÏùòÏõê Ï∂úÎßàÏÑ§"]
STATIC_FAKE_CORPUS = ["Ï∂©Í≤© Ìè≠Î°ú Í≤ΩÏïÖ", "Í∏¥Í∏â ÏÜçÎ≥¥ ÏÜåÎ¶Ñ", "Ïù¥Ïû¨Î™Ö ÌïúÎèôÌõà Ï∂©Í≤© Î∞úÏñ∏", "Í≤∞Íµ≠ Íµ¨ÏÜç ÏòÅÏû• Î∞úÎ∂Ä", "Î∞©ÏÜ° Î∂àÍ∞Ä ÏòÅÏÉÅ Ïú†Ï∂ú", "ÍøàÏÜç Í≥ÑÏãú ÏòàÏñ∏", "ÏÇ¨Ìòï ÏÑ†Í≥† ÏßëÌñâ", "Í±¥Í∞ï ÏïÖÌôî ÏúÑÎèÖÏÑ§"]

class VectorEngine:
    def __init__(self):
        self.vocab = set(); self.truth_vectors = []; self.fake_vectors = []
    def tokenize(self, text): return re.findall(r'[Í∞Ä-Ìû£]{2,}', text)
    def build_vocabulary(self, corpus):
        for text in corpus: self.vocab.update(self.tokenize(text))
        self.vocab = sorted(list(self.vocab))
    def text_to_vector(self, text):
        tokens = self.tokenize(text); token_counts = Counter(tokens); vector = []
        for word in self.vocab: vector.append(token_counts[word])
        return vector
    def cosine_similarity(self, vec1, vec2):
        dot = sum(a * b for a, b in zip(vec1, vec2))
        mag1 = math.sqrt(sum(a * a for a in vec1)); mag2 = math.sqrt(sum(b * b for b in vec2))
        return dot / (mag1 * mag2) if mag1 * mag2 > 0 else 0.0
    def train(self, truth, fake):
        self.build_vocabulary(truth + fake)
        self.truth_vectors = [self.text_to_vector(t) for t in truth]
        self.fake_vectors = [self.text_to_vector(t) for t in fake]
    def analyze_position(self, query):
        q_vec = self.text_to_vector(query)
        max_t = max([self.cosine_similarity(q_vec, v) for v in self.truth_vectors] or [0])
        max_f = max([self.cosine_similarity(q_vec, v) for v in self.fake_vectors] or [0])
        return max_t, max_f

vector_engine = VectorEngine()

def save_analysis(channel, title, prob, url, keywords):
    try: supabase.table("analysis_history").insert({
        "channel_name": channel, "video_title": title, "fake_prob": prob, 
        "analysis_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'), 
        "video_url": url, "keywords": keywords}).execute()
    except: pass

def train_dynamic_vector_engine():
    try:
        dt = [r['video_title'] for r in supabase.table("analysis_history").select("video_title").lt("fake_prob", 30).execute().data]
        df = [r['video_title'] for r in supabase.table("analysis_history").select("video_title").gt("fake_prob", 70).execute().data]
    except: dt, df = [], []
    vector_engine.train(STATIC_TRUTH_CORPUS + dt, STATIC_FAKE_CORPUS + df)
    return len(STATIC_TRUTH_CORPUS + dt) + len(STATIC_FAKE_CORPUS + df)

# --- [ÎàÑÎùΩÎêòÏóàÎçò Helper Functions Î≥µÍµ¨] ---
def colored_progress_bar(label, percent, color):
    st.markdown(f"""
        <div style="margin-bottom: 10px;">
            <div style="display: flex; justify-content: space-between; margin-bottom: 3px;">
                <span style="font-size: 13px; font-weight: 600; color: #555;">{label}</span>
                <span style="font-size: 13px; font-weight: 700; color: {color};">{round(percent * 100, 1)}%</span>
            </div>
            <div style="background-color: #eee; border-radius: 5px; height: 8px; width: 100%;">
                <div style="background-color: {color}; height: 8px; width: {percent * 100}%; border-radius: 5px;"></div>
            </div>
        </div>
        """, unsafe_allow_html=True)

def witty_loading_sequence(count):
    messages = [
        f"üß† [Intelligence Level: {count}] ÎàÑÏ†Å ÏßÄÏãù Î°úÎìú Ï§ë...",
        "üîç Ï†úÎ™©Í≥º ÏûêÎßâÏùÑ ÏúµÌï©ÌïòÏó¨ 'ÌïòÏù¥Î∏åÎ¶¨Îìú ÏøºÎ¶¨' ÏÉùÏÑ± Ï§ë...",
        "üéØ Îâ¥Ïä§ Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï†ïÎ∞Ä ÌÉÄÍ≤© Ï§ë...",
        "üöÄ ÏúÑÏÑ±Ïù¥ Ïú†ÌäúÎ∏å Î≥∏ÏÇ¨ ÏÉÅÍ≥µÏùÑ ÏßÄÎÇòÍ∞ÄÎäî Ï§ë..."
    ]
    with st.status("üïµÔ∏è Hybrid Core v48.1 Í∞ÄÎèô Ï§ë...", expanded=True) as status:
        for msg in messages:
            st.write(msg)
            time.sleep(0.4)
        st.write("‚úÖ Î∂ÑÏÑù Ï§ÄÎπÑ ÏôÑÎ£å!")
        status.update(label="Î∂ÑÏÑù ÏôÑÎ£å!", state="complete", expanded=False)

# --- [Advanced Logic Functions] ---
def get_noise_words():
    return ['Ï∂©Í≤©', 'Í≤ΩÏïÖ', 'Ïã§Ï≤¥', 'ÎÇúÎ¶¨', 'Í≥µÍ∞ú', 'Î∞òÏùë', 'Î™ÖÎã®', 'ÎèôÏòÅÏÉÅ', 'ÏÇ¨ÏßÑ', 'ÏßëÏïà', 'ÏÜçÎ≥¥', 
            'Îã®ÎèÖ', 'Í≤∞Íµ≠', 'MBC', 'Îâ¥Ïä§', 'Ïù¥ÎØ∏ÏßÄ', 'ÎÑàÎ¨¥', 'Îã§Î•∏', 'ÏïåÍ≥†Î≥¥Îãà', '„Ñ∑„Ñ∑', 'ÏßÑÏßú', 
            'Ï†ïÎßê', 'ÏòÅÏÉÅ', 'ÏÇ¨Îûå', 'ÏÉùÍ∞Å', 'Ïò§ÎäòÎ∞§', 'Ïò§Îäò', 'ÎÇ¥Ïùº', 'ÏßÄÍ∏à', 'Î™ªÎÑòÍ∏¥Îã§', 'ÎÑòÍ∏¥Îã§', 
            'Ïù¥Ïú†', 'Ïôú', 'Ïïà', 'ÎåÄÎ∂ÄÎ∂Ñ', 'Î™®Î•¥ÏßÄÎßå', 'ÏûàÎäî', 'ÏóÜÎäî', 'ÌïòÎäî', 'Î™∏ÏùÑ', 'Î™∏', 'Í±¥Í∞ï']

def extract_nouns(text):
    noise = get_noise_words()
    nouns = re.findall(r'[Í∞Ä-Ìû£A-Za-z0-9]{2,}', text)
    return [n for n in nouns if n not in noise]

def generate_hybrid_query(title, hashtags, transcript):
    title_text = title + " " + " ".join([h.replace("#", "") for h in hashtags])
    transcript_text = transcript if transcript else ""
    
    title_nouns = extract_nouns(title_text)
    transcript_nouns = extract_nouns(transcript_text)
    transcript_counter = Counter(transcript_nouns)
    top_transcript_nouns = [word for word, count in transcript_counter.most_common(3)]
    
    vip_found = [vip for vip in VIP_ENTITIES if vip in title_text]
    vital_found = [vital for vital in VITAL_KEYWORDS if vital in title_text]
    
    final_query = []
    
    if vip_found:
        final_query.extend(vip_found)
        if vital_found: final_query.extend(vital_found)
        for t_noun in title_nouns:
            if t_noun not in final_query and t_noun not in VIP_ENTITIES:
                final_query.append(t_noun)
                break 
    else:
        final_query.extend(title_nouns[:2]) 
        for tr_noun in top_transcript_nouns:
            if tr_noun not in final_query:
                final_query.append(tr_noun)
                if len(final_query) >= 3: break 
                
    return " ".join(final_query)

def summarize_transcript(text, max_sentences=3):
    if not text or len(text) < 50: return "‚ö†Ô∏è ÏöîÏïΩÌï† ÏûêÎßâ ÎÇ¥Ïö©Ïù¥ Ï∂©Î∂ÑÌïòÏßÄ ÏïäÏäµÎãàÎã§."
    sentences = re.split(r'(?<=[.?!])\s+', text)
    if len(sentences) <= max_sentences: return text
    nouns = re.findall(r'[Í∞Ä-Ìû£]{2,}', text); word_freq = Counter(nouns); ranked_sentences = []
    for i, sent in enumerate(sentences):
        sent_nouns = re.findall(r'[Í∞Ä-Ìû£]{2,}', sent)
        if not sent_nouns: continue
        score = sum(word_freq[w] for w in sent_nouns)
        if 10 < len(sent) < 150: ranked_sentences.append((i, sent, score / len(sent_nouns)))
    top_sentences = sorted(ranked_sentences, key=lambda x: x[2], reverse=True)[:max_sentences]
    top_sentences.sort(key=lambda x: x[0])
    return f"üìå **ÌïµÏã¨ ÏöîÏïΩ**: {' '.join([s[1] for s in top_sentences])}"

def clean_html(raw_html): return BeautifulSoup(raw_html, "html.parser").get_text()

def detect_ai_content(info):
    is_ai, reasons = False, []
    ai_keywords = ['ai', 'artificial intelligence', 'chatgpt', 'midjourney', 'sora', 'deepfake', 'synthetic', 'Ïù∏Í≥µÏßÄÎä•', 'Îî•ÌéòÏù¥ÌÅ¨', 'Í∞ÄÏÉÅÏù∏Í∞Ñ', 'Î≤ÑÏ∂îÏñº', 'gpt']
    text_to_check = (info.get('title', '') + " " + info.get('description', '') + " " + " ".join(info.get('tags', []))).lower()
    for kw in ai_keywords:
        if re.search(r'\b{}\b'.format(re.escape(kw)), text_to_check): is_ai = True; reasons.append(f"ÌÇ§ÏõåÎìú Í∞êÏßÄ: {kw}"); break
    return is_ai, ", ".join(reasons)

def check_is_official(channel_name):
    norm_name = channel_name.upper().replace(" ", "")
    for official in OFFICIAL_CHANNELS:
        if official in norm_name: return True
    return False

def count_sensational_words(text):
    triggers = ['Ï∂©Í≤©', 'Í≤ΩÏïÖ', 'Ïã§Ï≤¥', 'Ìè≠Î°ú', 'ÎÇúÎ¶¨', 'ÏÜçÎ≥¥', 'Í∏¥Í∏â', 'ÏÜåÎ¶Ñ', '„Ñ∑„Ñ∑', 'ÏßÑÏßú', 'Í≤∞Íµ≠', 'Í≥ÑÏãú', 'ÏòàÏñ∏', 'ÏúÑÎèÖ', 'ÏÇ¨Îßù', 'Ï§ëÌÉú']
    count = 0
    for w in triggers: count += text.count(w)
    return count

def check_tag_abuse(title, hashtags, channel_name):
    if check_is_official(channel_name): return 0, "Í≥µÏãù Ï±ÑÎÑê Î©¥Ï†ú"
    if not hashtags: return 0, "Ìï¥ÏãúÌÉúÍ∑∏ ÏóÜÏùå"
    title_nouns = extract_nouns(title); tag_nouns = set()
    for t in hashtags: tag_nouns.add(t.replace("#", "").split(":")[-1].strip())
    if len(tag_nouns) < 2: return 0, "ÏñëÌò∏"
    if not set(title_nouns).intersection(tag_nouns): return PENALTY_ABUSE, "üö® Ïã¨Í∞Å (Î∂àÏùºÏπò)"
    return 0, "ÏñëÌò∏"

def fetch_real_transcript(info_dict):
    sub_url = None
    if 'subtitles' in info_dict and 'ko' in info_dict['subtitles']:
        for fmt in info_dict['subtitles']['ko']:
            if fmt['ext'] == 'vtt': sub_url = fmt['url']; break
    if not sub_url and 'automatic_captions' in info_dict and 'ko' in info_dict['automatic_captions']:
        for fmt in info_dict['automatic_captions']['ko']:
            if fmt['ext'] == 'vtt': sub_url = fmt['url']; break
    if not sub_url: return None, "ÏûêÎßâ ÏóÜÏùå (ÏÑ§Î™ÖÎûÄ ÎåÄÏ≤¥)"
    try:
        response = requests.get(sub_url)
        if response.status_code == 200:
            lines = response.text.splitlines(); clean_lines = []; seen = set()
            for line in lines:
                line = line.strip()
                if '-->' in line or line == 'WEBVTT' or not line: continue
                line = re.sub(r'<[^>]+>', '', line)
                if line and line not in seen: clean_lines.append(line); seen.add(line)
            return " ".join(clean_lines), "‚úÖ Ïã§Ï†ú ÏûêÎßâ ÏàòÏßë ÏÑ±Í≥µ"
    except: pass
    return None, "ÏûêÎßâ Îã§Ïö¥Î°úÎìú Ïã§Ìå®"

def fetch_comments_via_api(video_id):
    url = "https://www.googleapis.com/youtube/v3/commentThreads"
    params = {'part': 'snippet', 'videoId': video_id, 'key': YOUTUBE_API_KEY, 'maxResults': 100, 'order': 'relevance'}
    try:
        res = requests.get(url, params=params)
        if res.status_code == 200:
            items = res.json().get('items', [])
            top_comments = [item['snippet']['topLevelComment']['snippet']['textDisplay'] for item in items]
            return top_comments[:50], f"‚úÖ API ÏàòÏßë ÏÑ±Í≥µ (Top {len(top_comments[:50])} by Likes)"
        elif res.status_code == 403: return [], "‚ö†Ô∏è API Í∂åÌïú Ïò§Î•ò"
        elif res.status_code == 404: return [], "‚ö†Ô∏è ÎåìÍ∏Ä ÏÇ¨Ïö© Ï§ëÏßÄÎê®"
        else: return [], f"‚ö†Ô∏è API Ïò§Î•ò ({res.status_code})"
    except: return [], f"‚ùå API ÌÜµÏã† Ïã§Ìå®"

def calculate_dual_match(news_item, query_nouns, transcript):
    if not news_item or not transcript: return 0, 0, 0
    news_title = news_item.get('title', ''); title_nouns = extract_nouns(news_title)
    intersection = len(set(query_nouns).intersection(set(title_nouns)))
    title_score = 1.0 if intersection >= 2 else (0.5 if intersection == 1 else 0)
    news_desc = news_item.get('desc', ''); desc_nouns = extract_nouns(news_desc)
    if not desc_nouns: content_score = 0
    else:
        match_count = 0
        for noun in desc_nouns:
            if noun in transcript: match_count += 1
        content_ratio = match_count / len(desc_nouns)
        content_score = 1.0 if content_ratio >= 0.3 else (0.5 if content_ratio >= 0.15 else 0)
    total_score = (title_score * 0.3) + (content_score * 0.7)
    return int(total_score * 100), int(title_score * 100), int(content_score * 100)

def analyze_comment_relevance(comments, context_text):
    if not comments: return [], 0, "Î∂ÑÏÑù Î∂àÍ∞Ä"
    all_comments_text = " ".join(comments); comment_nouns = extract_nouns(all_comments_text)
    if not comment_nouns: return [], 0, "Ïú†Ìö® ÌÇ§ÏõåÎìú ÏóÜÏùå"
    top_keywords = Counter(comment_nouns).most_common(5)
    context_nouns = extract_nouns(context_text); match_count = 0; context_set = set(context_nouns)
    for word, cnt in top_keywords:
        if word in context_set: match_count += 1
    relevance_score = int((match_count / len(top_keywords)) * 100)
    msg = "‚úÖ Ï£ºÏ†ú ÏßëÏ§ë" if relevance_score >= 60 else "‚ö†Ô∏è ÏùºÎ∂Ä Í¥ÄÎ†®" if relevance_score >= 20 else "‚ùå Î¨¥Í¥Ä/Ïû°Îã¥"
    return [f"{w}({c})" for w, c in top_keywords], relevance_score, msg

def check_red_flags(comments):
    keywords = ['Í∞ÄÏßúÎâ¥Ïä§', 'Í∞ÄÏßú Îâ¥Ïä§', 'Ï£ºÏûë', 'ÏÇ¨Í∏∞', 'Í±∞ÏßìÎßê', 'ÌóàÏúÑ', 'Íµ¨Îùº', 'Ìï©ÏÑ±', 'ÏÑ†Îèô', 'ÏÜåÏÑ§']
    count = 0; detected = []
    for c in comments:
        for k in keywords:
            if k in c: count += 1; detected.append(k)
    return count, list(set(detected))

# --- [Main] ---
def run_forensic_main(url):
    total_intelligence = train_dynamic_vector_engine()
    witty_loading_sequence(total_intelligence)
    video_id = re.search(r'(?:v=|\/)([0-9A-Za-z_-]{11}).*', url)
    if video_id: video_id = video_id.group(1)
    
    ydl_opts = {'quiet': True, 'skip_download': True, 'writesubtitles': True, 'subtitleslangs': ['ko'], 'extractor_args': {'youtube': {'skip': ['dash', 'hls']}}}
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        try:
            info = ydl.extract_info(url, download=False)
            title = info.get('title', ''); uploader = info.get('uploader', '')
            tags = info.get('tags', []); desc = info.get('description', '')
            
            transcript_text, transcript_status = fetch_real_transcript(info)
            analysis_text = transcript_text if transcript_text else desc
            
            refined_query = generate_hybrid_query(title, tags, transcript_text)
            
            is_official = check_is_official(uploader)
            is_ai_content, ai_reason = detect_ai_content(info)
            abuse_score, abuse_status = check_tag_abuse(title, tags, uploader)
            summary_text = summarize_transcript(analysis_text)
            agitation_count = count_sensational_words(analysis_text + title)
            agitation_level = "ÎÜíÏùå (ÏúÑÌóò)" if agitation_count > 3 else "Î≥¥ÌÜµ"
            
            # Î≤°ÌÑ∞ Î∂ÑÏÑù
            t_sim, f_sim = vector_engine.analyze_position(refined_query + " " + title)
            t_impact = int(t_sim * 35) * -1; f_impact = int(f_sim * 35)
            
            # Îâ¥Ïä§ Í≤ÄÏÉâ
            news_ev = []; max_dual_score = 0; news_cnt = 0
            try:
                rss_url = f"https://news.google.com/rss/search?q={requests.utils.quote(refined_query)}&hl=ko&gl=KR"
                root = ET.fromstring(requests.get(rss_url, timeout=5).content)
                items = root.findall('.//item'); news_cnt = len(items)
                for item in items[:3]:
                    nt = item.find('title').text; nd = clean_html(item.find('description').text)
                    total, t_sc, c_sc = calculate_dual_match({'title': nt, 'desc': nd}, extract_nouns(refined_query), analysis_text)
                    if total > max_dual_score: max_dual_score = total
                    news_ev.append({"Îâ¥Ïä§ Ï†úÎ™©": nt, "ÏùºÏπòÎèÑ": f"{total}%"})
            except: pass
            
            # ÎåìÍ∏Ä Î∂ÑÏÑù
            comments, c_status = fetch_comments_via_api(video_id)
            top_k, rel_score, rel_msg = analyze_comment_relevance(comments, title + " " + analysis_text)
            red_cnt, red_words = check_red_flags(comments)
            is_controversial = red_cnt > 0
            
            # Ï†êÏàò ÏÇ∞Ï†ï
            news_score = 0; silent_penalty = 0; mismatch_penalty = 0
            is_effective_silence = (news_cnt == 0) or (news_cnt > 0 and max_dual_score < 20)
            
            if is_effective_silence:
                if agitation_count >= 3: silent_penalty = PENALTY_SILENT_ECHO; t_impact *= 2; f_impact *= 2
                else: mismatch_penalty = 10 
            elif is_controversial:
                if max_dual_score < 60: news_score = PENALTY_NO_FACT 
                else: news_score = int((max_dual_score/100)**2 * 65) * -1
            else:
                news_score = int((max_dual_score/100)**2 * 45) * -1
            
            if is_official: news_score = -50; mismatch_penalty = 0; silent_penalty = 0
            
            total_score = 50 + t_impact + f_impact + news_score + silent_penalty + mismatch_penalty + abuse_score
            final_prob = max(5, min(99, total_score))
            
            save_analysis(uploader, title, final_prob, url, refined_query)
            
            # --- UI Output ---
            st.subheader("üïµÔ∏è ÌïµÏã¨ Î∂ÑÏÑù ÏßÄÌëú")
            c1, c2, c3 = st.columns(3)
            c1.metric("Í∞ÄÏßúÎâ¥Ïä§ ÌôïÎ•†", f"{final_prob}%", delta=f"{total_score-50}")
            c2.metric("ÌåêÏ†ï", "üö® ÏúÑÌóò" if final_prob>60 else "üü¢ ÏïàÏ†Ñ" if final_prob<30 else "üü† Ï£ºÏùò")
            c3.metric("AI ÏßÄÎä• Î†àÎ≤®", f"{total_intelligence}", "+1")
            
            if is_official: st.success(f"üõ°Ô∏è Í≥µÏãù Ïñ∏Î°†ÏÇ¨({uploader})ÏûÖÎãàÎã§.")
            if is_ai_content: st.warning(f"ü§ñ AI ÏΩòÌÖêÏ∏† Í∞êÏßÄ: {ai_reason}")
            if silent_penalty: st.error("üîá Ïπ®Î¨µÏùò Î©îÏïÑÎ¶¨: ÏûêÍ∑πÏ†ÅÏù¥ÎÇò Í∑ºÍ±∞Í∞Ä Î∂ÄÏ°±Ìï©ÎãàÎã§.")
            
            st.divider()
            col1, col2 = st.columns([1, 1.4])
            with col1:
                st.info(f"üéØ ÌïÄÌè¨Ïù∏Ìä∏ Í≤ÄÏÉâÏñ¥: {refined_query}")
                st.write("**ÏòÅÏÉÅ ÏöîÏïΩ**"); st.write(summary_text)
                st.table(pd.DataFrame([["Í∏∞Î≥∏ Ï†êÏàò", 50], ["Î≤°ÌÑ∞ Î∂ÑÏÑù", t_impact+f_impact], ["Îâ¥Ïä§ ÎåÄÏ°∞", news_score], ["Ïπ®Î¨µ/Î∂àÏùºÏπò", silent_penalty+mismatch_penalty]], columns=["Ìï≠Î™©", "Ï†êÏàò"]))
            with col2:
                colored_progress_bar("ÏßÑÏã§ Ïú†ÏÇ¨ÎèÑ", t_sim, "green"); colored_progress_bar("Í±∞Ïßì Ïú†ÏÇ¨ÎèÑ", f_sim, "red")
                st.write(f"**Îâ¥Ïä§ Í≤ÄÏÉâ ({news_cnt}Í±¥)**"); st.table(pd.DataFrame(news_ev)) if news_ev else st.warning("Í¥ÄÎ†® Îâ¥Ïä§ ÏóÜÏùå")
                st.write("**ÎåìÍ∏Ä Î∂ÑÏÑù**"); st.write(f"Ïó¨Î°†: {rel_msg}, ÎÖºÎûÄ ÌÇ§ÏõåÎìú: {red_cnt}Ìöå")

        except Exception as e: st.error(f"Î∂ÑÏÑù Ï§ë Ïò§Î•ò: {e}")

st.title("‚öñÔ∏è Triple-Evidence Intelligence Forensic v48.1")
url = st.text_input("üîó Ïú†ÌäúÎ∏å URL ÏûÖÎ†•")
if st.button("üöÄ Î∂ÑÏÑù ÏãúÏûë") and url: run_forensic_main(url)

st.divider()
st.subheader("üóÇÔ∏è ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ Í¥ÄÎ¶¨ (Cloud)")
try:
    df = pd.DataFrame(supabase.table("analysis_history").select("*").order("id", desc=True).execute().data)
    if not df.empty and st.session_state["is_admin"]:
        # Í¥ÄÎ¶¨ÏûêÏö© ÏÇ≠Ï†ú UI
        edited_df = st.data_editor(
            df,
            column_config={
                "Delete": st.column_config.CheckboxColumn("ÏÑ†ÌÉù ÏÇ≠Ï†ú", default=False)
            },
            disabled=["id", "analysis_date", "video_title", "keywords"],
            hide_index=True, use_container_width=True
        )
        # ÏÇ≠Ï†ú Î≤ÑÌäº (Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏóê 'Delete' Ïª¨ÎüºÏùÑ Ï∂îÍ∞ÄÌï¥ÏÑú Ï≤òÎ¶¨Ìï¥Ïïº Ìï®)
        if "Delete" not in edited_df.columns:
            edited_df["Delete"] = False # Ï¥àÍ∏∞Ìôî
            
        to_delete = edited_df[edited_df.Delete]
        if not to_delete.empty:
            if st.button(f"üóëÔ∏è ÏÑ†ÌÉùÌïú {len(to_delete)}Í±¥ ÏÇ≠Ï†ú"):
                for index, row in to_delete.iterrows():
                    supabase.table("analysis_history").delete().eq("id", row['id']).execute()
                st.success("ÏÇ≠Ï†ú ÏôÑÎ£å"); time.sleep(1); st.rerun()
                
    elif not df.empty:
        st.dataframe(df) # ÏùºÎ∞ò Ïú†Ï†ÄÎäî Î≥¥Í∏∞Îßå Í∞ÄÎä•
    else: st.info("Îç∞Ïù¥ÌÑ∞ ÏóÜÏùå")
except: pass
