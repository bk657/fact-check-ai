import streamlit as st
from supabase import create_client, Client
import re
import requests
import time
import random
import math
from datetime import datetime
from collections import Counter
import yt_dlp
import pandas as pd
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup

# --- [1. ì‹œìŠ¤í…œ ì„¤ì •] ---
st.set_page_config(page_title="Fact-Check Center v47.1 (Final Fix)", layout="wide", page_icon="âš–ï¸")

# ğŸŒŸ Secrets
try:
    YOUTUBE_API_KEY = st.secrets["YOUTUBE_API_KEY"]
    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    ADMIN_PASSWORD = st.secrets["ADMIN_PASSWORD"]
except:
    st.error("âŒ í•„ìˆ˜ í‚¤(API Key, DB Key, Password)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

@st.cache_resource
def init_supabase():
    return create_client(SUPABASE_URL, SUPABASE_KEY)

supabase = init_supabase()

# --- [ê´€ë¦¬ì ì¸ì¦] ---
if "is_admin" not in st.session_state:
    st.session_state["is_admin"] = False

with st.sidebar:
    st.header("ğŸ›¡ï¸ ê´€ë¦¬ì ë©”ë‰´")
    with st.form("login_form"):
        password_input = st.text_input("ê´€ë¦¬ì ë¹„ë°€ë²ˆí˜¸", type="password")
        submit_button = st.form_submit_button("ë¡œê·¸ì¸")
        if submit_button:
            if password_input == ADMIN_PASSWORD:
                st.session_state["is_admin"] = True; st.rerun()
            else:
                st.session_state["is_admin"] = False; st.error("ë¹„ë°€ë²ˆí˜¸ ë¶ˆì¼ì¹˜")

    if st.session_state["is_admin"]:
        st.success("âœ… ê´€ë¦¬ì ì¸ì¦ë¨")
        if st.button("ë¡œê·¸ì•„ì›ƒ"):
            st.session_state["is_admin"] = False; st.rerun()
    else:
        st.info("ë°ì´í„° ì‚­ì œëŠ” ê´€ë¦¬ìë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

# --- [ìƒìˆ˜] ---
WEIGHT_NEWS_DEFAULT = 45; WEIGHT_VECTOR = 35; WEIGHT_CONTENT = 15; WEIGHT_SENTIMENT_DEFAULT = 10
PENALTY_ABUSE = 20; PENALTY_MISMATCH = 30; PENALTY_NO_FACT = 25; PENALTY_SILENT_ECHO = 40

VITAL_KEYWORDS = ['ìœ„ë…', 'ì‚¬ë§', 'ë³„ì„¸', 'êµ¬ì†', 'ì²´í¬', 'ê¸°ì†Œ', 'ì‹¤í˜•', 'ì‘ê¸‰ì‹¤', 'ì´í˜¼', 'ë¶ˆí™”', 'íŒŒê²½', 'ì¶©ê²©', 'ê²½ì•…', 'ì†ë³´', 'ê¸´ê¸‰', 'í­ë¡œ', 'ì–‘ì„±', 'í™•ì§„', 'ì‹¬ì •ì§€', 'ë‡Œì‚¬', 'ì¤‘íƒœ', 'ì••ìˆ˜ìˆ˜ìƒ‰', 'ì†Œí™˜', 'í‡´ì§„', 'íƒ„í•µ', 'ë‚´ë€']
VIP_ENTITIES = ['ìœ¤ì„ì—´', 'ëŒ€í†µë ¹', 'ì´ì¬ëª…', 'í•œë™í›ˆ', 'ê¹€ê±´í¬', 'ë¬¸ì¬ì¸', 'ë°•ê·¼í˜œ', 'ì´ëª…ë°•', 'íŠ¸ëŸ¼í”„', 'ë°”ì´ë“ ', 'í‘¸í‹´', 'ì ¤ë ŒìŠ¤í‚¤', 'ì‹œì§„í•‘', 'ì •ì€', 'ì´ì¤€ì„', 'ì¡°êµ­', 'ì¶”ë¯¸ì• ', 'í™ì¤€í‘œ', 'ìœ ìŠ¹ë¯¼', 'ì•ˆì² ìˆ˜', 'ì†í¥ë¯¼', 'ì´ê°•ì¸', 'ê¹€ë¯¼ì¬', 'ë¥˜í˜„ì§„', 'ì¬ìš©', 'ì •ì˜ì„ ', 'ìµœíƒœì›']
OFFICIAL_CHANNELS = ['MBC', 'KBS', 'SBS', 'EBS', 'YTN', 'JTBC', 'TVCHOSUN', 'MBN', 'CHANNEL A', 'OBS', 'ì±„ë„A', 'TVì¡°ì„ ', 'ì—°í•©ë‰´ìŠ¤', 'YONHAP', 'í•œê²¨ë ˆ', 'ê²½í–¥', 'ì¡°ì„ ', 'ì¤‘ì•™', 'ë™ì•„']

STATIC_TRUTH_CORPUS = ["ë°•ë‚˜ë˜ ìœ„ì¥ì „ì… ë¬´í˜ì˜", "ì„ì˜ì›… ì•”í‘œ ëŒ€ì‘", "ì •í¬ì› ì €ì†ë…¸í™”", "ëŒ€ì „ ì¶©ë‚¨ í†µí•©", "ì„ ê±° ì¶œë§ˆ ì„ ì–¸"]
STATIC_FAKE_CORPUS = ["ì¶©ê²© í­ë¡œ ê²½ì•…", "ê¸´ê¸‰ ì†ë³´ ì†Œë¦„", "ì¶©ê²© ë°œì–¸ ë…¼ë€", "êµ¬ì† ì˜ì¥ ë°œë¶€", "ì˜ìƒ ìœ ì¶œ", "ê³„ì‹œ ì˜ˆì–¸", "ì‚¬í˜• ì§‘í–‰", "ìœ„ë…ì„¤"]

class VectorEngine:
    def __init__(self): self.vocab = set(); self.truth_vectors = []; self.fake_vectors = []
    def tokenize(self, t): return re.findall(r'[ê°€-í£]{2,}', t)
    def train(self, t_corpus, f_corpus):
        for t in t_corpus + f_corpus: self.vocab.update(self.tokenize(t))
        self.vocab = sorted(list(self.vocab))
        self.truth_vectors = [self.text_to_vector(t) for t in t_corpus]
        self.fake_vectors = [self.text_to_vector(t) for t in f_corpus]
    def text_to_vector(self, text):
        c = Counter(self.tokenize(text)); return [c[w] for w in self.vocab]
    def cosine_similarity(self, v1, v2):
        dot = sum(a*b for a,b in zip(v1,v2)); mag = math.sqrt(sum(a*a for a in v1)) * math.sqrt(sum(b*b for b in v2))
        return dot/mag if mag>0 else 0
    def analyze_position(self, query):
        qv = self.text_to_vector(query)
        mt = max([self.cosine_similarity(qv, v) for v in self.truth_vectors] or [0])
        mf = max([self.cosine_similarity(qv, v) for v in self.fake_vectors] or [0])
        return mt, mf

vector_engine = VectorEngine()

def save_analysis(channel, title, prob, url, keywords):
    try: supabase.table("analysis_history").insert({"channel_name": channel, "video_title": title, "fake_prob": prob, "analysis_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'), "video_url": url, "keywords": keywords}).execute()
    except: pass

def train_dynamic_vector_engine():
    try:
        dt = [row['video_title'] for row in supabase.table("analysis_history").select("video_title").lt("fake_prob", 30).execute().data]
        df = [row['video_title'] for row in supabase.table("analysis_history").select("video_title").gt("fake_prob", 70).execute().data]
    except: dt, df = [], []
    vector_engine.train(STATIC_TRUTH_CORPUS + dt, STATIC_FAKE_CORPUS + df)
    return len(STATIC_TRUTH_CORPUS + dt) + len(STATIC_FAKE_CORPUS + df)

# --- [UI Utils] ---
def colored_progress_bar(label, percent, color):
    st.markdown(f"""<div style="margin-bottom: 10px;"><div style="display: flex; justify-content: space-between; margin-bottom: 3px;"><span style="font-size: 13px; font-weight: 600; color: #555;">{label}</span><span style="font-size: 13px; font-weight: 700; color: {color};">{round(percent * 100, 1)}%</span></div><div style="background-color: #eee; border-radius: 5px; height: 8px; width: 100%;"><div style="background-color: {color}; height: 8px; width: {percent * 100}%; border-radius: 5px;"></div></div></div>""", unsafe_allow_html=True)

def render_score_breakdown(data_list):
    style = """<style>table.score-table { width: 100%; border-collapse: separate; border-spacing: 0; border: 1px solid #e0e0e0; border-radius: 8px; overflow: hidden; font-family: sans-serif; font-size: 14px; margin-top: 10px;} table.score-table th { background-color: #f8f9fa; color: #495057; font-weight: bold; padding: 12px 15px; text-align: left; border-bottom: 1px solid #e0e0e0; } table.score-table td { padding: 12px 15px; border-bottom: 1px solid #f0f0f0; color: #333; } table.score-table tr:last-child td { border-bottom: none; } .badge { padding: 4px 8px; border-radius: 6px; font-weight: 700; font-size: 11px; display: inline-block; text-align: center; min-width: 45px; } .badge-danger { background-color: #ffebee; color: #d32f2f; } .badge-success { background-color: #e8f5e9; color: #2e7d32; } .badge-neutral { background-color: #f5f5f5; color: #757575; border: 1px solid #e0e0e0; }</style>"""
    rows = ""
    for item, score, note in data_list:
        try:
            score_num = int(score)
            badge = f'<span class="badge badge-danger">+{score_num}</span>' if score_num > 0 else f'<span class="badge badge-success">{score_num}</span>' if score_num < 0 else f'<span class="badge badge-neutral">0</span>'
        except: badge = f'<span class="badge badge-neutral">{score}</span>'
        rows += f"<tr><td>{item}<br><span style='color:#888; font-size:11px;'>{note}</span></td><td style='text-align: right;'>{badge}</td></tr>"
    st.markdown(f"{style}<table class='score-table'><thead><tr><th>ë¶„ì„ í•­ëª© (Silent Echo Protocol)</th><th style='text-align: right;'>ë³€ë™</th></tr></thead><tbody>{rows}</tbody></table>", unsafe_allow_html=True)

def witty_loading_sequence(count):
    messages = [f"ğŸ§  [Intelligence Level: {count}] ëˆ„ì  ì§€ì‹ ë¡œë“œ ì¤‘...", "ğŸ”„ 'ì£¼ì–´(Modifier)' + 'í•µì‹¬ì–´(Head)' ì—­ë°©í–¥ ê²°í•©(Back-Merge) ì¤‘...", "ğŸ¯ ë¬¸ë§¥ì„ í†µí•©í•˜ì—¬ ì™„ë²½í•œ ê²€ìƒ‰ì–´(Contextual Query) ìƒì„±...", "ğŸš€ ìœ„ì„±ì´ ìœ íŠœë¸Œ ë³¸ì‚¬ ìƒê³µì„ ì§€ë‚˜ê°€ëŠ” ì¤‘..."]
    with st.status("ğŸ•µï¸ Context Merger v47.1 ê°€ë™ ì¤‘...", expanded=True) as status:
        for msg in messages: st.write(msg); time.sleep(0.4)
        st.write("âœ… ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ!"); status.update(label="ë¶„ì„ ì™„ë£Œ!", state="complete", expanded=False)

def extract_nouns(text):
    noise = ['ì¶©ê²©', 'ê²½ì•…', 'ì‹¤ì²´', 'ë‚œë¦¬', 'ê³µê°œ', 'ë°˜ì‘', 'ëª…ë‹¨', 'ë™ì˜ìƒ', 'ì‚¬ì§„', 'ì§‘ì•ˆ', 'ì†ë³´', 'ë‹¨ë…', 'ê²°êµ­', 'MBC', 'ë‰´ìŠ¤', 'ì´ë¯¸ì§€', 'ë„ˆë¬´', 'ë‹¤ë¥¸', 'ì•Œê³ ë³´ë‹ˆ', 'ã„·ã„·', 'ì§„ì§œ', 'ì •ë§', 'ì˜ìƒ', 'ì‚¬ëŒ', 'ìƒê°', 'ì˜¤ëŠ˜ë°¤', 'ì˜¤ëŠ˜', 'ë‚´ì¼', 'ì§€ê¸ˆ', 'ëª»ë„˜ê¸´ë‹¤', 'ë„˜ê¸´ë‹¤', 'ì´ìœ ', 'ì™œ', 'ì•ˆ']
    nouns = re.findall(r'[ê°€-í£]{2,}', text)
    return list(dict.fromkeys([n for n in nouns if n not in noise]))

def generate_pinpoint_query(title, hashtags):
    clean_text = title + " " + " ".join([h.replace("#", "") for h in hashtags])
    words = clean_text.split()
    subject_chunk, object_word, vital_word = "", "", ""
    for vital in VITAL_KEYWORDS:
        if vital in clean_text: vital_word = vital; break
    for i, word in enumerate(words):
        match = re.match(r'([ê°€-í£A-Za-z0-9]+)(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì—|ì—ê²Œ|ë¡œì„œ|ë¡œ)', word)
        if match:
            noun, josa = match.group(1), match.group(2)
            if noun in ['ì˜¤ëŠ˜ë°¤', 'ì§€ê¸ˆ', 'ì´ìœ ', 'ê²°êµ­']: continue
            if not subject_chunk and josa in ['ì€', 'ëŠ”', 'ì´', 'ê°€']:
                prev_noun = ""
                if i > 0:
                    prev_word = words[i-1]
                    if re.fullmatch(r'[ê°€-í£A-Za-z0-9]+', prev_word):
                        if prev_word not in VITAL_KEYWORDS + ['ì¶©ê²©', 'ì†ë³´']: prev_noun = prev_word
                subject_chunk = f"{prev_noun} {noun}" if prev_noun else noun
            elif not object_word and josa in ['ì„', 'ë¥¼', 'ì—', 'ì—ê²Œ', 'ë¡œ']:
                if noun not in VITAL_KEYWORDS and noun not in subject_chunk: object_word = noun
    query_parts = [p for p in [subject_chunk, object_word, vital_word] if p]
    if not subject_chunk: return " ".join(extract_nouns(title)[:3])
    return " ".join(query_parts)

def summarize_transcript(text):
    if not text or len(text) < 50: return "âš ï¸ ìš”ì•½í•  ìë§‰ ë‚´ìš©ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    sents = re.split(r'(?<=[.?!])\s+', text)
    if len(sents) <= 3: return text
    freq = Counter(re.findall(r'[ê°€-í£]{2,}', text))
    ranked = sorted([(i, s, sum(freq[w] for w in re.findall(r'[ê°€-í£]{2,}',s))/len(re.findall(r'[ê°€-í£]{2,}',s) or [1])) for i,s in enumerate(sents) if 10<len(s)<150], key=lambda x:x[2], reverse=True)[:3]
    return f"ğŸ“Œ **í•µì‹¬ ìš”ì•½**: {' '.join([r[1] for r in sorted(ranked, key=lambda x:x[0])])}"

def clean_html(raw_html): return BeautifulSoup(raw_html, "html.parser").get_text()

def detect_ai_content(info):
    is_ai, reasons = False, []
    text = (info.get('title', '') + " " + info.get('description', '') + " " + " ".join(info.get('tags', []))).lower()
    for kw in ['ai', 'artificial intelligence', 'chatgpt', 'deepfake', 'synthetic', 'ì¸ê³µì§€ëŠ¥', 'ë”¥í˜ì´í¬', 'ê°€ìƒì¸ê°„']:
        if kw in text: is_ai = True; reasons.append(f"í‚¤ì›Œë“œ ê°ì§€: {kw}"); break
    return is_ai, ", ".join(reasons)

def check_is_official(channel_name):
    norm_name = channel_name.upper().replace(" ", "")
    return any(o in norm_name for o in OFFICIAL_CHANNELS)

def count_sensational_words(text):
    return sum(text.count(w) for w in ['ì¶©ê²©', 'ê²½ì•…', 'ì‹¤ì²´', 'í­ë¡œ', 'ë‚œë¦¬', 'ì†ë³´', 'ê¸´ê¸‰', 'ì†Œë¦„', 'ã„·ã„·', 'ì§„ì§œ', 'ê²°êµ­', 'ê³„ì‹œ', 'ì˜ˆì–¸', 'ìœ„ë…', 'ì‚¬ë§', 'ì¤‘íƒœ'])

def check_tag_abuse(title, hashtags, channel_name):
    if check_is_official(channel_name): return 0, "ê³µì‹ ì±„ë„ ë©´ì œ"
    if not hashtags: return 0, "í•´ì‹œíƒœê·¸ ì—†ìŒ"
    tn = set(extract_nouns(title)); tgn = set(h.replace("#", "").split(":")[-1].strip() for h in hashtags)
    if len(tgn) < 2: return 0, "ì–‘í˜¸"
    return (PENALTY_ABUSE, "ğŸš¨ ì‹¬ê° (ë¶ˆì¼ì¹˜)") if not tn.intersection(tgn) else (0, "ì–‘í˜¸")

def fetch_real_transcript(info_dict):
    try:
        url = None
        for key in ['subtitles', 'automatic_captions']:
            if key in info_dict and 'ko' in info_dict[key]:
                for fmt in info_dict[key]['ko']:
                    if fmt['ext'] == 'vtt': url = fmt['url']; break
            if url: break
        if url:
            res = requests.get(url)
            if res.status_code == 200:
                clean = []
                for line in res.text.splitlines():
                    if '-->' not in line and 'WEBVTT' not in line and line.strip():
                        t = re.sub(r'<[^>]+>', '', line).strip()
                        if t and t not in clean: clean.append(t)
                return " ".join(clean), "âœ… ì‹¤ì œ ìë§‰ ìˆ˜ì§‘ ì„±ê³µ"
    except: pass
    return None, "ìë§‰ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"

def fetch_comments_via_api(video_id):
    try:
        url = "https://www.googleapis.com/youtube/v3/commentThreads"
        res = requests.get(url, params={'part': 'snippet', 'videoId': video_id, 'key': YOUTUBE_API_KEY, 'maxResults': 50, 'order': 'relevance'})
        if res.status_code == 200:
            items = [i['snippet']['topLevelComment']['snippet']['textDisplay'] for i in res.json().get('items', [])]
            return items, f"âœ… API ìˆ˜ì§‘ ì„±ê³µ (Top {len(items)})"
    except: pass
    return [], "âŒ API í†µì‹  ì‹¤íŒ¨"

def calculate_dual_match(news_item, query_nouns, transcript):
    tn = set(extract_nouns(news_item.get('title', ''))); dn = set(extract_nouns(news_item.get('desc', '')))
    qn = set(query_nouns)
    t_score = 1.0 if len(qn & tn) >= 2 else 0.5 if len(qn & tn) >= 1 else 0
    c_cnt = sum(1 for n in dn if n in transcript)
    c_score = 1.0 if (len(dn) > 0 and c_cnt/len(dn) >= 0.3) else 0.5 if (len(dn) > 0 and c_cnt/len(dn) >= 0.15) else 0
    return int((t_score * 0.3 + c_score * 0.7) * 100)

def analyze_comment_relevance(comments, context_text):
    if not comments: return [], 0, "ë¶„ì„ ë¶ˆê°€"
    cn = extract_nouns(" ".join(comments))
    top = Counter(cn).most_common(5)
    ctx = set(extract_nouns(context_text))
    match = sum(1 for w,c in top if w in ctx)
    score = int(match/len(top)*100) if top else 0
    msg = "âœ… ì£¼ì œ ì§‘ì¤‘" if score >= 60 else "âš ï¸ ì¼ë¶€ ê´€ë ¨" if score >= 20 else "âŒ ë¬´ê´€"
    return [f"{w}({c})" for w, c in top], score, msg

def check_red_flags(comments):
    detected = [k for c in comments for k in ['ê°€ì§œë‰´ìŠ¤', 'ì£¼ì‘', 'ì‚¬ê¸°', 'ê±°ì§“ë§', 'í—ˆìœ„', 'ì„ ë™'] if k in c]
    return len(detected), list(set(detected))

# --- [Main Execution] ---
def run_forensic_main(url):
    total_intelligence = train_dynamic_vector_engine()
    witty_loading_sequence(total_intelligence)
    
    vid = re.search(r'(?:v=|\/)([0-9A-Za-z_-]{11}).*', url)
    if vid: vid = vid.group(1)

    with yt_dlp.YoutubeDL({'quiet': True, 'skip_download': True}) as ydl:
        try:
            info = ydl.extract_info(url, download=False)
            title = info.get('title', ''); uploader = info.get('uploader', '')
            tags = info.get('tags', []); desc = info.get('description', '')
            
            trans, t_status = fetch_real_transcript(info)
            full_text = trans if trans else desc
            
            is_official = check_is_official(uploader)
            
            # ğŸŒŸ [Fix] ë³€ìˆ˜ëª… í†µì¼ (is_ai_content -> is_ai_content)
            is_ai_content, ai_msg = detect_ai_content(info) 
            
            w_news = 70 if is_ai_content else WEIGHT_NEWS_DEFAULT
            w_vec = 10 if is_ai_content else WEIGHT_VECTOR
            
            query = generate_pinpoint_query(title, tags)
            hashtag_display = ", ".join([f"#{t}" for t in tags]) if tags else "í•´ì‹œíƒœê·¸ ì—†ìŒ"
            abuse_score, abuse_msg = check_tag_abuse(title, tags, uploader)
            summary = summarize_transcript(full_text)
            agitation = count_sensational_words(full_text + title)
            
            ts, fs = vector_engine.analyze_position(query + " " + title)
            t_impact = int(ts * w_vec) * -1; f_impact = int(fs * w_vec)

            news_ev = []; max_match = 0
            try:
                rss_url = f"https://news.google.com/rss/search?q={requests.utils.quote(query)}&hl=ko&gl=KR"
                r = requests.get(rss_url, timeout=5)
                root = ET.fromstring(r.content)
                items = root.findall('.//item')
                
                for item in items[:3]:
                    nt = item.find('title').text
                    d_tag = item.find('description')
                    nd = clean_html(d_tag.text) if d_tag is not None else ""
                    m = calculate_dual_match({'title': nt, 'desc': nd}, extract_nouns(query), full_text)
                    if m > max_match: max_match = m
                    news_ev.append({"ë‰´ìŠ¤ ì œëª©": nt, "ìµœì¢… ì¼ì¹˜ë„": f"{m}%"})
            except: pass
            
            cmts, c_status = fetch_comments_via_api(vid)
            top_kw, rel_score, rel_msg = analyze_comment_relevance(cmts, title + " " + full_text)
            red_cnt, red_list = check_red_flags(cmts)
            is_controversial = red_cnt > 0
            
            w_news = 65 if is_controversial else w_news
            
            silent_penalty = 0; news_score = 0; mismatch_penalty = 0
            is_silent = (len(news_ev) == 0) or (len(news_ev) > 0 and max_match < 20)
            
            if is_silent:
                if agitation >= 3: silent_penalty = PENALTY_SILENT_ECHO; t_impact *= 2; f_impact *= 2
                else: mismatch_penalty = 10
            elif is_controversial:
                news_score = PENALTY_NO_FACT if max_match < 60 else int((max_match/100)**2 * w_news) * -1
            else:
                news_score = int((max_match/100)**2 * w_news) * -1
                
            if is_official: news_score = -50; mismatch_penalty = 0; silent_penalty = 0
            
            sent_score = 0
            if cmts and not is_controversial:
                neg = sum(1 for c in cmts for k in ['ê°€ì§œ','ì„ ë™'] if k in c) / len(cmts)
                sent_score = int(neg * 10)
                
            clickbait = 10 if any(w in title for w in ['ì¶©ê²©','ê²½ì•…','í­ë¡œ']) else -5
            total = 50 + t_impact + f_impact + news_score + sent_score + clickbait + abuse_score + mismatch_penalty + silent_penalty
            prob = max(5, min(99, total))
            
            save_analysis(uploader, title, prob, url, query)

            # --- UI ---
            st.subheader("ğŸ•µï¸ í•µì‹¬ ë¶„ì„ ì§€í‘œ (Key Indicators)")
            col_a, col_b, col_c = st.columns(3)
            with col_a: st.metric("ìµœì¢… ê°€ì§œë‰´ìŠ¤ í™•ë¥ ", f"{prob}%", delta=f"{total - 50}")
            with col_b:
                icon = "ğŸŸ¢" if prob < 30 else "ğŸ”´" if prob > 60 else "ğŸŸ "
                verdict = "ë§¤ìš° ì•ˆì „" if prob < 30 else "ìœ„í—˜ ê°ì§€" if prob > 60 else "ì£¼ì˜ ìš”ë§"
                st.metric("ì¢…í•© AI íŒì •", f"{icon} {verdict}")
            with col_c: st.metric("AI Intelligence Level", f"{total_intelligence} Knowledge Nodes", delta="+1 Added")

            if is_ai_content: st.warning(f"ğŸ¤– **AI ìƒì„± ì½˜í…ì¸  ê°ì§€ë¨**: {ai_msg}")
            if is_official: st.success(f"ğŸ›¡ï¸ **ê³µì‹ ì–¸ë¡ ì‚¬ ì±„ë„({uploader})ì…ë‹ˆë‹¤.**")
            if silent_penalty > 0: st.error("ğŸ”‡ **ì¹¨ë¬µì˜ ë©”ì•„ë¦¬(Silent Echo) ê²½ê³ **: ê·¼ê±° ì—†ëŠ” ìê·¹ì  ì£¼ì¥")

            st.divider()
            col1, col2 = st.columns([1, 1.4])
            with col1:
                st.write("**[ì˜ìƒ ìƒì„¸ ì •ë³´]**")
                st.table(pd.DataFrame({"í•­ëª©": ["ì˜ìƒ ì œëª©", "ì±„ë„ëª…", "ì¡°íšŒìˆ˜", "í•´ì‹œíƒœê·¸"], "ë‚´ìš©": [title, uploader, f"{info.get('view_count',0):,}íšŒ", hashtag_display]}))
                st.info(f"ğŸ¯ **í•€í¬ì¸íŠ¸ ë‰´ìŠ¤ ê²€ìƒ‰ì–´**: {query}")
                with st.container(border=True):
                    st.markdown("ğŸ“ **ì˜ìƒ ë‚´ìš© ìš”ì•½ (AI Abstract)**")
                    st.caption("ìë§‰ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ ë¬¸ì¥ 3ê°œë¥¼ ì¶”ì¶œí•œ ê²°ê³¼ì…ë‹ˆë‹¤.")
                    st.write(summary)
                st.write("**[Score Breakdown]**")
                render_score_breakdown([
                    ["ê¸°ë³¸ ìœ„í—˜ë„", 50, "Base Score"],
                    ["ì§„ì‹¤ ë§¥ë½ ë³´ë„ˆìŠ¤ (ë²¡í„°)", t_impact, ""], ["ê°€ì§œ íŒ¨í„´ ê°€ì  (ë²¡í„°)", f_impact, ""],
                    ["ë‰´ìŠ¤ êµì°¨ ëŒ€ì¡° (Dual)", news_safety_score, ""],
                    ["ì¹¨ë¬µì˜ ë©”ì•„ë¦¬ (No News)", silent_penalty, ""],
                    ["ì—¬ë¡ /ì œëª©/ìë§‰ ê°€ê°", sent_score + clickbait, ""],
                    ["ë‚´ìš© ë¶ˆì¼ì¹˜ ê¸°ë§Œ", mismatch_penalty, ""], ["í•´ì‹œíƒœê·¸ ì–´ë·°ì§•", abuse_score, ""]
                ])

            with col2:
                st.subheader("ğŸ“Š 5ëŒ€ ì •ë°€ ë¶„ì„ ì¦ê±°")
                st.markdown("**[ì¦ê±° 0] Semantic Vector Space (ì§„ì‹¤/ê±°ì§“ ë¶„í¬)**")
                st.caption(f"ğŸ’¡ Intelligence Level {total_intelligence} ê¸°ë°˜ ë¶„ì„")
                colored_progress_bar("âœ… ì§„ì‹¤ ì˜ì—­ ê·¼ì ‘ë„", ts, "#2ecc71")
                colored_progress_bar("ğŸš¨ ê±°ì§“ ì˜ì—­ ê·¼ì ‘ë„", fs, "#e74c3c")
                st.write("---")
                st.markdown(f"**[ì¦ê±° 1] ë‰´ìŠ¤ êµì°¨ ëŒ€ì¡° (Query: {query})**")
                st.caption(f"ğŸ“¡ ìˆ˜ì§‘: **{len(news_ev)}ê±´**")
                if news_ev: st.table(pd.DataFrame(news_ev))
                else: st.warning("ğŸ” ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (Silent Echo Risk Increased)")
                st.markdown("**[ì¦ê±° 2] ì‹œì²­ì ì—¬ë¡  ì‹¬ì¸µ ë¶„ì„**")
                st.caption(f"ğŸ’¬ ìƒíƒœ: **{c_status}**")
                if cmts:
                    st.table(pd.DataFrame([["ìµœë‹¤ ë¹ˆì¶œ í‚¤ì›Œë“œ", ", ".join(top_kw)], ["ë…¼ë€ ê°ì§€ ì—¬ë¶€", f"{red_cnt}íšŒ"], ["ì£¼ì œ ì¼ì¹˜ë„", f"{rel_score}% ({rel_msg})"]], columns=["í•­ëª©", "ë‚´ìš©"]))
                else: st.warning("âš ï¸ ëŒ“ê¸€ ìˆ˜ì§‘ ë¶ˆê°€.")
                st.markdown("**[ì¦ê±° 3] ìë§‰ ì„¸ë§Œí‹± ì‹¬ì¸µ ëŒ€ì¡°**")
                st.caption(f"ğŸ“ **{t_status}** | ğŸ“š ì „ì²´ ë‹¨ì–´: **{len(full_text.split())}ê°œ**")
                st.table(pd.DataFrame([["ì œëª© ë‚šì‹œì–´", "ìˆìŒ" if clickbait > 0 else "ì—†ìŒ"], ["ì„ ë™ì„± ì§€ìˆ˜", f"{agitation}íšŒ"], ["ê¸°ì‚¬-ì˜ìƒ ì¼ì¹˜ë„", f"{max_match}%"]], columns=["ë¶„ì„ í•­ëª©", "íŒì • ê²°ê³¼"]))
                st.markdown("**[ì¦ê±° 4] AI ìµœì¢… ë¶„ì„ íŒë‹¨**")
                st.success(f"ğŸ” í˜„ì¬ ë¶„ì„ëœ ì¢…í•© ì ìˆ˜ëŠ” {prob}ì ì…ë‹ˆë‹¤.")
                if prob < 30 or prob > 70: st.toast(f"ğŸ¤– AIê°€ ì´ ê²°ê³¼ë¥¼ í•™ìŠµí–ˆìŠµë‹ˆë‹¤!", icon="ğŸ§ ")

        except Exception as e: st.error(f"ì˜¤ë¥˜: {e}")

st.title("âš–ï¸ Triple-Evidence Intelligence Forensic v47.1")
with st.container(border=True):
    st.markdown("### ğŸ›¡ï¸ ë²•ì  ê³ ì§€ ë° ì±…ì„ í•œê³„ (Disclaimer)\në³¸ ì„œë¹„ìŠ¤ëŠ” **ì¸ê³µì§€ëŠ¥(AI) ë° ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜**ìœ¼ë¡œ ì˜ìƒì˜ ì‹ ë¢°ë„ë¥¼ ë¶„ì„í•˜ëŠ” ë³´ì¡° ë„êµ¬ì…ë‹ˆë‹¤.\n* **ìµœì¢… íŒë‹¨ì˜ ì£¼ì²´:** ì •ë³´ì˜ ì§„ìœ„ ì—¬ë¶€ì— ëŒ€í•œ ìµœì¢…ì ì¸ íŒë‹¨ê³¼ ê·¸ì— ë”°ë¥¸ ì±…ì„ì€ **ì‚¬ìš©ì ë³¸ì¸**ì—ê²Œ ìˆìŠµë‹ˆë‹¤.")
    agree = st.checkbox("ìœ„ ë‚´ìš©ì„ í™•ì¸í•˜ì˜€ìœ¼ë©°, ì´ì— ë™ì˜í•©ë‹ˆë‹¤. (ë™ì˜ ì‹œ ë¶„ì„ ë²„íŠ¼ í™œì„±í™”)")

url = st.text_input("ğŸ”— ë¶„ì„í•  ìœ íŠœë¸Œ URL")
if st.button("ğŸš€ ì •ë°€ ë¶„ì„ ì‹œì‘", use_container_width=True, disabled=not agree):
    if url_input: run_forensic_main(url_input)
    else: st.warning("URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

st.divider()
st.subheader("ğŸ—‚ï¸ í•™ìŠµ ë°ì´í„° ê´€ë¦¬ (Cloud Knowledge Base)")
try:
    response = supabase.table("analysis_history").select("*").order("id", desc=True).execute()
    df = pd.DataFrame(response.data)
except: df = pd.DataFrame()

if not df.empty:
    df['Delete'] = False
    cols = ['Delete', 'id', 'analysis_date', 'video_title', 'fake_prob', 'keywords']
    df = df[cols]
    if st.session_state.get("is_admin", False):
        edited_df = st.data_editor(df, column_config={"Delete": st.column_config.CheckboxColumn("ì„ íƒ ì‚­ì œ", default=False)}, disabled=["id", "analysis_date", "video_title", "keywords"], hide_index=True, use_container_width=True)
        to_delete = edited_df[edited_df.Delete]
        if not to_delete.empty:
            if st.button(f"ğŸ—‘ï¸ ì„ íƒí•œ {len(to_delete)}ê±´ì˜ ê¸°ë¡ ì˜êµ¬ ì‚­ì œ", type="primary"):
                try:
                    for index, row in to_delete.iterrows(): supabase.table("analysis_history").delete().eq("id", row['id']).execute()
                    st.success("âœ… ì‚­ì œ ì™„ë£Œ!"); time.sleep(1); st.rerun()
                except Exception as e: st.error(f"ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    else:
        st.dataframe(df.drop(columns=['Delete']), hide_index=True, use_container_width=True)
        st.info("ğŸ”’ ë°ì´í„° ì‚­ì œ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤. (ê´€ë¦¬ì ë¡œê·¸ì¸ í•„ìš”)")
else: st.info("â˜ï¸ í´ë¼ìš°ë“œ DBì— ì €ì¥ëœ ë¶„ì„ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
