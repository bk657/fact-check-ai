import streamlit as st
from supabase import create_client
import google.generativeai as genai
import re
import requests
import time
import json
import yt_dlp
import pandas as pd
import altair as alt
from datetime import datetime
from collections import Counter

# --- [1. ì‹œìŠ¤í…œ ì„¤ì • ë° ì´ˆê¸°í™”] ---
st.set_page_config(page_title="Fact-Check Center v60.0 (Gemini Core)", layout="wide", page_icon="âš–ï¸")

# ğŸŒŸ Secrets ë¡œë“œ
try:
    YOUTUBE_API_KEY = st.secrets["YOUTUBE_API_KEY"]
    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    ADMIN_PASSWORD = st.secrets["ADMIN_PASSWORD"]
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"] # Gemini í‚¤ ì¶”ê°€ í•„ìˆ˜
except KeyError as e:
    st.error(f"âŒ í•„ìˆ˜ í‚¤ ì„¤ì • ëˆ„ë½: {e}")
    st.stop()

# ğŸŒŸ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
@st.cache_resource
def init_services():
    sb = create_client(SUPABASE_URL, SUPABASE_KEY)
    genai.configure(api_key=GOOGLE_API_KEY)
    model = genai.GenerativeModel('gemini-1.5-flash') # ì†ë„ì™€ ê°€ì„±ë¹„ ìµœì í™”
    return sb, model

supabase, gemini_model = init_services()

# --- [2. Gemini AI ì—ì´ì „íŠ¸ í´ë˜ìŠ¤] ---
class GeminiAgent:
    def __init__(self, model):
        self.model = model

    def analyze_content(self, title, channel, transcript, news_context, comments):
        """
        Geminiì—ê²Œ ì˜ìƒ ë‚´ìš©, ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼, ëŒ“ê¸€ ë°˜ì‘ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„ ìš”ì²­
        """
        prompt = f"""
        ë‹¹ì‹ ì€ ëƒ‰ì² í•˜ê³  ê°ê´€ì ì¸ 'ê°€ì§œë‰´ìŠ¤ íŒë³„ ì „ë¬¸ AI'ì…ë‹ˆë‹¤. ì•„ë˜ ì œê³µëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ìƒì„ ë¶„ì„í•˜ì—¬ JSON í¬ë§·ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.

        [ë¶„ì„ ëŒ€ìƒ]
        - ì˜ìƒ ì œëª©: {title}
        - ì±„ë„ëª…: {channel}
        - ìë§‰(ë‚´ìš©): {transcript[:15000]} (ë„ˆë¬´ ê¸¸ë©´ ì˜ë¦¼)
        - ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼: {news_context}
        - ì‹œì²­ì ëŒ“ê¸€ ë°˜ì‘: {comments}

        [ë¶„ì„ ì§€ì¹¨]
        1. **ìš”ì•½**: ì˜ìƒì˜ í•µì‹¬ ì£¼ì¥ 3ê°€ì§€ë¥¼ ìš”ì•½í•˜ì„¸ìš”.
        2. **íŒ©íŠ¸ì²´í¬**: ì˜ìƒì˜ ì£¼ì¥ì´ ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼(Facts)ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ êµì°¨ ê²€ì¦í•˜ì„¸ìš”. ë‰´ìŠ¤ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ì£¼ì¥ì„ ë’·ë°›ì¹¨í•˜ì§€ ëª»í•˜ë©´ ê°€ì§œ í™•ë¥ ì„ ë†’ì´ì„¸ìš”.
        3. **ì„ ë™ì„± íŒë‹¨**: ì œëª©ì´ë‚˜ ë‚´ìš©ì— ê³¼ë„í•œ ê°ì •ì  ì–¸ì–´(ì¶©ê²©, ê²½ì•… ë“±)ë‚˜ ê·¼ê±° ì—†ëŠ” ë£¨ë¨¸ê°€ ìˆëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.
        4. **ìµœì¢… íŒì •**: 0~100 ì‚¬ì´ì˜ 'ê°€ì§œë‰´ìŠ¤/ìœ„í—˜ í™•ë¥ (fake_prob)'ì„ ì‚°ì¶œí•˜ì„¸ìš”. (ë†’ì„ìˆ˜ë¡ ìœ„í—˜)

        [ì¶œë ¥ í˜•ì‹ (JSON)]
        {{
            "summary": "í•µì‹¬ ë‚´ìš© 3ì¤„ ìš”ì•½",
            "fake_prob": 75,
            "verdict": "ìœ„í—˜/ì£¼ì˜/ì•ˆì „ ì¤‘ íƒ1",
            "reasoning": "ì ìˆ˜ ì‚°ì •ì˜ êµ¬ì²´ì ì¸ ì´ìœ  (200ì ë‚´ì™¸)",
            "fact_check_status": "ë‰´ìŠ¤ êµì°¨ ê²€ì¦ ê²°ê³¼ (ì˜ˆ: ê·¼ê±° ì—†ìŒ, ë¶€ë¶„ ì¼ì¹˜, í™•ì¸ ë¶ˆê°€)",
            "clickbait_score": 0~100 (ë‚šì‹œì„± ì ìˆ˜)
        }}
        JSON í˜•ì‹ë§Œ ì •í™•íˆ ì¶œë ¥í•˜ì„¸ìš”. ë§ˆí¬ë‹¤ìš´ íƒœê·¸ ì—†ì´ raw í…ìŠ¤íŠ¸ë¡œ ì£¼ì„¸ìš”.
        """
        
        try:
            response = self.model.generate_content(prompt)
            return json.loads(response.text.replace("```json", "").replace("```", ""))
        except Exception as e:
            return {"error": str(e), "fake_prob": 50, "summary": "AI ë¶„ì„ ì‹¤íŒ¨", "reasoning": "API ì˜¤ë¥˜ ë°œìƒ"}

gemini_agent = GeminiAgent(gemini_model)

# --- [3. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜] ---
def fetch_youtube_info(url):
    """yt_dlpë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ìƒ ë©”íƒ€ë°ì´í„°ì™€ ìë§‰ ì¶”ì¶œ"""
    ydl_opts = {'quiet': True, 'skip_download': True}
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        try:
            info = ydl.extract_info(url, download=False)
            video_id = info['id']
            title = info.get('title', '')
            channel = info.get('uploader', '')
            
            # ìë§‰ ì¶”ì¶œ ë¡œì§
            transcript = ""
            if 'subtitles' in info and 'ko' in info['subtitles']:
                sub_url = next((x['url'] for x in info['subtitles']['ko'] if x['ext'] == 'vtt'), None)
                if sub_url:
                    res = requests.get(sub_url)
                    transcript = clean_vtt(res.text)
            
            # ìë™ ìë§‰ì´ë¼ë„ ê°€ì ¸ì˜¤ê¸°
            if not transcript and 'automatic_captions' in info and 'ko' in info['automatic_captions']:
                sub_url = next((x['url'] for x in info['automatic_captions']['ko'] if x['ext'] == 'vtt'), None)
                if sub_url:
                    res = requests.get(sub_url)
                    transcript = clean_vtt(res.text)
            
            if not transcript:
                transcript = info.get('description', '') # ìë§‰ ì—†ìœ¼ë©´ ì„¤ëª…ë€ ì‚¬ìš©

            return {"id": video_id, "title": title, "channel": channel, "transcript": transcript}
        except Exception as e:
            return None

def clean_vtt(text):
    """VTT ìë§‰ í¬ë§· ì •ë¦¬"""
    lines = text.splitlines()
    clean_lines = []
    for line in lines:
        if '-->' in line or line.strip() == '' or line.startswith('WEBVTT') or line.startswith('NOTE'):
            continue
        clean = re.sub(r'<[^>]+>', '', line).strip()
        if clean and clean not in clean_lines: # ì¤‘ë³µ ì œê±°
            clean_lines.append(clean)
    return " ".join(clean_lines)

def fetch_google_news(query):
    """êµ¬ê¸€ ë‰´ìŠ¤ RSS ê²€ìƒ‰"""
    try:
        rss_url = f"https://news.google.com/rss/search?q={requests.utils.quote(query)}&hl=ko&gl=KR&ceid=KR:ko"
        res = requests.get(rss_url, timeout=5)
        items = re.findall(r'<item>(.*?)</item>', res.text, re.DOTALL)
        news_data = []
        for item in items[:5]: # ìƒìœ„ 5ê°œë§Œ
            t = re.search(r'<title>(.*?)</title>', item)
            news_data.append(t.group(1).replace("<![CDATA[", "").replace("]]>", "") if t else "")
        return " | ".join(news_data) if news_data else "ê´€ë ¨ ë‰´ìŠ¤ ì—†ìŒ"
    except:
        return "ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨"

def fetch_comments(video_id):
    """ìœ íŠœë¸Œ API ëŒ“ê¸€ ìˆ˜ì§‘"""
    try:
        url = "https://www.googleapis.com/youtube/v3/commentThreads"
        params = {'part': 'snippet', 'videoId': video_id, 'key': YOUTUBE_API_KEY, 'maxResults': 20, 'order': 'relevance'}
        res = requests.get(url, params=params)
        if res.status_code == 200:
            comments = [item['snippet']['topLevelComment']['snippet']['textDisplay'] for item in res.json().get('items', [])]
            return " | ".join(comments)
    except: pass
    return "ëŒ“ê¸€ ìˆ˜ì§‘ ë¶ˆê°€"

def save_history(data):
    try:
        supabase.table("analysis_history").insert({
            "channel_name": data['channel'],
            "video_title": data['title'],
            "fake_prob": data['fake_prob'],
            "analysis_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "video_url": f"https://youtu.be/{data['id']}",
            "keywords": data['verdict']
        }).execute()
    except Exception as e:
        print(f"DB Save Error: {e}")

# --- [4. UI êµ¬ì„±] ---
# ì‚¬ì´ë“œë°”: ê´€ë¦¬ì
with st.sidebar:
    st.header("ğŸ›¡ï¸ ê´€ë¦¬ì ë©”ë‰´")
    if "is_admin" not in st.session_state: st.session_state["is_admin"] = False
    
    if not st.session_state["is_admin"]:
        pw = st.text_input("Admin Password", type="password")
        if st.button("Login"):
            if pw == ADMIN_PASSWORD:
                st.session_state["is_admin"] = True
                st.rerun()
            else:
                st.error("ë¹„ë°€ë²ˆí˜¸ ë¶ˆì¼ì¹˜")
    else:
        st.success("Admin Logged In")
        if st.button("Logout"):
            st.session_state["is_admin"] = False
            st.rerun()

# ë©”ì¸ UI
st.title("âš–ï¸ Gemini Fact-Check Center v60.0")
st.caption("Powered by Google Gemini 1.5 & Streamlit")

with st.container(border=True):
    st.info("ğŸ’¡ **Google Gemini AI**ê°€ ì˜ìƒ ìë§‰ê³¼ ì‹¤ì‹œê°„ ë‰´ìŠ¤ë¥¼ êµì°¨ ê²€ì¦í•˜ì—¬ ì§„ìœ„ ì—¬ë¶€ë¥¼ íŒë…í•©ë‹ˆë‹¤.")
    url_input = st.text_input("ìœ íŠœë¸Œ ì˜ìƒ URLì„ ì…ë ¥í•˜ì„¸ìš”")
    start_btn = st.button("ğŸš€ AI ì •ë°€ ë¶„ì„ ì‹œì‘", use_container_width=True, type="primary")

if start_btn and url_input:
    with st.status("ğŸ•µï¸ Gemini AIê°€ ì˜ìƒì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...", expanded=True) as status:
        # 1. ì˜ìƒ ë°ì´í„° ìˆ˜ì§‘
        st.write("ğŸ“¥ ì˜ìƒ ë©”íƒ€ë°ì´í„° ë° ìë§‰ ì¶”ì¶œ ì¤‘...")
        video_info = fetch_youtube_info(url_input)
        
        if not video_info:
            status.update(label="ì˜ìƒ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", state="error")
            st.stop()
            
        # 2. ë‰´ìŠ¤ ë° ëŒ“ê¸€ ë°ì´í„° ìˆ˜ì§‘
        st.write("ğŸ“° ê´€ë ¨ ë‰´ìŠ¤ ë° ì—¬ë¡  ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        # ê²€ìƒ‰ì–´ ìµœì í™”: ì œëª©ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ ì‚¬ìš©
        clean_title = re.sub(r'[^\w\s]', '', video_info['title'])
        news_context = fetch_google_news(clean_title)
        comments = fetch_comments(video_info['id'])
        
        # 3. Gemini ë¶„ì„ ìˆ˜í–‰
        st.write("ğŸ§  Gemini 1.5 ëª¨ë¸ ì¶”ë¡  ë° íŒ©íŠ¸ì²´í¬ ìˆ˜í–‰ ì¤‘...")
        ai_result = gemini_agent.analyze_content(
            video_info['title'],
            video_info['channel'],
            video_info['transcript'],
            news_context,
            comments
        )
        
        # 4. ì €ì¥
        save_data = {**video_info, **ai_result}
        save_history(save_data)
        
        status.update(label="âœ… ë¶„ì„ ì™„ë£Œ!", state="complete", expanded=False)

    # --- [ê²°ê³¼ ë¦¬í¬íŠ¸] ---
    st.divider()
    
    # ìƒë‹¨ ë©”íŠ¸ë¦­
    col1, col2, col3 = st.columns(3)
    prob = ai_result.get('fake_prob', 0)
    
    col1.metric("ê°€ì§œë‰´ìŠ¤ ìœ„í—˜ë„", f"{prob}%", delta="High Risk" if prob > 60 else "-Safe")
    col2.metric("AI íŒì •", ai_result.get('verdict', 'íŒë‹¨ ë¶ˆê°€'))
    col3.metric("ë‚šì‹œì„± ì§€ìˆ˜", f"{ai_result.get('clickbait_score', 0)}ì ")
    
    # ê²Œì´ì§€ ì°¨íŠ¸ (Altair)
    chart_df = pd.DataFrame({'value': [prob]})
    base = alt.Chart(chart_df).mark_bar().encode(x=alt.X('value', scale=alt.Scale(domain=[0, 100])))
    st.progress(prob / 100)
    
    if prob > 70:
        st.error(f"ğŸš¨ **ìœ„í—˜ ê°ì§€**: {ai_result.get('reasoning')}")
    elif prob < 30:
        st.success(f"âœ… **ì•ˆì „**: {ai_result.get('reasoning')}")
    else:
        st.warning(f"âš ï¸ **ì£¼ì˜**: {ai_result.get('reasoning')}")

    # ìƒì„¸ ë‚´ìš©
    col_l, col_r = st.columns([1, 1])
    
    with col_l:
        st.subheader("ğŸ“ AI ìš”ì•½ & ë¶„ì„")
        st.info(f"**ìš”ì•½**: {ai_result.get('summary')}")
        st.write(f"**íŒ©íŠ¸ì²´í¬ ìƒíƒœ**: {ai_result.get('fact_check_status')}")
        
        with st.expander("ì°¸ì¡°ëœ ë‰´ìŠ¤ ë°ì´í„° ë³´ê¸°"):
            st.write(news_context)

    with col_r:
        st.subheader("ğŸ“º ì˜ìƒ ì •ë³´")
        st.table(pd.DataFrame({
            "í•­ëª©": ["ì œëª©", "ì±„ë„", "ìë§‰ ê¸¸ì´"],
            "ë‚´ìš©": [video_info['title'], video_info['channel'], f"{len(video_info['transcript']):,}ì"]
        }))

# --- [5. íˆìŠ¤í† ë¦¬ (ê´€ë¦¬ì ì „ìš© ê¸°ëŠ¥ ì‚­ì œ ê°€ëŠ¥)] ---
st.divider()
st.subheader("ğŸ—‚ï¸ ìµœê·¼ ë¶„ì„ ê¸°ë¡")
try:
    rows = supabase.table("analysis_history").select("*").order("id", desc=True).limit(5).execute()
    if rows.data:
        df = pd.DataFrame(rows.data)
        st.dataframe(df[['video_title', 'fake_prob', 'analysis_date', 'keywords']], hide_index=True, use_container_width=True)
except:
    st.caption("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ëŒ€ê¸° ì¤‘...")
