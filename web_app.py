import streamlit as st
from supabase import create_client, Client
import re
import requests
import time
import random
import math
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from datetime import datetime
from collections import Counter
import yt_dlp
import pandas as pd
import altair as alt
import json

# --- [1. ì‹œìŠ¤í…œ ì„¤ì •] ---
st.set_page_config(page_title="Fact-Check Center v73.0 (Smart Summary)", layout="wide", page_icon="âš–ï¸")

if "is_admin" not in st.session_state:
    st.session_state["is_admin"] = False

# ğŸŒŸ Secrets ë¡œë“œ
try:
    YOUTUBE_API_KEY = st.secrets["YOUTUBE_API_KEY"]
    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    ADMIN_PASSWORD = st.secrets["ADMIN_PASSWORD"]
    GOOGLE_API_KEY_A = st.secrets["GOOGLE_API_KEY_A"]
    GOOGLE_API_KEY_B = st.secrets["GOOGLE_API_KEY_B"]
except:
    st.error("âŒ í•„ìˆ˜ í‚¤(API Keys)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

@st.cache_resource
def init_supabase():
    return create_client(SUPABASE_URL, SUPABASE_KEY)

supabase = init_supabase()

# --- [2. ìƒìˆ˜ ì •ì˜] ---
WEIGHT_ALGO = 0.6
WEIGHT_AI = 0.4

VITAL_KEYWORDS = ['ìœ„ë…', 'ì‚¬ë§', 'ë³„ì„¸', 'êµ¬ì†', 'ì²´í¬', 'ê¸°ì†Œ', 'ì‹¤í˜•', 'ì‘ê¸‰ì‹¤', 'ì´í˜¼', 'ë¶ˆí™”', 'íŒŒê²½', 'ì¶©ê²©', 'ê²½ì•…', 'ì†ë³´', 'ê¸´ê¸‰', 'í­ë¡œ', 'ì–‘ì„±', 'í™•ì§„', 'ì‹¬ì •ì§€', 'ë‡Œì‚¬', 'ì¤‘íƒœ', 'ì••ìˆ˜ìˆ˜ìƒ‰', 'ì†Œí™˜', 'í‡´ì§„', 'íƒ„í•µ', 'ë‚´ë€', 'ê°„ì²©']
CRITICAL_STATE_KEYWORDS = ['ë³„ê±°', 'ì´í˜¼', 'íŒŒê²½', 'ì‚¬ë§', 'ìœ„ë…', 'êµ¬ì†', 'ì²´í¬', 'ì‹¤í˜•', 'ë¶ˆí™”', 'í­ë¡œ', 'ì¶©ê²©', 'ë…¼ë€', 'ì¤‘íƒœ', 'ì‹¬ì •ì§€', 'ë‡Œì‚¬', 'ì••ìˆ˜ìˆ˜ìƒ‰', 'ì†Œí™˜', 'íŒŒì‚°', 'ë¹šë”ë¯¸', 'ì „ê³¼', 'ê°ì˜¥', 'ê°„ì²©']
OFFICIAL_CHANNELS = ['MBC', 'KBS', 'SBS', 'EBS', 'YTN', 'JTBC', 'TVCHOSUN', 'MBN', 'CHANNEL A', 'OBS', 'ì±„ë„A', 'TVì¡°ì„ ', 'ì—°í•©ë‰´ìŠ¤', 'YONHAP', 'í•œê²¨ë ˆ', 'ê²½í–¥', 'ì¡°ì„ ', 'ì¤‘ì•™', 'ë™ì•„']

STATIC_TRUTH_CORPUS = ["ë°•ë‚˜ë˜ ìœ„ì¥ì „ì… ë¬´í˜ì˜", "ì„ì˜ì›… ì•”í‘œ ëŒ€ì‘", "ì •í¬ì› ì €ì†ë…¸í™”", "ëŒ€ì „ ì¶©ë‚¨ í†µí•©", "ì„ ê±° ì¶œë§ˆ ì„ ì–¸"]
STATIC_FAKE_CORPUS = ["ì¶©ê²© í­ë¡œ ê²½ì•…", "ê¸´ê¸‰ ì†ë³´ ì†Œë¦„", "ì¶©ê²© ë°œì–¸ ë…¼ë€", "êµ¬ì† ì˜ì¥ ë°œë¶€", "ì˜ìƒ ìœ ì¶œ", "ê³„ì‹œ ì˜ˆì–¸", "ì‚¬í˜• ì§‘í–‰", "ìœ„ë…ì„¤"]

# --- [3. VectorEngine] ---
class VectorEngine:
    def __init__(self):
        self.vocab = set()
        self.truth_vectors = []
        self.fake_vectors = []
    def tokenize(self, text): return re.findall(r'[ê°€-í£]{2,}', text)
    def train(self, truth, fake):
        for t in truth + fake: self.vocab.update(self.tokenize(t))
        self.vocab = sorted(list(self.vocab))
        self.truth_vectors = [self.text_to_vector(t) for t in truth]
        self.fake_vectors = [self.text_to_vector(t) for t in fake]
    def text_to_vector(self, text, vocabulary=None):
        target_vocab = vocabulary if vocabulary else self.vocab
        c = Counter(self.tokenize(text))
        return [c[w] for w in target_vocab]
    def cosine_similarity(self, v1, v2):
        dot = sum(a*b for a,b in zip(v1,v2))
        mag = math.sqrt(sum(a*a for a in v1)) * math.sqrt(sum(b*b for b in v2))
        return dot/mag if mag>0 else 0
    def analyze_position(self, query):
        qv = self.text_to_vector(query)
        mt = max([self.cosine_similarity(qv, v) for v in self.truth_vectors] or [0])
        mf = max([self.cosine_similarity(qv, v) for v in self.fake_vectors] or [0])
        return mt, mf
    def compute_content_similarity(self, text1, text2):
        tokens1 = self.tokenize(text1); tokens2 = self.tokenize(text2)
        local_vocab = sorted(list(set(tokens1 + tokens2)))
        if not local_vocab: return 0.0
        v1 = self.text_to_vector(text1, local_vocab)
        v2 = self.text_to_vector(text2, local_vocab)
        return self.cosine_similarity(v1, v2)

vector_engine = VectorEngine()

# --- [4. Gemini Logic] ---

# ğŸš¨ ì•ˆì „ ì„¤ì •: í•„í„°ë§ ì™„ì „ í•´ì œ
safety_settings_none = {
    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,
    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
}

# [Engine A] ìˆ˜ì‚¬ê´€: (ì œëª© + ìš”ì•½ë³¸)ë§Œ ì‚¬ìš© -> ì´ˆê²½ëŸ‰í™”/ê³ íš¨ìœ¨
def get_gemini_search_keywords(title, summary):
    genai.configure(api_key=GOOGLE_API_KEY_A)
    model = genai.GenerativeModel('gemini-2.0-flash') 
    
    # [í•µì‹¬ ë³€ê²½] ì „ì²´ ìë§‰(full_context) ëŒ€ì‹  ìš”ì•½ë³¸(summary)ì„ ì‚¬ìš©
    prompt = f"""
    You are a Fact-Check Investigator.
    
    [Input Data]
    - Video Title: {title}
    - Content Summary: {summary}
    
    [Task]
    Based on the summary, extract the single most important 'keyword' or 'short phrase' to verify the claims on Google News.
    
    [Rules] 
    1. Ignore clickbait words like 'Shocking', 'Vlog', 'Diet'. 
    2. Focus on specific Drug names, Medical terms, or Crimes mentioned in the summary.
    3. Output ONLY the Korean query string. No explanations.
    """

    try:
        response = model.generate_content(prompt, safety_settings=safety_settings_none)
        return response.text.strip(), "âœ¨ Gemini 2.0 (Summary-Based)"
    except Exception as e:
        time.sleep(2)
        try:
            response = model.generate_content(prompt, safety_settings=safety_settings_none)
            return response.text.strip(), "âœ¨ Gemini 2.0 (Retry)"
        except Exception as e2:
            return f"Error: {str(e2)}", "âŒ Key A Error"

# [Engine B] íŒì‚¬: (ì œëª© + ì „ì²´ ìë§‰) ì‚¬ìš© -> ì •ë°€ íŒë… (ê¸°ì¡´ ìœ ì§€)
def get_gemini_verdict(title, transcript, news_items):
    genai.configure(api_key=GOOGLE_API_KEY_B)
    model = genai.GenerativeModel('gemini-2.0-flash', generation_config={"response_mime_type": "application/json"})
    
    news_text = ""
    if not news_items:
        news_text = "No related news articles found."
    else:
        for idx, item in enumerate(news_items[:5]):
            safe_title = item.get('title', 'ì œëª© ì—†ìŒ')
            safe_desc = item.get('desc', 'ë‚´ìš© ì—†ìŒ')
            news_text += f"{idx+1}. {safe_title} : {safe_desc}\n"
            
    # Key BëŠ” ì •í™•ë„ë¥¼ ìœ„í•´ ì „ì²´ ìë§‰(ìµœëŒ€ 30,000ì) ì‚¬ìš©
    full_context = transcript[:30000]

    prompt = f"""
    You are a professional Fact-Check AI Judge.
    
    [Task]
    Compare the Video Transcript with the Search Results.
    
    [Video Info]
    Title: {title}
    Transcript Summary: {full_context[:2000]}...
    
    [Search Results]
    {news_text}
    
    [Instruction]
    1. Identify the core claim.
    2. If the video warns about 'Drug Side Effects' and news confirms it -> TRUTH (Score 0-30).
    3. If the video makes 'Unfounded Conspiracy Claims' -> FAKE (Score 80-100).
    4. Provide a 'fake_score' (0=Truth, 100=Fake) and a short 'reason' in Korean.

    [Output Format - JSON Only]
    {{"score": <int>, "reason": "<string>"}}
    """
    
    try:
        response = model.generate_content(prompt, safety_settings=safety_settings_none)
        res_json = json.loads(response.text)
        return res_json['score'], res_json['reason']
    except Exception as e:
        return 50, f"AI ì¶”ë¡  ì‹¤íŒ¨ (Key B Error: {str(e)})"

# --- [5. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜] ---
def normalize_korean_word(word):
    word = re.sub(r'[^ê°€-í£0-9]', '', word)
    for j in ['ì€','ëŠ”','ì´','ê°€','ì„','ë¥¼','ì˜','ì—','ì—ê²Œ','ë¡œ','ìœ¼ë¡œ']:
        if word.endswith(j): return word[:-len(j)]
    return word

def extract_meaningful_tokens(text):
    raw = re.findall(r'[ê°€-í£]{2,}', text)
    noise = ['ì¶©ê²©','ì†ë³´','ê¸´ê¸‰','ì˜¤ëŠ˜','ì§€ê¸ˆ','ê²°êµ­','ë‰´ìŠ¤','ì˜ìƒ']
    return [normalize_korean_word(w) for w in raw if w not in noise]

def train_dynamic_vector_engine():
    try:
        res_t = supabase.table("analysis_history").select("video_title").lt("fake_prob", 40).execute()
        res_f = supabase.table("analysis_history").select("video_title").gt("fake_prob", 60).execute()
        
        dt = [row['video_title'] for row in res_t.data] if res_t.data else []
        df = [row['video_title'] for row in res_f.data] if res_f.data else []
        
        vector_engine.train(STATIC_TRUTH_CORPUS + dt, STATIC_FAKE_CORPUS + df)
        return len(STATIC_TRUTH_CORPUS + dt) + len(STATIC_FAKE_CORPUS + df), len(dt), len(df)
    except: 
        vector_engine.train(STATIC_TRUTH_CORPUS, STATIC_FAKE_CORPUS)
        return 0, 0, 0

def save_analysis(channel, title, prob, url, keywords):
    try: supabase.table("analysis_history").insert({"channel_name": channel, "video_title": title, "fake_prob": prob, "analysis_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'), "video_url": url, "keywords": keywords}).execute()
    except: pass

def render_intelligence_distribution(current_prob):
    try:
        res = supabase.table("analysis_history").select("fake_prob").execute()
        if not res.data: return
        df = pd.DataFrame(res.data)
        base = alt.Chart(df).transform_density('fake_prob', as_=['fake_prob', 'density'], extent=[0, 100], bandwidth=5).mark_area(opacity=0.3, color='#888').encode(x=alt.X('fake_prob:Q', title='ê°€ì§œë‰´ìŠ¤ í™•ë¥  ë¶„í¬'), y=alt.Y('density:Q', title='ë°ì´í„° ë°€ë„'))
        rule = alt.Chart(pd.DataFrame({'x': [current_prob]})).mark_rule(color='blue', size=3).encode(x='x')
        st.altair_chart(base + rule, use_container_width=True)
    except: pass

def colored_progress_bar(label, percent, color):
    st.markdown(f"""<div style="margin-bottom: 10px;"><div style="display: flex; justify-content: space-between; margin-bottom: 3px;"><span style="font-size: 13px; font-weight: 600; color: #555;">{label}</span><span style="font-size: 13px; font-weight: 700; color: {color};">{round(percent * 100, 1)}%</span></div><div style="background-color: #eee; border-radius: 5px; height: 8px; width: 100%;"><div style="background-color: {color}; height: 8px; width: {percent * 100}%; border-radius: 5px;"></div></div></div>""", unsafe_allow_html=True)

def render_score_breakdown(data_list):
    style = """<style>table.score-table { width: 100%; border-collapse: separate; border-spacing: 0; border: 1px solid #e0e0e0; border-radius: 8px; overflow: hidden; font-family: sans-serif; font-size: 14px; margin-top: 10px;} table.score-table th { background-color: #f8f9fa; color: #495057; font-weight: bold; padding: 12px 15px; text-align: left; border-bottom: 1px solid #e0e0e0; } table.score-table td { padding: 12px 15px; border-bottom: 1px solid #f0f0f0; color: #333; } table.score-table tr:last-child td { border-bottom: none; } .badge { padding: 4px 8px; border-radius: 6px; font-weight: 700; font-size: 11px; display: inline-block; text-align: center; min-width: 45px; } .badge-danger { background-color: #ffebee; color: #d32f2f; } .badge-success { background-color: #e8f5e9; color: #2e7d32; } .badge-neutral { background-color: #f5f5f5; color: #757575; border: 1px solid #e0e0e0; }</style>"""
    rows = ""
    for item, score, note in data_list:
        try:
            score_num = int(score)
            badge = f'<span class="badge badge-danger">+{score_num}</span>' if score_num > 0 else f'<span class="badge badge-success">{score_num}</span>' if score_num < 0 else f'<span class="badge badge-neutral">0</span>'
        except: badge = f'<span class="badge badge-neutral">{score}</span>'
        rows += f"<tr><td>{item}<br><span style='color:#888; font-size:11px;'>{note}</span></td><td style='text-align: right;'>{badge}</td></tr>"
    st.markdown(f"{style}<table class='score-table'><thead><tr><th>ë¶„ì„ í•­ëª© (Score Breakdown)</th><th style='text-align: right;'>ë³€ë™</th></tr></thead><tbody>{rows}</tbody></table>", unsafe_allow_html=True)

def summarize_transcript(text, title, max_sentences=3):
    if not text or len(text) < 50: return "âš ï¸ ìš”ì•½í•  ìë§‰ ë‚´ìš©ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    clean_text = re.sub(r'http\S+|#EXTM3U|#EXT-X-VERSION:3', '', text)
    clean_text = re.sub(r'\[.*?\]|[>]+', '', clean_text)
    sentences = re.split(r'(?<=[.?!])\s+', clean_text)
    if len(sentences) <= 3: return clean_text.strip()
    title_nouns = set(extract_meaningful_tokens(title))
    scored_sentences = []
    for i, sent in enumerate(sentences):
        if len(sent) < 15: continue
        score = 0
        sent_tokens = extract_meaningful_tokens(sent)
        score += len(sent_tokens)
        for n in sent_tokens:
            if n in title_nouns: score += 10
        if i < len(sentences) * 0.2: score += 3
        elif i > len(sentences) * 0.8: score += 2
        scored_sentences.append((i, sent, score))
    top_sentences = sorted(scored_sentences, key=lambda x:x[2], reverse=True)[:max_sentences]
    top_sentences.sort(key=lambda x:x[0])
    return " ".join([s[1] for s in top_sentences])

def clean_html_regex(text):
    if not text: return ""
    return re.sub('<.*?>', '', text).strip()

def detect_ai_content(info):
    is_ai, reasons = False, []
    text = (info.get('title', '') + " " + info.get('description', '') + " " + " ".join(info.get('tags', []))).lower()
    for kw in ['ai', 'artificial intelligence', 'chatgpt', 'deepfake', 'synthetic', 'ì¸ê³µì§€ëŠ¥', 'ë”¥í˜ì´í¬', 'ê°€ìƒì¸ê°„']:
        if kw in text: is_ai = True; reasons.append(f"í‚¤ì›Œë“œ ê°ì§€: {kw}"); break
    return is_ai, ", ".join(reasons)

def check_is_official(channel_name):
    norm_name = channel_name.upper().replace(" ", "")
    return any(o in norm_name for o in OFFICIAL_CHANNELS)

def count_sensational_words(text):
    return sum(text.count(w) for w in ['ì¶©ê²©', 'ê²½ì•…', 'ì‹¤ì²´', 'í­ë¡œ', 'ë‚œë¦¬', 'ì†ë³´', 'ê¸´ê¸‰', 'ì†Œë¦„', 'ã„·ã„·', 'ì§„ì§œ', 'ê²°êµ­', 'ê³„ì‹œ', 'ì˜ˆì–¸', 'ìœ„ë…', 'ì‚¬ë§', 'ì¤‘íƒœ'])

def check_tag_abuse(title, hashtags, channel_name):
    if check_is_official(channel_name): return 0, "ê³µì‹ ì±„ë„ ë©´ì œ"
    if not hashtags: return 0, "í•´ì‹œíƒœê·¸ ì—†ìŒ"
    tn = set(extract_meaningful_tokens(title)); tgn = set(h.replace("#", "").split(":")[-1].strip() for h in hashtags)
    if len(tgn) < 2: return 0, "ì–‘í˜¸"
    return (20, "ğŸš¨ ì‹¬ê° (ë¶ˆì¼ì¹˜)") if not tn.intersection(tgn) else (0, "ì–‘í˜¸")

def fetch_real_transcript(info_dict):
    try:
        url = None
        for key in ['subtitles', 'automatic_captions']:
            if key in info_dict and 'ko' in info_dict[key]:
                for fmt in info_dict[key]['ko']:
                    if fmt['ext'] == 'vtt': url = fmt['url']; break
            if url: break
        if url:
            res = requests.get(url)
            if res.status_code == 200:
                content = res.text
                if "#EXTM3U" in content: return None, "ìë§‰ í¬ë§· ì˜¤ë¥˜"
                clean = []
                for line in content.splitlines():
                    if '-->' not in line and 'WEBVTT' not in line and line.strip():
                        t = re.sub(r'<[^>]+>', '', line).strip()
                        if t and t not in clean: clean.append(t)
                full_text = " ".join(clean)
                return full_text, f"âœ… ì „ì²´ ìë§‰ ìˆ˜ì§‘ ì™„ë£Œ (ì´ {len(full_text):,}ì)"
    except: pass
    return None, "ìë§‰ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"

def fetch_comments_via_api(video_id):
    try:
        url = "https://www.googleapis.com/youtube/v3/commentThreads"
        res = requests.get(url, params={'part': 'snippet', 'videoId': video_id, 'key': YOUTUBE_API_KEY, 'maxResults': 50, 'order': 'relevance'})
        if res.status_code == 200:
            items = [i['snippet']['topLevelComment']['snippet']['textDisplay'] for i in res.json().get('items', [])]
            return items, f"âœ… API ìˆ˜ì§‘ ì„±ê³µ (Top {len(items)})"
    except: pass
    return [], "âŒ API í†µì‹  ì‹¤íŒ¨"

def fetch_news_regex(query):
    news_res = []
    try:
        rss = f"https://news.google.com/rss/search?q={requests.utils.quote(query)}&hl=ko&gl=KR"
        raw = requests.get(rss, timeout=5).text
        items = re.findall(r'<item>(.*?)</item>', raw, re.DOTALL)
        for item in items[:10]:
            t = re.search(r'<title>(.*?)</title>', item)
            d = re.search(r'<description>(.*?)</description>', item)
            nt = t.group(1).replace("<![CDATA[", "").replace("]]>", "") if t else "ì œëª© ì—†ìŒ"
            nd = clean_html_regex(d.group(1).replace("<![CDATA[", "").replace("]]>", "")) if d else "ë‚´ìš© ì—†ìŒ"
            news_res.append({'title': nt, 'desc': nd})
    except: pass
    return news_res

def extract_top_keywords_from_transcript(text, top_n=5):
    if not text: return []
    tokens = extract_meaningful_tokens(text)
    return Counter(tokens).most_common(top_n)

def calculate_dual_match(news_item, query_nouns, video_summary):
    news_title_tokens = set(extract_meaningful_tokens(news_item.get('title', '')))
    qn = set(query_nouns)
    title_match_score = 0
    if len(qn & news_title_tokens) >= 2: title_match_score = 100
    elif len(qn & news_title_tokens) >= 1: title_match_score = 50
    
    news_desc = news_item.get('desc', '')
    content_sim_score = 0
    if news_desc and video_summary:
        sim = vector_engine.compute_content_similarity(video_summary, news_desc)
        content_sim_score = int(sim * 100)
    
    final_score = int((title_match_score * 0.4) + (content_sim_score * 0.6))
    for critical in CRITICAL_STATE_KEYWORDS:
        if critical in query_nouns and critical not in news_title_tokens:
            final_score = 0
            
    return title_match_score, content_sim_score, final_score

def analyze_comment_relevance(comments, context_text):
    if not comments: return [], 0, "ë¶„ì„ ë¶ˆê°€"
    cn = extract_meaningful_tokens(" ".join(comments))
    top = Counter(cn).most_common(5)
    ctx = set(extract_meaningful_tokens(context_text))
    match = sum(1 for w,c in top if w in ctx)
    score = int(match/len(top)*100) if top else 0
    msg = "âœ… ì£¼ì œ ì§‘ì¤‘" if score >= 60 else "âš ï¸ ì¼ë¶€ ê´€ë ¨" if score >= 20 else "âŒ ë¬´ê´€"
    return [f"{w}({c})" for w, c in top], score, msg

def check_red_flags(comments):
    detected = [k for c in comments for k in ['ê°€ì§œë‰´ìŠ¤', 'ì£¼ì‘', 'ì‚¬ê¸°', 'ê±°ì§“ë§', 'í—ˆìœ„', 'ì„ ë™'] if k in c]
    return len(detected), list(set(detected))

def witty_loading_sequence(total, t_cnt, f_cnt):
    messages = [f"ğŸ§  [Intelligence: {total}] ì§‘ë‹¨ ì§€ì„± ë¡œë“œ ì¤‘...", f"ğŸ”‘ Twin-Gemini Protocol í™œì„±í™”...", "ğŸš€ ìˆ˜ì‚¬ê´€(Investigator) ë° íŒì‚¬(Judge) ì—”ì§„ ê°€ë™"]
    with st.status("ğŸ•µï¸ Dual-Engine Fact-Check v73.0...", expanded=True) as status:
        for msg in messages: st.write(msg); time.sleep(0.3)
        status.update(label="ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ", state="complete", expanded=False)

def run_forensic_main(url):
    total_intelligence, t_cnt, f_cnt = train_dynamic_vector_engine()
    witty_loading_sequence(total_intelligence, t_cnt, f_cnt)
    
    vid = re.search(r'(?:v=|\/)([0-9A-Za-z_-]{11}).*', url)
    if vid: vid = vid.group(1)

    with yt_dlp.YoutubeDL({'quiet': True, 'skip_download': True}) as ydl:
        try:
            info = ydl.extract_info(url, download=False)
            title = info.get('title', ''); uploader = info.get('uploader', '')
            tags = info.get('tags', []); desc = info.get('description', '')
            
            # [Step 1] ìë§‰ ìˆ˜ì§‘
            trans, t_status = fetch_real_transcript(info)
            full_text = trans if trans else desc
            
            # [Step 1.5] ìš”ì•½ ìƒì„± (ì´ê²Œ ë¨¼ì € ë˜ì–´ì•¼ í•¨)
            summary = summarize_transcript(full_text, title)
            top_transcript_keywords = extract_top_keywords_from_transcript(full_text)
            
            # [Step 2] Gemini Key A (ìˆ˜ì‚¬ê´€) - ìš”ì•½ë³¸ ì „ë‹¬ (í† í° ì ˆì•½)
            query, source = get_gemini_search_keywords(title, summary)

            # [Step 3] ê¸°ë³¸ ì•Œê³ ë¦¬ì¦˜ ë¶„ì„
            is_official = check_is_official(uploader)
            is_ai, ai_msg = detect_ai_content(info)
            hashtag_display = ", ".join([f"#{t}" for t in tags]) if tags else "í•´ì‹œíƒœê·¸ ì—†ìŒ"
            abuse_score, abuse_msg = check_tag_abuse(title, tags, uploader)
            agitation = count_sensational_words(full_text + title)
            ts, fs = vector_engine.analyze_position(query + " " + title)
            
            w_news = 70 if is_ai else 45
            w_vec = 10 if is_ai else 35
            t_impact = int(ts * w_vec) * -1; f_impact = int(fs * w_vec)

            # [Step 4] ë‰´ìŠ¤ ê²€ìƒ‰
            news_items = fetch_news_regex(query)
            news_ev = []; max_match = 0
            mismatch_count = 0
            
            for item in news_items:
                safe_title = item.get('title', '')
                t_score, c_score, final = calculate_dual_match(item, extract_meaningful_tokens(query), summary)
                if final > max_match: max_match = final
                if final < 20: mismatch_count += 1
                news_ev.append({
                    "ë‰´ìŠ¤ ì œëª©": safe_title,
                    "ì œëª© ì¼ì¹˜": f"{t_score}%",
                    "ë‚´ìš© ìœ ì‚¬": f"{c_score}%",
                    "ìµœì¢… ì ìˆ˜": f"{final}%"
                })
            
            # [Step 5] ì•Œê³ ë¦¬ì¦˜ ì ìˆ˜
            if not news_ev:
                news_score = 0
            else:
                if max_match >= 60: news_score = int((max_match / 100) * w_news) * -1
                else:
                    if mismatch_count >= len(news_ev) * 0.5: news_score = 20
                    else: news_score = 0

            cmts, c_status = fetch_comments_via_api(vid)
            top_kw, rel_score, rel_msg = analyze_comment_relevance(cmts, title + " " + full_text)
            red_cnt, red_list = check_red_flags(cmts)
            
            silent_penalty = 0; mismatch_penalty = 0
            is_silent = (len(news_ev) == 0)
            
            if is_silent:
                if any(k in title for k in CRITICAL_STATE_KEYWORDS): silent_penalty = 5; t_impact = 0; f_impact = 0
                elif agitation >= 3: silent_penalty = 40; t_impact *= 2; f_impact *= 2
                else: mismatch_penalty = 10
            
            if not is_silent and mismatch_count > 0 and max_match < 30: mismatch_penalty = 30
            if is_official: news_score = -50; mismatch_penalty = 0; silent_penalty = 0
            
            sent_score = 0
            if cmts and red_cnt == 0:
                neg = sum(1 for c in cmts for k in ['ê°€ì§œ','ì„ ë™'] if k in c) / len(cmts)
                sent_score = int(neg * 10)
            
            clickbait = 10 if any(w in title for w in ['ì¶©ê²©','ê²½ì•…','í­ë¡œ']) else -5
            
            algo_base_score = 50 + t_impact + f_impact + news_score + sent_score + clickbait + abuse_score + mismatch_penalty + silent_penalty
            algo_final_prob = max(5, min(99, algo_base_score))
            
            # [Step 6] Gemini Key B (íŒì‚¬) - ì „ì²´ ìë§‰ ì „ë‹¬ (ì •ë°€ ë¶„ì„)
            ai_judge_score, ai_judge_reason = get_gemini_verdict(title, full_text, news_ev)
            
            # [Step 7] ìµœì¢… í•©ì‚°
            final_prob = int((algo_final_prob * WEIGHT_ALGO) + (ai_judge_score * WEIGHT_AI))
            final_prob = max(1, min(99, final_prob))
            
            save_analysis(uploader, title, final_prob, url, query)

            st.subheader("ğŸ•µï¸ Dual-Engine Analysis Result")
            col_a, col_b, col_c = st.columns(3)
            with col_a: 
                st.metric("ìµœì¢… ê°€ì§œë‰´ìŠ¤ í™•ë¥ ", f"{final_prob}%", delta=f"AI Judge: {ai_judge_score}pt")
            with col_b:
                icon = "ğŸŸ¢" if final_prob < 30 else "ğŸ”´" if final_prob > 60 else "ğŸŸ "
                verdict = "ì•ˆì „ (Verified)" if final_prob < 30 else "ìœ„í—˜ (Fake/Bias)" if final_prob > 60 else "ì£¼ì˜ (Caution)"
                st.metric("ì¢…í•© AI íŒì •", f"{icon} {verdict}")
            with col_c: 
                st.metric("AI Intelligence Level", f"{total_intelligence} Nodes", delta="Twin-Engine Active")
            
            st.divider()
            st.subheader("ğŸ§  Intelligence Map")
            render_intelligence_distribution(final_prob)

            if is_ai: st.warning(f"ğŸ¤– **AI ìƒì„± ì½˜í…ì¸  ê°ì§€ë¨**: {ai_msg}")
            if is_official: st.success(f"ğŸ›¡ï¸ **ê³µì‹ ì–¸ë¡ ì‚¬ ì±„ë„({uploader})ì…ë‹ˆë‹¤.**")

            st.divider()
            col1, col2 = st.columns([1, 1.4])
            with col1:
                st.write("**[ì˜ìƒ ìƒì„¸ ì •ë³´]**")
                st.table(pd.DataFrame({"í•­ëª©": ["ì˜ìƒ ì œëª©", "ì±„ë„ëª…", "ì¡°íšŒìˆ˜", "í•´ì‹œíƒœê·¸"], "ë‚´ìš©": [title, uploader, f"{info.get('view_count',0):,}íšŒ", hashtag_display]}))
                st.info(f"ğŸ¯ **Investigator (Key A) ì¶”ì¶œ ê²€ìƒ‰ì–´**: {query}")
                with st.container(border=True):
                    st.markdown("ğŸ“ **ì˜ìƒ ë‚´ìš© ìš”ì•½**")
                    st.write(summary)
                
                st.write("**[Score Breakdown]**")
                render_score_breakdown([
                    ["Rule-based Algo Score (60%)", algo_final_prob, "ê¸°ì¡´ íŒ¨í„´/ë‰´ìŠ¤ ë§¤ì¹­ ì ìˆ˜"],
                    ["AI Judge Score (40%)", ai_judge_score, "Gemini ìµœì¢… ì¶”ë¡  ì ìˆ˜"],
                    ["ì§„ì‹¤ ë°ì´í„° ë§¥ë½", t_impact, "ë‚´ë¶€ DB ì§„ì‹¤ ë°ì´í„°ì™€ ìœ ì‚¬ì„±"],
                    ["ê°€ì§œ íŒ¨í„´ ë§¥ë½", f_impact, "ë‚´ë¶€ DB ê°€ì§œ ë°ì´í„°ì™€ ìœ ì‚¬ì„±"],
                    ["ë‰´ìŠ¤ ë§¤ì¹­ ìƒíƒœ", f"{max_match}%", "ê°€ì¥ ìœ ì‚¬í•œ ê¸°ì‚¬ì™€ì˜ ì¼ì¹˜ìœ¨"],
                    ["ì¹¨ë¬µì˜ ë©”ì•„ë¦¬", silent_penalty, "ê´€ë ¨ ê¸°ì‚¬ ë¶€ì¬ ì‹œ í˜ë„í‹°"],
                    ["ì—¬ë¡ /ì œëª©/íƒœê·¸ ê°€ê°", sent_score + clickbait + abuse_score, ""]
                ])

            with col2:
                st.subheader("ğŸ“Š 5ëŒ€ ì •ë°€ ë¶„ì„ ì¦ê±°")
                
                st.markdown("**[ì¦ê±° 0] Semantic Vector Space (Internal DB)**")
                colored_progress_bar("âœ… ì§„ì‹¤ ì˜ì—­ ê·¼ì ‘ë„", ts, "#2ecc71")
                colored_progress_bar("ğŸš¨ ê±°ì§“ ì˜ì—­ ê·¼ì ‘ë„", fs, "#e74c3c")
                st.write("---")

                st.markdown(f"**[ì¦ê±° 1] ë‰´ìŠ¤ êµì°¨ ëŒ€ì¡° (Dual-Layer)**")
                if news_ev: st.dataframe(pd.DataFrame(news_ev), use_container_width=True, hide_index=True)
                else: st.warning("ğŸ” ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (Silent Echo Risk)")
                    
                st.markdown("**[ì¦ê±° 2] ì‹œì²­ì ì—¬ë¡  ì‹¬ì¸µ ë¶„ì„**")
                if cmts: st.table(pd.DataFrame([["ìµœë‹¤ ë¹ˆì¶œ í‚¤ì›Œë“œ", ", ".join(top_kw)], ["ë…¼ë€ ê°ì§€ ì—¬ë¶€", f"{red_cnt}íšŒ"], ["ì£¼ì œ ì¼ì¹˜ë„", f"{rel_score}% ({rel_msg})"]], columns=["í•­ëª©", "ë‚´ìš©"]))
                
                st.markdown("**[ì¦ê±° 3] ìë§‰ ì„¸ë§Œí‹± ì‹¬ì¸µ ëŒ€ì¡°**")
                top_kw_str = ", ".join([f"{w}({c})" for w, c in top_transcript_keywords])
                st.table(pd.DataFrame([["ì˜ìƒ ìµœë‹¤ ì–¸ê¸‰ í‚¤ì›Œë“œ", top_kw_str], ["ì œëª© ë‚šì‹œì–´", "ìˆìŒ" if clickbait > 0 else "ì—†ìŒ"], ["ì„ ë™ì„± ì§€ìˆ˜", f"{agitation}íšŒ"]], columns=["ë¶„ì„ í•­ëª©", "íŒì • ê²°ê³¼"]))
                
                st.markdown("**[ì¦ê±° 4] AI ìµœì¢… ë¶„ì„ íŒë‹¨ (Judge Verdict)**")
                with st.container(border=True):
                    st.write(f"âš–ï¸ **íŒê²°:** {ai_judge_reason}")
                    st.caption(f"* Gemini ë…ë¦½ ì¶”ë¡  ì ìˆ˜: {ai_judge_score}ì  (Key B)")

                # ê²°ê³¼ í•´ì„ ë¦¬í¬íŠ¸
                reasons = []
                if final_prob >= 60:
                    reasons.append("ğŸš¨ **ìœ„í—˜ ê°ì§€**: AI íŒì‚¬ì™€ ì•Œê³ ë¦¬ì¦˜ ëª¨ë‘ ì´ ì˜ìƒì˜ ì£¼ì¥ì„ ì˜ì‹¬í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
                    if len(news_ev) == 0: reasons.append("ğŸ”‡ **ê·¼ê±° ë¶€ì¬**: ìê·¹ì ì¸ ì£¼ì¥ì— ë¹„í•´ ì–¸ë¡  ë³´ë„ê°€ ì „ë¬´í•©ë‹ˆë‹¤.")
                elif final_prob <= 30:
                    reasons.append("âœ… **ì•ˆì „ íŒì •**: ì˜ìƒ ë‚´ìš©ì´ ì£¼ìš” ë‰´ìŠ¤ ë³´ë„ì™€ ì¼ì¹˜í•˜ë©°, AI ì¶”ë¡  ê²°ê³¼ë„ ê¸ì •ì ì…ë‹ˆë‹¤.")
                else:
                    reasons.append("âš ï¸ **ì£¼ì˜ ìš”ë§**: ì¼ë¶€ ê³¼ì¥ëœ í‘œí˜„ì´ë‚˜ í™•ì¸ë˜ì§€ ì•Šì€ ì‚¬ì‹¤ì´ í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                
                st.success(f"ğŸ” ìµœì¢… ë¶„ì„ ê²°ê³¼: **{final_prob}ì **")
                for r in reasons: st.write(r)

        except Exception as e: st.error(f"ì˜¤ë¥˜: {e}")

# --- [UI Layout] ---
st.title("âš–ï¸ Fact-Check Center v73.0 (Smart Summary)")

# [ë²•ì  ê³ ì§€ ë³µêµ¬]
with st.container(border=True):
    st.markdown("### ğŸ›¡ï¸ ë²•ì  ê³ ì§€ ë° ì±…ì„ í•œê³„ (Disclaimer)\në³¸ ì„œë¹„ìŠ¤ëŠ” **ì¸ê³µì§€ëŠ¥(AI) ë° ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜**ìœ¼ë¡œ ì˜ìƒì˜ ì‹ ë¢°ë„ë¥¼ ë¶„ì„í•˜ëŠ” ë³´ì¡° ë„êµ¬ì…ë‹ˆë‹¤. \në¶„ì„ ê²°ê³¼ëŠ” ë²•ì  íš¨ë ¥ì´ ì—†ìœ¼ë©°, ìµœì¢… íŒë‹¨ì˜ ì±…ì„ì€ ì‚¬ìš©ìì—ê²Œ ìˆìŠµë‹ˆë‹¤.")
    st.markdown("* **Engine A (Investigator)**: ë¬¸ë§¥ ìµœì í™” ê²€ìƒ‰ì–´ ì¶”ì¶œ (Summary-Based)\n* **Engine B (Judge)**: ë‰´ìŠ¤ ëŒ€ì¡° ë° ìµœì¢… ì§„ì‹¤ ì¶”ë¡  (Full-Context)")
    agree = st.checkbox("ìœ„ ë‚´ìš©ì„ í™•ì¸í•˜ì˜€ìœ¼ë©°, ì´ì— ë™ì˜í•©ë‹ˆë‹¤. (ë™ì˜ ì‹œ ë¶„ì„ ë²„íŠ¼ í™œì„±í™”)")

url_input = st.text_input("ğŸ”— ë¶„ì„í•  ìœ íŠœë¸Œ URL")
if st.button("ğŸš€ ì •ë°€ ë¶„ì„ ì‹œì‘", use_container_width=True, disabled=not agree):
    if url_input: run_forensic_main(url_input)
    else: st.warning("URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

st.divider()
st.subheader("ğŸ—‚ï¸ í•™ìŠµ ë°ì´í„° ê´€ë¦¬ (Cloud Knowledge Base)")
try:
    response = supabase.table("analysis_history").select("*").order("id", desc=True).execute()
    df = pd.DataFrame(response.data)
except: df = pd.DataFrame()

if not df.empty:
    if st.session_state["is_admin"]:
        df['Delete'] = False
        edited_df = st.data_editor(df[['Delete', 'id', 'analysis_date', 'video_title', 'fake_prob', 'keywords']], hide_index=True, use_container_width=True)
        if st.button("ğŸ—‘ï¸ ì„ íƒ í•­ëª© ì‚­ì œ", type="primary"):
            to_delete = edited_df[edited_df.Delete]
            if not to_delete.empty:
                for index, row in to_delete.iterrows(): supabase.table("analysis_history").delete().eq("id", row['id']).execute()
                st.success("ì‚­ì œ ì™„ë£Œ!"); time.sleep(1); st.rerun()
    else:
        st.dataframe(df[['analysis_date', 'video_title', 'fake_prob', 'keywords']], hide_index=True, use_container_width=True)
else: st.info("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

st.write("")
with st.expander("ğŸ” ê´€ë¦¬ì ì ‘ì† (Admin Access)"):
    if st.session_state["is_admin"]:
        st.success("ê´€ë¦¬ì ê¶Œí•œ í™œì„±í™”ë¨")
        if st.button("ë¡œê·¸ì•„ì›ƒ"):
            st.session_state["is_admin"] = False
            st.rerun()
    else:
        input_pwd = st.text_input("Admin Password", type="password")
        if st.button("Login"):
            if input_pwd == ADMIN_PASSWORD:
                st.session_state["is_admin"] = True
                st.rerun()
            else:
                st.error("Access Denied")
