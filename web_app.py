import streamlit as st
from supabase import create_client, Client
import re
import requests
import time
import random
import math
import google.generativeai as genai
from google.generativeai.types import HarmCategory, HarmBlockThreshold
from datetime import datetime
from collections import Counter
import yt_dlp
import pandas as pd
import altair as alt
import json
from bs4 import BeautifulSoup

# --- [1. ì‹œìŠ¤í…œ ì„¤ì •] ---
st.set_page_config(page_title="Fact-Check Center v95.0 (Smart Cache)", layout="wide", page_icon="âš¡")

if "is_admin" not in st.session_state:
    st.session_state["is_admin"] = False

if "debug_logs" not in st.session_state:
    st.session_state["debug_logs"] = []

# ğŸŒŸ Secrets ë¡œë“œ (Streamlit Cloud í™˜ê²½ ê¸°ì¤€)
try:
    YOUTUBE_API_KEY = st.secrets["YOUTUBE_API_KEY"]
    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    ADMIN_PASSWORD = st.secrets["ADMIN_PASSWORD"]
    GOOGLE_API_KEY_A = st.secrets["GOOGLE_API_KEY_A"]
    GOOGLE_API_KEY_B = st.secrets["GOOGLE_API_KEY_B"]
except:
    st.error("âŒ í•„ìˆ˜ í‚¤(API Keys)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. secrets.toml íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

@st.cache_resource
def init_supabase():
    return create_client(SUPABASE_URL, SUPABASE_KEY)

supabase = init_supabase()

# --- [2. ìœ í‹¸ë¦¬í‹°] ---
def parse_gemini_json(text):
    """Gemini ì‘ë‹µì—ì„œ ë§ˆí¬ë‹¤ìš´ì„ ì œê±°í•˜ê³  ìˆœìˆ˜ JSON ê°ì²´ë§Œ ì¶”ì¶œ"""
    try:
        return json.loads(text)
    except:
        try:
            # ```json ... ``` íŒ¨í„´ ì œê±°
            text = re.sub(r'```json\s*', '', text).replace('```', '')
            match = re.search(r'(\{.*\})', text, re.DOTALL)
            if match: return json.loads(match.group(1))
        except: pass
    return None

def extract_video_id(url):
    """ìœ íŠœë¸Œ URLì—ì„œ 11ìë¦¬ ê³ ìœ  ID ì¶”ì¶œ"""
    match = re.search(r'(?:v=|\/)([0-9A-Za-z_-]{11}).*', url)
    return match.group(1) if match else None

# --- [3. ëª¨ë¸ íƒìƒ‰ & ìƒìˆ˜] ---
@st.cache_data(ttl=3600)
def get_all_available_models(api_key):
    genai.configure(api_key=api_key)
    try:
        models = [m.name.replace("models/", "") for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        # Lite -> Flash -> Pro ìˆœìœ¼ë¡œ ê°€ì¤‘ì¹˜ ë¶€ì—¬
        models.sort(key=lambda x: 0 if 'lite' in x else 1 if 'flash' in x else 2)
        return models
    except: return ["gemini-2.0-flash", "gemini-1.5-flash"]

WEIGHT_ALGO = 0.6
WEIGHT_AI = 0.4
OFFICIAL_CHANNELS = ['MBC', 'KBS', 'SBS', 'EBS', 'YTN', 'JTBC', 'TVCHOSUN', 'MBN', 'ì±„ë„A', 'TVì¡°ì„ ', 'ì—°í•©ë‰´ìŠ¤', 'í•œê²¨ë ˆ', 'ì¡°ì„ ì¼ë³´', 'ì¤‘ì•™ì¼ë³´', 'ë™ì•„ì¼ë³´']
STATIC_TRUTH_CORPUS = ["ìœ„ì¥ì „ì… ë¬´í˜ì˜ íŒê²°", "ì„ì˜ì›… ì•”í‘œ ê°•ë ¥ ëŒ€ì‘", "ì •í¬ì› êµìˆ˜ ì €ì†ë…¸í™” ì‹ë‹¨", "ëŒ€ì „ ì¶©ë‚¨ í–‰ì •í†µí•© í•©ì˜"]
STATIC_FAKE_CORPUS = ["ì¶©ê²© í­ë¡œ ê²½ì•…", "ê¸´ê¸‰ ì†ë³´ ì†Œë¦„", "êµ¬ì† ì˜ì¥ ì¦‰ì‹œ ë°œë¶€", "ìœ ì–¸ì¥ ì „ê²© ê³µê°œ", "ì‚¬í˜• ì§‘í–‰ í™•ì •"]

# --- [4. VectorEngine (ë‚´ë¶€ ë°ì´í„° ë¶„ì„)] ---
class VectorEngine:
    def __init__(self):
        self.vocab = set()
        self.truth_vectors = []
        self.fake_vectors = []
    def tokenize(self, text): return re.findall(r'[ê°€-í£]{2,}', text)
    def train(self, truth, fake):
        for t in truth + fake: self.vocab.update(self.tokenize(t))
        self.vocab = sorted(list(self.vocab))
        self.truth_vectors = [self.text_to_vector(t) for t in truth]
        self.fake_vectors = [self.text_to_vector(t) for t in fake]
    def text_to_vector(self, text, vocabulary=None):
        target_vocab = vocabulary if vocabulary else self.vocab
        c = Counter(self.tokenize(text))
        return [c[w] for w in target_vocab]
    def cosine_similarity(self, v1, v2):
        dot = sum(a*b for a,b in zip(v1,v2))
        mag1 = math.sqrt(sum(a*a for a in v1))
        mag2 = math.sqrt(sum(b*b for b in v2))
        return dot/(mag1*mag2) if mag1*mag2 > 0 else 0
    def analyze_position(self, query):
        if not self.vocab: return 0, 0
        qv = self.text_to_vector(query)
        mt = max([self.cosine_similarity(qv, v) for v in self.truth_vectors] or [0])
        mf = max([self.cosine_similarity(qv, v) for v in self.fake_vectors] or [0])
        return mt, mf

vector_engine = VectorEngine()

# --- [5. Gemini Logic (Survior Mode)] ---
safety_settings_none = {HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE, HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE}

def call_gemini_survivor(api_key, prompt, is_json=False):
    genai.configure(api_key=api_key)
    generation_config = {"response_mime_type": "application/json"} if is_json else {}
    all_models = get_all_available_models(api_key)
    logs = []
    for model_name in all_models:
        try:
            model = genai.GenerativeModel(model_name, generation_config=generation_config)
            response = model.generate_content(prompt, safety_settings=safety_settings_none)
            if response.text:
                logs.append(f"âœ… Success: {model_name}")
                return response.text, model_name, logs
        except Exception as e:
            logs.append(f"âŒ Failed ({model_name}): {str(e)[:30]}...")
            time.sleep(0.2)
            continue
    return None, "All Failed", logs

def get_gemini_search_keywords(title, transcript):
    prompt = f"Role: Fact-Check Investigator. Title: {title}. Transcript: {transcript[:10000]}. Extract ONE Korean search query for Google News (Proper Noun + Core Issue). Output: Query string only."
    res, model, logs = call_gemini_survivor(GOOGLE_API_KEY_A, prompt)
    st.session_state["debug_logs"].extend([f"[Key A] {l}" for l in logs])
    return (res.strip(), f"âœ¨ {model}") if res else (title, "âŒ Error")

def scrape_news_content_robust(url):
    try:
        res = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=5, allow_redirects=True)
        soup = BeautifulSoup(res.text, 'html.parser')
        for t in soup(['script', 'style', 'nav', 'footer', 'header']): t.decompose()
        text = " ".join([p.get_text().strip() for p in soup.find_all('p') if len(p.get_text()) > 30])
        return (text[:4000], res.url) if len(text) > 100 else (None, res.url)
    except: return None, url

def deep_verify_news(video_summary, news_url, news_snippet):
    txt, real_url = scrape_news_content_robust(news_url)
    evidence = txt if txt else news_snippet
    source = "Full Article" if txt else "Snippet"
    prompt = f"Context: {video_summary[:2000]}. News: {evidence}. Task: Score match from 0(Truth) to 100(Fake). Output JSON {{'score': int, 'reason': 'short korean reason'}}"
    res, model, logs = call_gemini_survivor(GOOGLE_API_KEY_B, prompt, is_json=True)
    st.session_state["debug_logs"].extend([f"[Key B-Verify] {l}" for l in logs])
    parsed = parse_gemini_json(res)
    if parsed: return parsed.get('score', 50), parsed.get('reason', 'N/A'), source, evidence, real_url
    return 50, "ë¶„ì„ ì‹¤íŒ¨", "Error", "", news_url

def get_gemini_verdict_final(title, transcript, news_list):
    news_summary = "\n".join([f"- {n['ë‰´ìŠ¤ ì œëª©']} (Match Score:{n['ìµœì¢… ì ìˆ˜']}, Evidence:{n['ë¶„ì„ ê·¼ê±°']})" for n in news_list])
    prompt = f"Judge Final Verdict. Video: {title}. News Evidence: {news_summary}. Task: Final Fake Score (0-100). Higher = Fake. Output JSON {{'score': int, 'reason': 'korean reason'}}"
    res, model, logs = call_gemini_survivor(GOOGLE_API_KEY_B, prompt, is_json=True)
    st.session_state["debug_logs"].extend([f"[Key B-Final] {l}" for l in logs])
    parsed = parse_gemini_json(res)
    if parsed: return parsed.get('score', 50), f"{parsed.get('reason')} (By {model})"
    return 50, "íŒê²° ì‹¤íŒ¨"

# --- [6. ìºì‹± ë° ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬] ---
def train_dynamic_vector_engine():
    try:
        res_t = supabase.table("analysis_history").select("video_title").lt("fake_prob", 40).execute()
        res_f = supabase.table("analysis_history").select("video_title").gt("fake_prob", 60).execute()
        dt = [row['video_title'] for row in res_t.data] if res_t.data else []
        df = [row['video_title'] for row in res_f.data] if res_f.data else []
        vector_engine.train(STATIC_TRUTH_CORPUS + dt, STATIC_FAKE_CORPUS + df)
        return len(dt)+len(df), len(dt), len(df)
    except: 
        vector_engine.train(STATIC_TRUTH_CORPUS, STATIC_FAKE_CORPUS)
        return 0, 0, 0

def check_cache(video_id):
    try:
        response = supabase.table("analysis_history").select("*").ilike("video_url", f"%{video_id}%").order("id", desc=True).limit(1).execute()
        if response.data: return response.data[0]
    except: pass
    return None

def save_analysis(channel, title, prob, url, keywords, full_report):
    try:
        data = {
            "channel_name": channel, "video_title": title, "fake_prob": prob,
            "analysis_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "video_url": url, "keywords": keywords,
            "detail_json": json.dumps(full_report, ensure_ascii=False)
        }
        supabase.table("analysis_history").insert(data).execute()
    except Exception as e:
        st.warning(f"ë°ì´í„° ì €ì¥ ì‹¤íŒ¨ (detail_json ì»¬ëŸ¼ í™•ì¸ ìš”ë§): {e}")

# --- [7. UI Helper] ---
def render_final_report(final_prob, db_count, title, query, report_data, is_cached=False):
    if is_cached:
        st.success(f"ğŸ‰ **ê¸°ì¡´ ë¶„ì„ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ! (Smart Cache)**: {report_data.get('analysis_date', 'N/A')}ì— ì´ë¯¸ ë¶„ì„ëœ ì˜ìƒì…ë‹ˆë‹¤.")
    
    st.subheader("ğŸ•µï¸ Dual-Engine Analysis Result")
    col_a, col_b, col_c = st.columns(3)
    with col_a: st.metric("ìµœì¢… ê°€ì§œë‰´ìŠ¤ í™•ë¥ ", f"{final_prob}%", delta="AI Judge Score")
    with col_b:
        icon = "ğŸŸ¢" if final_prob < 30 else "ğŸ”´" if final_prob > 60 else "ğŸŸ "
        verdict = "ì•ˆì „ (Verified)" if final_prob < 30 else "ìœ„í—˜ (Fake/Bias)" if final_prob > 60 else "ì£¼ì˜ (Caution)"
        st.metric("ì¢…í•© AI íŒì •", f"{icon} {verdict}")
    with col_c: st.metric("AI Intelligence Level", f"{db_count} Nodes", delta="Active Memory")
    
    st.divider()
    st.write(f"**ì˜ìƒ ì œëª©:** {title}")
    st.info(f"ğŸ¯ **ì¶”ì¶œ ê²€ìƒ‰ í‚¤ì›Œë“œ:** {query}")
    
    # ì ìˆ˜í‘œ ë Œë”ë§
    st.write("### ğŸ“Š ë¶„ì„ ìŠ¤ì½”ì–´ ì„¸ë¶€ ì •ë³´")
    st.table(pd.DataFrame(report_data.get('score_breakdown', []), columns=["ë¶„ì„ í•­ëª©", "ë³€ë™ ì ìˆ˜", "ìƒì„¸ ì„¤ëª…"]))
    
    # ë‰´ìŠ¤ ì¦ê±° ë Œë”ë§
    st.write("### ğŸ“° ë‰´ìŠ¤ êµì°¨ ëŒ€ì¡° ì¦ê±°")
    news_ev = report_data.get('news_evidence', [])
    if news_ev:
        st.dataframe(pd.DataFrame(news_ev), column_config={"ì›ë¬¸": st.column_config.LinkColumn("ê¸°ì‚¬ ë§í¬")}, hide_index=True)
    else: st.warning("ëŒ€ì¡°í•  ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
    with st.container(border=True):
        st.write(f"âš–ï¸ **ìµœì¢… íŒê²° ìš”ì•½:** {report_data.get('ai_reason', 'N/A')}")

# --- [8. ìœ íŠœë¸Œ ë°ì´í„° ìˆ˜ì§‘ê¸°] ---
def fetch_real_transcript(info):
    try:
        subs = info.get('subtitles') or {}
        auto = info.get('automatic_captions') or {}
        merged = {**subs, **auto}
        if 'ko' in merged:
            for f in merged['ko']:
                if f['ext'] == 'vtt':
                    res = requests.get(f['url'])
                    lines = [l.strip() for l in res.text.splitlines() if l.strip() and '-->' not in l and '<' not in l]
                    return " ".join(lines[2:]), "Success"
    except: pass
    return None, "Fail"

def fetch_news_regex(query):
    try:
        rss = f"https://news.google.com/rss/search?q={requests.utils.quote(query)}&hl=ko&gl=KR"
        raw = requests.get(rss, timeout=5).text
        items = re.findall(r'<item>(.*?)</item>', raw, re.DOTALL)
        res = []
        for i in items:
            t = re.search(r'<title>(.*?)</title>', i)
            l = re.search(r'<link>(.*?)</link>', i)
            d = re.search(r'<description>(.*?)</description>', i)
            if t and l:
                res.append({
                    'title': t.group(1).replace("<![CDATA[", "").replace("]]>", ""),
                    'desc': re.sub('<.*?>', '', d.group(1)) if d else "",
                    'link': l.group(1).strip()
                })
        return res[:5]
    except: return []

# --- [9. ë©”ì¸ ë¡œì§] ---
def run_forensic_main(url):
    st.session_state["debug_logs"] = []
    vid = extract_video_id(url)
    if not vid:
        st.error("ìœ íš¨í•˜ì§€ ì•Šì€ ìœ íŠœë¸Œ URLì…ë‹ˆë‹¤.")
        return

    db_count, _, _ = train_dynamic_vector_engine()
    cached = check_cache(vid)
    
    if cached:
        try:
            details = json.loads(cached.get('detail_json', '{}'))
            render_final_report(cached['fake_prob'], db_count, cached['video_title'], cached.get('keywords', 'N/A'), details, is_cached=True)
            return
        except: pass

    # ì‹ ê·œ ë¶„ì„ (Progress Bar)
    my_bar = st.progress(0, text="ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ì¤‘...")
    
    with yt_dlp.YoutubeDL({'quiet': True, 'skip_download': True, 'writesubtitles': True}) as ydl:
        try:
            info = ydl.extract_info(url, download=False)
            title = info.get('title', 'ì œëª© ì—†ìŒ')
            uploader = info.get('uploader', 'ë¯¸ìƒ')
            desc = info.get('description', '')
            
            my_bar.progress(20, "1ë‹¨ê³„: ìë§‰ ë° ë¬¸ë§¥ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            trans, _ = fetch_real_transcript(info)
            full_text = trans if trans else desc
            
            my_bar.progress(40, "2ë‹¨ê³„: AI ìˆ˜ì‚¬ê´€ ê°€ë™ ë° í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
            query, _ = get_gemini_search_keywords(title, full_text)
            
            my_bar.progress(60, "3ë‹¨ê³„: ë‰´ìŠ¤ ë”¥ ì›¹ í¬ë¡¤ë§ ë° íŒ©íŠ¸ì²´í¬ ì¤‘...")
            news_items = fetch_news_regex(query)
            news_ev = []; max_match = 0
            for item in news_items[:3]:
                score, reason, src, _, real_url = deep_verify_news(full_text, item['link'], item['desc'])
                if score > max_match: max_match = score
                news_ev.append({"ë‰´ìŠ¤ ì œëª©": item['title'], "ì¼ì¹˜ë„": f"{score}%", "ìµœì¢… ì ìˆ˜": score, "ë¶„ì„ ê·¼ê±°": reason, "ì›ë¬¸": real_url})
            
            news_penalty = -30 if max_match <= 20 else (30 if max_match >= 80 else 0)
            
            ts, fs = vector_engine.analyze_position(query + " " + title)
            t_impact = int(ts * 30) * -1; f_impact = int(fs * 30)
            
            my_bar.progress(90, "4ë‹¨ê³„: AI íŒì‚¬ ìµœì¢… íŒê²°ë¬¸ ì‘ì„± ì¤‘...")
            ai_score, ai_reason = get_gemini_verdict_final(title, full_text, news_ev)
            
            algo_base = 50 + t_impact + f_impact + news_penalty
            final_prob = max(1, min(99, int(algo_base * WEIGHT_ALGO + ai_score * WEIGHT_AI)))
            
            full_report = {
                "score_breakdown": [
                    ["ê¸°ë³¸ ì¤‘ë¦½ ì ìˆ˜", 50, "ì¤‘ë¦½ ìƒíƒœì—ì„œ ë¶„ì„ ì‹œì‘"],
                    ["ì§„ì‹¤ ë°ì´í„° ìœ ì‚¬ì„±", t_impact, "ë‚´ë¶€ DB ì§„ì‹¤ ë°ì´í„°ì™€ ì¼ì¹˜ë„"],
                    ["ê±°ì§“ íŒ¨í„´ ìœ ì‚¬ì„±", f_impact, "ë‚´ë¶€ DB ê°€ì§œ ë°ì´í„°ì™€ ì¼ì¹˜ë„"],
                    ["ë‰´ìŠ¤ êµì°¨ ëŒ€ì¡° ê²°ê³¼", news_penalty, "ë‰´ìŠ¤ ë³´ë„ ë‚´ìš©ê³¼ì˜ ë¶€í•© ì—¬ë¶€"],
                    ["AI ìµœì¢… ì¶”ë¡ ", ai_score, ai_reason]
                ],
                "news_evidence": news_ev,
                "ai_reason": ai_reason,
                "analysis_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            }
            
            save_analysis(uploader, title, final_prob, url, query, full_report)
            my_bar.empty()
            render_final_report(final_prob, db_count, title, query, full_report, is_cached=False)

        except Exception as e:
            st.error(f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# --- [10. UI ë ˆì´ì•„ì›ƒ] ---
st.title("âš–ï¸ Fact-Check Center v95.0")
st.markdown("> **ìœ íŠœë¸Œ URL í•˜ë‚˜ë¡œ ì§„ì‹¤ì„ ê°€ë ¤ë‚´ëŠ” AI ì—ì´ì „íŠ¸**")

with st.container(border=True):
    st.caption("ğŸ›¡ï¸ ë³¸ ì„œë¹„ìŠ¤ëŠ” AI ë° ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜ ë³´ì¡° ë„êµ¬ë¡œ, ë²•ì  íš¨ë ¥ì´ ì—†ìŒì„ ê³ ì§€í•©ë‹ˆë‹¤.")
    url_input = st.text_input("ğŸ”— ë¶„ì„í•  ìœ íŠœë¸Œ URLì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="https://www.youtube.com/watch?v=...")
    analyze_btn = st.button("ğŸš€ ì •ë°€ ë¶„ì„ ì‹œì‘ (ë¬´ë£Œ AI ì¿¼í„° ì‚¬ìš©)", use_container_width=True)

if analyze_btn and url_input:
    run_forensic_main(url_input)

st.divider()
st.subheader("ğŸ—‚ï¸ ìµœê·¼ ë¶„ì„ íˆìŠ¤í† ë¦¬")
try:
    history = supabase.table("analysis_history").select("analysis_date, video_title, fake_prob, keywords").order("id", desc=True).limit(10).execute()
    if history.data:
        st.dataframe(pd.DataFrame(history.data), hide_index=True, use_container_width=True)
    else: st.info("ì•„ì§ ë¶„ì„ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
except: pass

# Admin Section
with st.expander("ğŸ” ê´€ë¦¬ì ì ‘ì†"):
    pwd = st.text_input("Admin PW", type="password")
    if pwd == ADMIN_PASSWORD:
        st.session_state["is_admin"] = True
        st.success("ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™”")
        if st.session_state["debug_logs"]:
            st.text_area("System Logs", "\n".join(st.session_state["debug_logs"]), height=200)
