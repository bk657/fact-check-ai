import streamlit as st
import re
import requests
import time
import random
import google.generativeai as genai # ğŸŒŸ êµ¬ê¸€ AI ë¼ì´ë¸ŒëŸ¬ë¦¬
from datetime import datetime
from collections import Counter
import yt_dlp
import pandas as pd
from bs4 import BeautifulSoup
import altair as alt
import traceback

# --- [1. ì‹œìŠ¤í…œ ì„¤ì •] ---
st.set_page_config(page_title="Fact-Check v57.0 (Gemini Powered)", layout="wide", page_icon="âš–ï¸")

# ğŸŒŸ Secrets ë¡œë“œ (Gemini Key í•„ìˆ˜)
try:
    YOUTUBE_API_KEY = st.secrets["YOUTUBE_API_KEY"]
    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    ADMIN_PASSWORD = st.secrets["ADMIN_PASSWORD"]
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"] # ğŸŒŸ í•„ìˆ˜
except:
    st.error("âŒ í•„ìˆ˜ í‚¤(Secrets)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. GEMINI_API_KEYë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()

# DB ì—°ê²°
from supabase import create_client
@st.cache_resource
def init_supabase():
    return create_client(SUPABASE_URL, SUPABASE_KEY)

try:
    supabase = init_supabase()
except:
    st.error("âŒ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨")
    st.stop()

# --- [2. í•µì‹¬ ì—”ì§„: Gemini AI Keyword Extractor] ---
def ask_gemini_keywords(title, transcript):
    """
    Geminiì—ê²Œ ì œëª©ê³¼ ìë§‰ì„ ì£¼ê³ , ë‰´ìŠ¤ ê²€ìƒ‰ìš© ìµœì  í‚¤ì›Œë“œë¥¼ ë°›ì•„ì˜µë‹ˆë‹¤.
    """
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-1.5-flash') # ë¹ ë¥´ê³  ë˜‘ë˜‘í•œ ëª¨ë¸
        
        # ğŸŒŸ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§: AIì—ê²Œ êµ¬ì²´ì ì¸ ì§€ì‹œë¥¼ ë‚´ë¦¼
        prompt = f"""
        ë„ˆëŠ” íŒ©íŠ¸ì²´í¬ë¥¼ ìœ„í•œ ì „ë¬¸ ê²€ìƒ‰ì›ì´ì•¼. 
        ì•„ë˜ ìœ íŠœë¸Œ ì˜ìƒì˜ [ì œëª©]ê³¼ [ìë§‰ ìš”ì•½]ì„ ë¶„ì„í•´ì„œ, ì´ ë‚´ìš©ì´ ì‚¬ì‹¤ì¸ì§€ ë‰´ìŠ¤ ê¸°ì‚¬ë¡œ í™•ì¸í•˜ê¸° ìœ„í•œ 'ìµœì ì˜ ê²€ìƒ‰ì–´' 1ê°œë¥¼ ë§Œë“¤ì–´ì¤˜.

        [ì¡°ê±´]
        1. ì˜ìƒì˜ í•µì‹¬ ì£¼ì¥(ëˆ„ê°€, ë¬´ì—‡ì„, ì–´ë–¤ ì‚¬ê±´)ì´ í¬í•¨ë˜ì–´ì•¼ í•´.
        2. 'ì¶©ê²©', 'ê²½ì•…', 'ìŠ¬í”ˆ' ê°™ì€ ê°ì •ì  ìˆ˜ì‹ì–´ëŠ” ë¹¼ê³ , 'íŒ©íŠ¸(ëª…ì‚¬)' ìœ„ì£¼ë¡œ êµ¬ì„±í•´.
        3. ì˜ˆì‹œ: 'ì´ì¬ìš© íšŒì¥ì˜ ìŠ¬í”ˆ ì‚¬ì—°' (X) -> 'ì´ì¬ìš© ì´í˜¼ ì‚¬ìœ ' or 'ì´ì¬ìš© ì„ì„¸ë ¹ ìœ„ìë£Œ' (O)
        4. ì˜¤ì§ ê²€ìƒ‰ì–´ ë¬¸ìì—´ í•˜ë‚˜ë§Œ ì¶œë ¥í•´. (ë”°ì˜´í‘œ ì—†ì´)

        [ì œëª©]: {title}
        [ìë§‰ ì•ë¶€ë¶„]: {transcript[:1500]}
        """
        
        response = model.generate_content(prompt)
        return response.text.strip()
    except Exception as e:
        # ì—ëŸ¬ ë‚˜ë©´ ê¸°ì¡´ ë¡œì§(ë°±ì—…)ìœ¼ë¡œ ë¦¬í„´
        print(f"Gemini Error: {e}")
        return None

# --- [3. ë³´ì¡° ê¸°ëŠ¥ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)] ---
VITAL_KEYWORDS = ['ìœ„ë…', 'ì‚¬ë§', 'ë³„ì„¸', 'êµ¬ì†', 'ì²´í¬', 'ê¸°ì†Œ', 'ì‹¤í˜•', 'ì‘ê¸‰ì‹¤', 'ì´í˜¼', 'ë¶ˆí™”', 'íŒŒê²½', 'ì¶©ê²©', 'ê²½ì•…', 'ì†ë³´', 'ê¸´ê¸‰', 'í­ë¡œ', 'ì–‘ì„±', 'í™•ì§„', 'ì‹¬ì •ì§€', 'ë‡Œì‚¬', 'ì¤‘íƒœ', 'ì••ìˆ˜ìˆ˜ìƒ‰', 'ì†Œí™˜', 'í‡´ì§„', 'íƒ„í•µ', 'ë‚´ë€', 'ê°„ì²©']
CRITICAL_STATE_KEYWORDS = ['ë³„ê±°', 'ì´í˜¼', 'íŒŒê²½', 'ì‚¬ë§', 'ìœ„ë…', 'êµ¬ì†', 'ì²´í¬', 'ì‹¤í˜•', 'ë¶ˆí™”', 'í­ë¡œ', 'ì¶©ê²©', 'ë…¼ë€', 'ì¤‘íƒœ', 'ì‹¬ì •ì§€', 'ë‡Œì‚¬', 'ì••ìˆ˜ìˆ˜ìƒ‰', 'ì†Œí™˜', 'íŒŒì‚°', 'ë¹šë”ë¯¸', 'ì „ê³¼', 'ê°ì˜¥', 'ê°„ì²©']
OFFICIAL_CHANNELS = ['MBC', 'KBS', 'SBS', 'EBS', 'YTN', 'JTBC', 'TVCHOSUN', 'MBN', 'CHANNEL A', 'OBS', 'ì±„ë„A', 'TVì¡°ì„ ', 'ì—°í•©ë‰´ìŠ¤', 'YONHAP', 'í•œê²¨ë ˆ', 'ê²½í–¥', 'ì¡°ì„ ', 'ì¤‘ì•™', 'ë™ì•„']

def normalize_korean_word(word):
    word = re.sub(r'[^ê°€-í£0-9]', '', word)
    josa_list = ['ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ê²Œ', 'ë¡œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'ë„', 'ë§Œ', 'í•œí…Œ', 'ê¹Œì§€', 'ë¶€í„°']
    for josa in josa_list:
        if word.endswith(josa): return word[:-len(josa)]
    return word

def extract_meaningful_tokens(text):
    raw_tokens = re.findall(r'[ê°€-í£]{2,}', text)
    noise = ['ì¶©ê²©', 'ê²½ì•…', 'ì†ë³´', 'ê¸´ê¸‰', 'ì˜¤ëŠ˜', 'ë‚´ì¼', 'ì§€ê¸ˆ', 'ê²°êµ­', 'ë‰´ìŠ¤', 'ì˜ìƒ', 'ëŒ€ë¶€ë¶„', 'ì´ìœ ', 'ì™œ', 'ìˆëŠ”', 'ì—†ëŠ”', 'í•˜ëŠ”', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ì§„ì§œ', 'ì •ë§', 'ë„ˆë¬´', 'ê·¸ëƒ¥', 'ì´ì œ', 'ì‚¬ì‹¤', 'êµ­ë¯¼', 'ìš°ë¦¬', 'ëŒ€í•œë¯¼êµ­', 'ì—¬ëŸ¬ë¶„']
    return [normalize_korean_word(w) for w in raw_tokens if normalize_korean_word(w) not in noise]

def generate_backup_query(title):
    # Gemini ì‹¤íŒ¨ ì‹œ ì‚¬ìš©í•  ë°±ì—… ë¡œì§
    tokens = extract_meaningful_tokens(title)
    return " ".join(tokens[:3]) if tokens else title

def fetch_real_transcript(info):
    try:
        url = None
        for key in ['subtitles', 'automatic_captions']:
            if key in info and 'ko' in info[key]:
                for fmt in info[key]['ko']:
                    if fmt['ext'] == 'vtt': url = fmt['url']; break
            if url: break
        if url:
            res = requests.get(url)
            if res.status_code == 200:
                content = res.text
                if "#EXTM3U" in content: return None
                clean = []
                for line in content.splitlines():
                    if '-->' not in line and 'WEBVTT' not in line and line.strip():
                        t = re.sub(r'<[^>]+>', '', line).strip()
                        if t and t not in clean: clean.append(t)
                return " ".join(clean)
    except: pass
    return info.get('description', '')

def fetch_comments_via_api(video_id):
    try:
        url = "https://www.googleapis.com/youtube/v3/commentThreads"
        res = requests.get(url, params={'part': 'snippet', 'videoId': video_id, 'key': YOUTUBE_API_KEY, 'maxResults': 50, 'order': 'relevance'})
        if res.status_code == 200:
            items = [i['snippet']['topLevelComment']['snippet']['textDisplay'] for i in res.json().get('items', [])]
            return items
    except: pass
    return []

def fetch_news_regex(query):
    news_res = []
    try:
        rss_url = f"https://news.google.com/rss/search?q={requests.utils.quote(query)}&hl=ko&gl=KR&ceid=KR:ko"
        raw = requests.get(rss_url, timeout=5).text
        items = re.findall(r'<item>(.*?)</item>', raw, re.DOTALL)
        for item in items[:10]:
            t = re.search(r'<title>(.*?)</title>', item)
            nt = t.group(1).replace("<![CDATA[", "").replace("]]>", "") if t else ""
            news_res.append({'title': nt})
    except: pass
    return news_res

def calculate_match_score(news_title, query):
    q_tokens = set(extract_meaningful_tokens(query))
    n_tokens = set(extract_meaningful_tokens(news_title))
    match_cnt = len(q_tokens & n_tokens)
    if match_cnt >= 2: return 80
    elif match_cnt == 1: return 40
    return 0

def summarize_text_simple(text):
    return ". ".join([s.strip() for s in text.split('.')[:3] if s.strip()]) + "."

def save_analysis(channel, title, score, url, query):
    try:
        supabase.table("analysis_history").insert({
            "channel_name": channel, "video_title": title, "fake_prob": score,
            "analysis_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "video_url": url, "keywords": query
        }).execute()
    except: pass

def render_score_breakdown(data_list):
    style = """<style>table.score-table { width: 100%; border-collapse: separate; border-spacing: 0; border: 1px solid #e0e0e0; border-radius: 8px; overflow: hidden; font-family: sans-serif; font-size: 14px; margin-top: 10px;} table.score-table th { background-color: #f8f9fa; color: #495057; font-weight: bold; padding: 12px 15px; text-align: left; border-bottom: 1px solid #e0e0e0; } table.score-table td { padding: 12px 15px; border-bottom: 1px solid #f0f0f0; color: #333; } table.score-table tr:last-child td { border-bottom: none; } .badge { padding: 4px 8px; border-radius: 6px; font-weight: 700; font-size: 11px; display: inline-block; text-align: center; min-width: 45px; } .badge-danger { background-color: #ffebee; color: #d32f2f; } .badge-success { background-color: #e8f5e9; color: #2e7d32; } .badge-neutral { background-color: #f5f5f5; color: #757575; border: 1px solid #e0e0e0; }</style>"""
    rows = ""
    for item, score, note in data_list:
        badge = f'<span class="badge badge-danger">+{score}</span>' if score > 0 else f'<span class="badge badge-success">{score}</span>' if score < 0 else f'<span class="badge badge-neutral">0</span>'
        rows += f"<tr><td>{item}<br><span style='color:#888; font-size:11px;'>{note}</span></td><td style='text-align: right;'>{badge}</td></tr>"
    st.markdown(f"{style}<table class='score-table'><thead><tr><th>ë¶„ì„ í•­ëª©</th><th style='text-align: right;'>ì ìˆ˜</th></tr></thead><tbody>{rows}</tbody></table>", unsafe_allow_html=True)

def colored_progress_bar(label, percent, color):
    st.markdown(f"""<div style="margin-bottom: 10px;"><div style="display: flex; justify-content: space-between; margin-bottom: 3px;"><span style="font-size: 13px; font-weight: 600; color: #555;">{label}</span><span style="font-size: 13px; font-weight: 700; color: {color};">{round(percent * 100, 1)}%</span></div><div style="background-color: #eee; border-radius: 5px; height: 8px; width: 100%;"><div style="background-color: {color}; height: 8px; width: {percent * 100}%; border-radius: 5px;"></div></div></div>""", unsafe_allow_html=True)

# --- [4. ë©”ì¸ ì‹¤í–‰] ---
st.title("âš–ï¸ Fact-Check v57.0 (Gemini Engine)")
with st.container(border=True):
    st.markdown("### ğŸ›¡ï¸ Disclaimer\në³¸ ì„œë¹„ìŠ¤ëŠ” **Gemini AI**ë¥¼ í™œìš©í•˜ì—¬ ì˜ìƒì˜ ë§¥ë½ì„ ë¶„ì„í•˜ê³  ë‰´ìŠ¤ì™€ ëŒ€ì¡°í•©ë‹ˆë‹¤. ìµœì¢… íŒë‹¨ì€ ì‚¬ìš©ìì—ê²Œ ìˆìŠµë‹ˆë‹¤.")
    agree = st.checkbox("ë™ì˜í•©ë‹ˆë‹¤.")

url_input = st.text_input("ğŸ”— ë¶„ì„í•  ìœ íŠœë¸Œ URL")
if st.button("ğŸš€ ì •ë°€ ë¶„ì„ ì‹œì‘", use_container_width=True, disabled=not agree):
    if url_input:
        vid = re.search(r'(?:v=|\/)([0-9A-Za-z_-]{11}).*', url_input)
        if vid: vid = vid.group(1)

        with st.status("ğŸ•µï¸ Gemini AIê°€ ì˜ìƒì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...", expanded=True) as status:
            with yt_dlp.YoutubeDL({'quiet': True}) as ydl:
                try:
                    info = ydl.extract_info(url_input, download=False)
                    title = info.get('title', ''); uploader = info.get('uploader', '')
                    tags = info.get('tags', [])
                    
                    st.write("ğŸ“ ìë§‰(Transcript) ì¶”ì¶œ ì¤‘...")
                    full_text = fetch_real_transcript(info)
                    
                    # ğŸŒŸ [í•µì‹¬] Geminiì—ê²Œ í‚¤ì›Œë“œ ì¶”ì¶œ ìš”ì²­
                    st.write("ğŸ§  Geminiê°€ ìµœì ì˜ ê²€ìƒ‰ì–´ë¥¼ ì¶”ë¡  ì¤‘...")
                    query = ask_gemini_keywords(title, full_text)
                    
                    # Geminiê°€ ì‹¤íŒ¨í•˜ë©´ ë°±ì—… ë¡œì§ ì‚¬ìš©
                    q_source = "âœ¨ Gemini AI"
                    if not query:
                        query = generate_backup_query(title)
                        q_source = "âš¡ Backup Logic"
                    
                    st.write(f"ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤í–‰: {query}")
                    news_items = fetch_news_regex(query)
                    cmts = fetch_comments_via_api(vid)
                    
                    # ë¶„ì„
                    max_match = 0
                    verified_news = []
                    for item in news_items:
                        s = calculate_match_score(item['title'], query)
                        if s > max_match: max_match = s
                        verified_news.append({'ë‰´ìŠ¤ ì œëª©': item['title'], 'ì¼ì¹˜ë„': f"{s}%"})
                    
                    # ì ìˆ˜ ê³„ì‚°
                    score = 50
                    breakdown = []
                    
                    is_silent = (len(news_items) == 0) or (max_match < 30)
                    has_critical = any(k in title for k in CRITICAL_STATE_KEYWORDS)
                    
                    news_diff = 0; news_msg = ""
                    if is_silent:
                        if has_critical: news_diff = 5; news_msg = "ë¯¸ê²€ì¦ ìœ„í—˜ ì£¼ì¥"
                        else: news_diff = 10; news_msg = "ì¦ê±° ë¶ˆì¶©ë¶„"
                    else:
                        if max_match >= 80: news_diff = -45; news_msg = "íŒ©íŠ¸ í™•ì¸ë¨"
                        elif max_match >= 40: news_diff = -20; news_msg = "ë¶€ë¶„ì  ì‚¬ì‹¤"
                        else: news_diff = 10; news_msg = "ê´€ë ¨ì„± ë‚®ìŒ"
                    breakdown.append(["ë‰´ìŠ¤ êµì°¨ ê²€ì¦", news_diff, news_msg])
                    
                    agitation = sum(title.count(w) + full_text.count(w) for w in ['ì¶©ê²©','ê²½ì•…','í­ë¡œ','ì†ë³´','ê¸´ê¸‰'])
                    if agitation > 0:
                        breakdown.append(["ìê·¹ì  í‘œí˜„", min(agitation*5, 20), f"ì„ ë™ í‚¤ì›Œë“œ {agitation}íšŒ"])
                    
                    if any(o in uploader for o in OFFICIAL_CHANNELS):
                        breakdown.append(["ê³µì‹ ì–¸ë¡ ì‚¬", -50, "ì‹ ë¢°ë„ ë³´ì¥"])
                        
                    final_score = max(5, min(99, 50 + sum(item[1] for item in breakdown)))
                    save_analysis(uploader, title, final_score, url_input, query)
                    status.update(label="ë¶„ì„ ì™„ë£Œ!", state="complete", expanded=False)
                    
                    # --- UI ì¶œë ¥ ---
                    st.subheader("ğŸ•µï¸ ë¶„ì„ ê²°ê³¼")
                    c1, c2 = st.columns([1, 2])
                    with c1:
                        st.metric("ê°€ì§œë‰´ìŠ¤ í™•ë¥ ", f"{final_score}%", delta=f"{final_score-50}")
                        st.info(f"ğŸ¯ **{q_source} ì¶”ì¶œ ê²€ìƒ‰ì–´**:\n{query}")
                        with st.expander("ì˜ìƒ ìš”ì•½ ë³´ê¸°"):
                            st.write(summarize_text_simple(full_text))
                        st.caption("ì ìˆ˜ ìƒì„¸:")
                        render_score_breakdown([["ê¸°ë³¸ ìœ„í—˜ë„", 50, "Base Score"]] + breakdown)
                        
                    with c2:
                        st.subheader("ğŸ“° íŒ©íŠ¸ì²´í¬ (ë‰´ìŠ¤ ëŒ€ì¡°)")
                        if verified_news:
                            st.table(pd.DataFrame(verified_news))
                        else:
                            st.warning("ê´€ë ¨ëœ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                            
                        st.subheader("ğŸ“Š ì •ë°€ ì§€í‘œ")
                        colored_progress_bar("ì§„ì‹¤ ê·¼ì ‘ë„", 0.8 if final_score < 40 else 0.2, "#2ecc71")
                        colored_progress_bar("ê±°ì§“ ê·¼ì ‘ë„", 0.8 if final_score > 60 else 0.2, "#e74c3c")
                        
                        if cmts:
                            st.markdown("**ğŸ’¬ ì‹œì²­ì ë°˜ì‘ (ìµœê·¼ ëŒ“ê¸€)**")
                            st.write(", ".join(cmts[:3]) + "...")

                except Exception as e:
                    st.error(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
                    st.code(traceback.format_exc())

st.divider()
st.subheader("ğŸ—‚ï¸ ë¶„ì„ ê¸°ë¡")
try:
    response = supabase.table("analysis_history").select("*").order("id", desc=True).execute()
    if response.data:
        st.dataframe(pd.DataFrame(response.data)[['video_title', 'fake_prob', 'keywords', 'analysis_date']])
except: pass
