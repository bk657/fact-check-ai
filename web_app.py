import streamlit as st
from supabase import create_client
import google.generativeai as genai
import re
import requests
import time
import json
import yt_dlp
import pandas as pd
import altair as alt
from datetime import datetime

# --- [1. ì‹œìŠ¤í…œ ì„¤ì •] ---
st.set_page_config(page_title="Fact-Check Center v60.1 (Keyword Logic Fix)", layout="wide", page_icon="âš–ï¸")

# ğŸŒŸ Secrets ë¡œë“œ
try:
    YOUTUBE_API_KEY = st.secrets["YOUTUBE_API_KEY"]
    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    ADMIN_PASSWORD = st.secrets["ADMIN_PASSWORD"]
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
except KeyError as e:
    st.error(f"âŒ í•„ìˆ˜ í‚¤ ì„¤ì • ëˆ„ë½: {e}")
    st.stop()

# ğŸŒŸ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
@st.cache_resource
def init_services():
    try:
        sb = create_client(SUPABASE_URL, SUPABASE_KEY)
        genai.configure(api_key=GOOGLE_API_KEY)
        model = genai.GenerativeModel('gemini-1.5-flash')
        return sb, model
    except Exception as e:
        st.error(f"ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return None, None

supabase, gemini_model = init_services()

# --- [2. Gemini AI ì—ì´ì „íŠ¸ (2ë‹¨ê³„ ë¡œì§)] ---
class GeminiAgent:
    def __init__(self, model):
        self.model = model

    def extract_keywords(self, title, transcript):
        """
        1ë‹¨ê³„: ë‰´ìŠ¤ ê²€ìƒ‰ì„ ìœ„í•œ 'ìµœì ì˜ í‚¤ì›Œë“œ' ì¶”ì¶œ
        """
        prompt = f"""
        ë„ˆëŠ” íŒ©íŠ¸ì²´í¬ ê²€ìƒ‰ì›ì´ì•¼. ì•„ë˜ ìœ íŠœë¸Œ ì˜ìƒ ë‚´ìš©ì„ í™•ì¸í•˜ê³ , ì‹¤ì œ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ì°¾ê¸° ìœ„í•œ 'ê²€ìƒ‰ìš© í‚¤ì›Œë“œ'ë¥¼ 1ê°œë§Œ ì¶”ì¶œí•´.
        
        [ì¡°ê±´]
        1. ìê·¹ì ì¸ í˜•ìš©ì‚¬(ì¶©ê²©, ê²½ì•… ë“±)ëŠ” ëª¨ë‘ ì œê±°í•´.
        2. 'ì¸ë¬¼ëª…'ê³¼ 'í•µì‹¬ ì‚¬ê±´(ëª…ì‚¬)' ìœ„ì£¼ë¡œ ì¡°í•©í•´.
        3. ì˜ˆì‹œ: 'ì´ì¬ìš©ì˜ ì¶©ê²©ì ì¸ ëˆˆë¬¼' -> 'ì´ì¬ìš© ëˆˆë¬¼ ì´ìœ '
        4. ì˜¤ì§ í‚¤ì›Œë“œ ë¬¸ìì—´ë§Œ ì¶œë ¥í•´. (ì„¤ëª… ê¸ˆì§€)

        ì œëª©: {title}
        ë‚´ìš©: {transcript[:1000]}
        """
        try:
            response = self.model.generate_content(prompt)
            return response.text.strip()
        except:
            return title # ì‹¤íŒ¨í•˜ë©´ ì œëª© ê·¸ëŒ€ë¡œ ì‚¬ìš©

    def analyze_content(self, title, channel, transcript, news_context, comments):
        """
        2ë‹¨ê³„: ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë¶„ì„ (JSON ì¶œë ¥)
        """
        prompt = f"""
        ë‹¹ì‹ ì€ íŒ©íŠ¸ì²´í¬ ì „ë¬¸ AIì…ë‹ˆë‹¤. ì•„ë˜ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”.

        [ë°ì´í„°]
        - ì˜ìƒ ì œëª©: {title}
        - ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼: {news_context}
        - ì˜ìƒ ìë§‰: {transcript[:10000]}
        - ëŒ“ê¸€ ì—¬ë¡ : {comments}

        [ì§€ì‹œì‚¬í•­]
        1. **fake_prob**: ë‰´ìŠ¤ ê²€ìƒ‰ ê²°ê³¼ì™€ ì˜ìƒ ì£¼ì¥ì´ ë‹¤ë¥´ë©´ ì ìˆ˜ë¥¼ ë†’ê²Œ(80~100), ì¼ì¹˜í•˜ë©´ ë‚®ê²Œ(0~30) ì±…ì •í•˜ì„¸ìš”. ë‰´ìŠ¤ê°€ ì•„ì˜ˆ ì—†ìœ¼ë©´ 'ê·¼ê±° ì—†ìŒ'ìœ¼ë¡œ ê°„ì£¼í•˜ì—¬ 60~80ì ì„ ì£¼ì„¸ìš”.
        2. **verdict**: ì ìˆ˜ì— ë”°ë¼ 'ìœ„í—˜', 'ì£¼ì˜', 'ì•ˆì „' ì¤‘ í•˜ë‚˜ ì„ íƒ.
        3. **fact_check_status**: ë‰´ìŠ¤ ê¸°ì‚¬ì™€ ëŒ€ì¡°í–ˆì„ ë•Œì˜ ê²°ê³¼ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½. (ì˜ˆ: "ê´€ë ¨ ë³´ë„ í™•ì¸ë¨", "ê·¼ê±° ì—†ëŠ” ë£¨ë¨¸")

        [ì¶œë ¥ í¬ë§· (JSON)]
        {{
            "summary": "ì˜ìƒ í•µì‹¬ ë‚´ìš© 3ì¤„ ìš”ì•½",
            "fake_prob": 0~100 ìˆ«ì,
            "verdict": "ìœ„í—˜/ì£¼ì˜/ì•ˆì „",
            "reasoning": "íŒë‹¨ ì´ìœ  (ë‰´ìŠ¤ ëŒ€ì¡° ê²°ê³¼ í¬í•¨)",
            "fact_check_status": "íŒ©íŠ¸ì²´í¬ ìƒíƒœ ìš”ì•½",
            "clickbait_score": 0~100 ìˆ«ì
        }}
        """
        try:
            response = self.model.generate_content(prompt)
            # JSON íŒŒì‹± ì•ˆì „ì¥ì¹˜
            text = response.text.replace("```json", "").replace("```", "").strip()
            return json.loads(text)
        except Exception as e:
            return {
                "summary": "AI ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "fake_prob": 50,
                "verdict": "ì˜¤ë¥˜",
                "reasoning": f"ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}",
                "fact_check_status": "ë¶„ì„ ë¶ˆê°€",
                "clickbait_score": 0
            }

gemini_agent = GeminiAgent(gemini_model)

# --- [3. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜] ---
def fetch_youtube_info(url):
    ydl_opts = {'quiet': True, 'skip_download': True}
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        try:
            info = ydl.extract_info(url, download=False)
            transcript = ""
            # ìë§‰ ì¶”ì¶œ ì‹œë„
            if 'subtitles' in info and 'ko' in info['subtitles']:
                url = next((x['url'] for x in info['subtitles']['ko'] if x['ext'] == 'vtt'), None)
                if url: transcript = requests.get(url).text
            elif 'automatic_captions' in info and 'ko' in info['automatic_captions']:
                url = next((x['url'] for x in info['automatic_captions']['ko'] if x['ext'] == 'vtt'), None)
                if url: transcript = requests.get(url).text
            
            # VTT í´ë¦¬ë‹
            clean_text = ""
            if transcript:
                lines = [line.strip() for line in transcript.splitlines() if '-->' not in line and line.strip() and not line.startswith(('WEBVTT', 'NOTE'))]
                clean_text = " ".join(list(dict.fromkeys(lines))) # ì¤‘ë³µ ì œê±°
            else:
                clean_text = info.get('description', '')

            return {
                "id": info['id'], "title": info.get('title', ''), 
                "channel": info.get('uploader', ''), "transcript": clean_text
            }
        except: return None

def fetch_google_news(query):
    try:
        # ì •í™•ë„ë¥¼ ìœ„í•´ ì¿¼ë¦¬ ì¸ì½”ë”©
        rss_url = f"https://news.google.com/rss/search?q={requests.utils.quote(query)}&hl=ko&gl=KR&ceid=KR:ko"
        res = requests.get(rss_url, timeout=5)
        items = re.findall(r'<item>(.*?)</item>', res.text, re.DOTALL)
        news_list = []
        for item in items[:5]:
            t = re.search(r'<title>(.*?)</title>', item)
            if t: news_list.append(t.group(1).replace("<![CDATA[", "").replace("]]>", ""))
        return " | ".join(news_list) if news_list else "ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬ ì—†ìŒ"
    except: return "ë‰´ìŠ¤ ê²€ìƒ‰ ì‹¤íŒ¨"

def fetch_comments(video_id):
    try:
        url = "https://www.googleapis.com/youtube/v3/commentThreads"
        params = {'part': 'snippet', 'videoId': video_id, 'key': YOUTUBE_API_KEY, 'maxResults': 10, 'order': 'relevance'}
        res = requests.get(url, params=params)
        if res.status_code == 200:
            return " | ".join([i['snippet']['topLevelComment']['snippet']['textDisplay'] for i in res.json().get('items', [])])
    except: pass
    return "ëŒ“ê¸€ ì—†ìŒ"

def save_history(data):
    try:
        supabase.table("analysis_history").insert({
            "channel_name": data['channel'], "video_title": data['title'],
            "fake_prob": data['fake_prob'], "analysis_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "video_url": f"https://youtu.be/{data['id']}", "keywords": data['fact_check_status']
        }).execute()
    except: pass

# --- [4. UI êµ¬ì„±] ---
with st.sidebar:
    st.header("ğŸ›¡ï¸ ê´€ë¦¬ì")
    if not st.session_state.get("is_admin"):
        if st.button("Login"):
            st.session_state["is_admin"] = True
            st.rerun()

st.title("âš–ï¸ Fact-Check Center v60.1")
st.caption("Gemini AI Based â€¢ Keyword Optimization Engine")

with st.container(border=True):
    url_input = st.text_input("ìœ íŠœë¸Œ URL ì…ë ¥")
    if st.button("ğŸš€ ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True):
        if url_input and gemini_model:
            with st.status("ğŸ•µï¸ AI ë¶„ì„ í”„ë¡œì„¸ìŠ¤ ê°€ë™...", expanded=True) as status:
                
                # 1. ì˜ìƒ ì •ë³´
                st.write("ğŸ“¥ ì˜ìƒ ë°ì´í„° ì¶”ì¶œ ì¤‘...")
                v_info = fetch_youtube_info(url_input)
                if not v_info:
                    st.error("ì˜ìƒ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                    st.stop()
                
                # 2. í‚¤ì›Œë“œ ì¶”ì¶œ (í•µì‹¬!)
                st.write("ğŸ§  Gemini: ë‰´ìŠ¤ ê²€ìƒ‰ìš© í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
                search_keyword = gemini_agent.extract_keywords(v_info['title'], v_info['transcript'])
                st.info(f"ğŸ‘‰ ì¶”ì¶œëœ ê²€ìƒ‰ì–´: **{search_keyword}**")
                
                # 3. ë‰´ìŠ¤ ê²€ìƒ‰
                st.write(f"ğŸ“° '{search_keyword}' ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘...")
                news_result = fetch_google_news(search_keyword)
                
                # 4. ì¢…í•© ë¶„ì„
                st.write("âš–ï¸ íŒ©íŠ¸ êµì°¨ ê²€ì¦ ë° íŒê²° ì¤‘...")
                comments = fetch_comments(v_info['id'])
                result = gemini_agent.analyze_content(
                    v_info['title'], v_info['channel'], v_info['transcript'], news_result, comments
                )
                
                # ì €ì¥
                save_data = {**v_info, **result}
                save_history(save_data)
                
                status.update(label="âœ… ë¶„ì„ ì™„ë£Œ!", state="complete", expanded=False)

            # --- ê²°ê³¼ í‘œì‹œ ---
            st.divider()
            c1, c2, c3 = st.columns(3)
            c1.metric("ê°€ì§œë‰´ìŠ¤ ìœ„í—˜ë„", f"{result['fake_prob']}%", delta="High" if result['fake_prob']>50 else "-Safe")
            c2.metric("AI íŒì •", result['verdict'])
            c3.metric("ë‚šì‹œì„± ì§€ìˆ˜", f"{result['clickbait_score']}ì ")
            
            if result['fake_prob'] > 60:
                st.error(f"ğŸš¨ **ì£¼ì˜**: {result['reasoning']}")
            else:
                st.success(f"âœ… **ì–‘í˜¸**: {result['reasoning']}")
                
            st.subheader("ğŸ“ ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸")
            st.info(f"**ê²€ì¦ ìƒíƒœ**: {result['fact_check_status']}")
            st.write(f"**ìš”ì•½**: {result['summary']}")
            
            with st.expander("ğŸ” ì°¸ì¡°ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„°"):
                st.write(news_result)
