import streamlit as st
from supabase import create_client, Client
import re
import requests
import time
import random
import math
from datetime import datetime
from collections import Counter
import yt_dlp
import pandas as pd
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup

# --- [1. ì‹œìŠ¤í…œ ì„¤ì •] ---
st.set_page_config(page_title="Fact-Check Center v47.1 (Revert)", layout="wide", page_icon="âš–ï¸")

# ğŸŒŸ Secretsì—ì„œ í‚¤ ê°€ì ¸ì˜¤ê¸°
try:
    YOUTUBE_API_KEY = st.secrets["YOUTUBE_API_KEY"]
    SUPABASE_URL = st.secrets["SUPABASE_URL"]
    SUPABASE_KEY = st.secrets["SUPABASE_KEY"]
    ADMIN_PASSWORD = st.secrets["ADMIN_PASSWORD"]
except:
    st.error("âŒ í•„ìˆ˜ í‚¤(API Key, DB Key, Password)ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    st.stop()

# ğŸŒŸ Supabase ì—°ê²°
@st.cache_resource
def init_supabase():
    return create_client(SUPABASE_URL, SUPABASE_KEY)

supabase = init_supabase()

# --- [ê´€ë¦¬ì ì¸ì¦ ë¡œì§ (Form ì‚¬ìš©)] ---
if "is_admin" not in st.session_state:
    st.session_state["is_admin"] = False

with st.sidebar:
    st.header("ğŸ›¡ï¸ ê´€ë¦¬ì ë©”ë‰´")
    
    with st.form("login_form"):
        password_input = st.text_input("ê´€ë¦¬ì ë¹„ë°€ë²ˆí˜¸", type="password")
        submit_button = st.form_submit_button("ë¡œê·¸ì¸")
        
        if submit_button:
            if password_input == ADMIN_PASSWORD:
                st.session_state["is_admin"] = True
                st.rerun()
            else:
                st.session_state["is_admin"] = False
                st.error("ë¹„ë°€ë²ˆí˜¸ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    if st.session_state["is_admin"]:
        st.success("âœ… ê´€ë¦¬ì ì¸ì¦ë¨ (ì‚­ì œ ê¶Œí•œ ë³´ìœ )")
        if st.button("ë¡œê·¸ì•„ì›ƒ"):
            st.session_state["is_admin"] = False
            st.rerun()
    else:
        st.info("ë°ì´í„° ì‚­ì œëŠ” ê´€ë¦¬ìë§Œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")

# --- [ìƒìˆ˜ ì„¤ì •] ---
WEIGHT_NEWS_DEFAULT = 45        
WEIGHT_VECTOR = 35      
WEIGHT_CONTENT = 15     
WEIGHT_SENTIMENT_DEFAULT = 10   
PENALTY_ABUSE = 20      
PENALTY_MISMATCH = 30
PENALTY_NO_FACT = 25
PENALTY_SILENT_ECHO = 40   

# í•µì‹¬ ìƒíƒœì–´ ì‚¬ì „
VITAL_KEYWORDS = [
    'ìœ„ë…', 'ì‚¬ë§', 'ë³„ì„¸', 'êµ¬ì†', 'ì²´í¬', 'ê¸°ì†Œ', 'ì‹¤í˜•', 'ì‘ê¸‰ì‹¤', 'ì“°ëŸ¬ì ¸', 
    'ì´í˜¼', 'ë¶ˆí™”', 'íŒŒê²½', 'ì¶©ê²©', 'ê²½ì•…', 'ì†ë³´', 'ê¸´ê¸‰', 'í­ë¡œ', 'ì–‘ì„±', 
    'í™•ì§„', 'ì‹¬ì •ì§€', 'ë‡Œì‚¬', 'ì¤‘íƒœ', 'ì••ìˆ˜ìˆ˜ìƒ‰', 'ì†Œí™˜', 'í‡´ì§„', 'íƒ„í•µ', 'ëª»ë„˜ê¸´ë‹¤'
]

# VIP ì¸ë¬¼ ì‚¬ì „
VIP_ENTITIES = [
    'ìœ¤ì„ì—´', 'ëŒ€í†µë ¹', 'ì´ì¬ëª…', 'í•œë™í›ˆ', 'ê¹€ê±´í¬', 'ë¬¸ì¬ì¸', 'ë°•ê·¼í˜œ', 'ì´ëª…ë°•',
    'íŠ¸ëŸ¼í”„', 'ë°”ì´ë“ ', 'í‘¸í‹´', 'ì ¤ë ŒìŠ¤í‚¤', 'ì‹œì§„í•‘', 'ì •ì€', 
    'ì´ì¤€ì„', 'ì¡°êµ­', 'ì¶”ë¯¸ì• ', 'í™ì¤€í‘œ', 'ìœ ìŠ¹ë¯¼', 'ì•ˆì² ìˆ˜',
    'ì†í¥ë¯¼', 'ì´ê°•ì¸', 'ê¹€ë¯¼ì¬', 'ë¥˜í˜„ì§„', 'ì¬ìš©', 'ì •ì˜ì„ ', 'ìµœíƒœì›'
]

OFFICIAL_CHANNELS = [
    'MBC', 'KBS', 'SBS', 'YTN', 'JTBC', 'TVCHOSUN', 'MBN', 'CHANNEL A', 'YONHAP', 
    'NEWS', 'ë‰´ìŠ¤', 'ì±„ë„A', 'TVì¡°ì„ ', 'ì—°í•©ë‰´ìŠ¤', 'í•œê²¨ë ˆ', 'ê²½í–¥', 'ì¡°ì„ ', 'ì¤‘ì•™', 'ë™ì•„'
]

STATIC_TRUTH_CORPUS = [
    "ë°•ë‚˜ë˜ ìœ„ì¥ì „ì… ì˜í˜¹ ë¬´í˜ì˜ ìˆ˜ì‚¬ ì¢…ê²° ê³µì‹ ë°œí‘œ",
    "ì„ì˜ì›… ì½˜ì„œíŠ¸ ì•”í‘œ ì†Œì†ì‚¬ ê°•ë ¥ ë²•ì  ëŒ€ì‘ ê³µì§€",
    "ì •í¬ì› êµìˆ˜ ì €ì†ë…¸í™” ìŠ¤í† í‚¹ í”¼í•´ í˜¸ì†Œ ì–¸ë¡  ë³´ë„",
    "ëŒ€ì „ ì¶©ë‚¨ í–‰ì • í†µí•© ë…¼ì˜ ì§€ìì²´ ê³µì‹ í˜‘ì˜",
    "êµ­íšŒì˜ì› ì„ ê±° ì¶œë§ˆ ê³µì‹ ì„ ì–¸ ê¸°ìíšŒê²¬",
    "ê°•í›ˆì‹ ì˜ì› ì¶©ë‚¨ì§€ì‚¬ ì¶œë§ˆì„¤ ë³´ë„"
]
STATIC_FAKE_CORPUS = [
    "ì¶©ê²© í­ë¡œ ê²½ì•… ê·¸ ì‹¤ì²´ëŠ”?",
    "ê¸´ê¸‰ ì†ë³´ ì•Œê³ ë³´ë‹ˆ ã„·ã„· ì†Œë¦„ ë‹ëŠ” ì§„ì‹¤",
    "ì´ì¬ëª… í•œë™í›ˆ ì¶©ê²© ë°œì–¸ ë…¼ë€",
    "ê²°êµ­ êµ¬ì† ì˜ì¥ ë°œë¶€ ëˆˆë¬¼ ë°”ë‹¤",
    "ë°©ì†¡ ë¶ˆê°€ íŒì • ë°›ì€ ì˜ìƒ ìœ ì¶œ",
    "ê¿ˆì† ê³„ì‹œ í•˜ë‚˜ë‹˜ ë§ì”€ ì˜ˆì–¸",
    "ì‚¬í˜• ì„ ê³  ì§‘í–‰ í™•ì •",
    "ê±´ê°• ì•…í™” ìœ„ë…ì„¤ ì‘ê¸‰ì‹¤"
]

# --- [ë²¡í„° ì—”ì§„] ---
class VectorEngine:
    def __init__(self):
        self.vocab = set()
        self.truth_vectors = []
        self.fake_vectors = []
    def tokenize(self, text):
        words = re.findall(r'[ê°€-í£]{2,}', text)
        return [w for w in words]
    def build_vocabulary(self, corpus):
        for text in corpus:
            tokens = self.tokenize(text)
            self.vocab.update(tokens)
        self.vocab = sorted(list(self.vocab))
    def text_to_vector(self, text):
        tokens = self.tokenize(text)
        token_counts = Counter(tokens)
        vector = []
        for word in self.vocab: vector.append(token_counts[word])
        return vector
    def cosine_similarity(self, vec1, vec2):
        dot_product = sum(a * b for a, b in zip(vec1, vec2))
        magnitude1 = math.sqrt(sum(a * a for a in vec1))
        magnitude2 = math.sqrt(sum(b * b for b in vec2))
        if magnitude1 == 0 or magnitude2 == 0: return 0.0
        return dot_product / (magnitude1 * magnitude2)
    def train(self, truth_corpus, fake_corpus):
        self.build_vocabulary(truth_corpus + fake_corpus)
        self.truth_vectors = [self.text_to_vector(t) for t in truth_corpus]
        self.fake_vectors = [self.text_to_vector(t) for t in fake_corpus]
    def analyze_position(self, query):
        query_vec = self.text_to_vector(query)
        max_truth_sim = 0
        for tv in self.truth_vectors:
            sim = self.cosine_similarity(query_vec, tv)
            if sim > max_truth_sim: max_truth_sim = sim
        max_fake_sim = 0
        for fv in self.fake_vectors:
            sim = self.cosine_similarity(query_vec, fv)
            if sim > max_fake_sim: max_fake_sim = sim
        return max_truth_sim, max_fake_sim

vector_engine = VectorEngine()

# --- [DB í•¨ìˆ˜] ---
def save_analysis(channel, title, prob, url, keywords):
    now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    data = {
        "channel_name": channel,
        "video_title": title,
        "fake_prob": prob,
        "analysis_date": now,
        "video_url": url,
        "keywords": keywords
    }
    try:
        supabase.table("analysis_history").insert(data).execute()
    except: pass

def train_dynamic_vector_engine():
    try:
        response_truth = supabase.table("analysis_history").select("video_title").lt("fake_prob", 30).execute()
        dynamic_truth = [row['video_title'] for row in response_truth.data]
        
        response_fake = supabase.table("analysis_history").select("video_title").gt("fake_prob", 70).execute()
        dynamic_fake = [row['video_title'] for row in response_fake.data]
    except:
        dynamic_truth, dynamic_fake = [], []
    
    final_truth = STATIC_TRUTH_CORPUS + dynamic_truth
    final_fake = STATIC_FAKE_CORPUS + dynamic_fake
    
    vector_engine.train(final_truth, final_fake)
    return len(final_truth) + len(final_fake)

# --- [UI Helper Functions] ---
def colored_progress_bar(label, percent, color):
    st.markdown(f"""
        <div style="margin-bottom: 10px;">
            <div style="display: flex; justify-content: space-between; margin-bottom: 3px;">
                <span style="font-size: 13px; font-weight: 600; color: #555;">{label}</span>
                <span style="font-size: 13px; font-weight: 700; color: {color};">{round(percent * 100, 1)}%</span>
            </div>
            <div style="background-color: #eee; border-radius: 5px; height: 8px; width: 100%;">
                <div style="background-color: {color}; height: 8px; width: {percent * 100}%; border-radius: 5px;"></div>
            </div>
        </div>
        """, unsafe_allow_html=True)

def render_score_breakdown(data_list):
    style = """
    <style>
        table.score-table { width: 100%; border-collapse: separate; border-spacing: 0; border: 1px solid #e0e0e0; border-radius: 8px; overflow: hidden; font-family: sans-serif; font-size: 14px; margin-top: 10px;}
        table.score-table th { background-color: #f8f9fa; color: #495057; font-weight: bold; padding: 12px 15px; text-align: left; border-bottom: 1px solid #e0e0e0; }
        table.score-table td { padding: 12px 15px; border-bottom: 1px solid #f0f0f0; color: #333; }
        table.score-table tr:last-child td { border-bottom: none; }
        .badge { padding: 4px 8px; border-radius: 6px; font-weight: 700; font-size: 11px; display: inline-block; text-align: center; min-width: 45px; }
        .badge-danger { background-color: #ffebee; color: #d32f2f; }
        .badge-success { background-color: #e8f5e9; color: #2e7d32; }
        .badge-neutral { background-color: #f5f5f5; color: #757575; border: 1px solid #e0e0e0; }
    </style>
    """
    rows = ""
    for item, score, note in data_list:
        try:
            score_num = int(score)
            if score_num > 0: badge = f'<span class="badge badge-danger">+{score_num}</span>'
            elif score_num < 0: badge = f'<span class="badge badge-success">{score_num}</span>'
            else: badge = f'<span class="badge badge-neutral">0</span>'
        except: badge = f'<span class="badge badge-neutral">{score}</span>'
        rows += f"<tr><td>{item}<br><span style='color:#888; font-size:11px;'>{note}</span></td><td style='text-align: right;'>{badge}</td></tr>"

    st.markdown(f"{style}<table class='score-table'><thead><tr><th>ë¶„ì„ í•­ëª© (Silent Echo Protocol)</th><th style='text-align: right;'>ë³€ë™</th></tr></thead><tbody>{rows}</tbody></table>", unsafe_allow_html=True)

def witty_loading_sequence(count):
    messages = [
        f"ğŸ§  [Intelligence Level: {count}] ëˆ„ì  ì§€ì‹ ë¡œë“œ ì¤‘...",
        "ğŸ”„ 'ì£¼ì–´(Modifier)' + 'í•µì‹¬ì–´(Head)' ì—­ë°©í–¥ ê²°í•©(Back-Merge) ì¤‘...",
        "ğŸ¯ ë¬¸ë§¥ì„ í†µí•©í•˜ì—¬ ì™„ë²½í•œ ê²€ìƒ‰ì–´(Contextual Query) ìƒì„±...",
        "ğŸš€ ìœ„ì„±ì´ ìœ íŠœë¸Œ ë³¸ì‚¬ ìƒê³µì„ ì§€ë‚˜ê°€ëŠ” ì¤‘..."
    ]
    with st.status("ğŸ•µï¸ Context Merger v46.0 ê°€ë™ ì¤‘...", expanded=True) as status:
        for msg in messages:
            st.write(msg)
            time.sleep(0.4)
        st.write("âœ… ë¶„ì„ ì¤€ë¹„ ì™„ë£Œ!")
        status.update(label="ë¶„ì„ ì™„ë£Œ!", state="complete", expanded=False)

# --- [Logic Functions] ---
def extract_nouns(text):
    noise = ['ì¶©ê²©', 'ê²½ì•…', 'ì‹¤ì²´', 'ë‚œë¦¬', 'ê³µê°œ', 'ë°˜ì‘', 'ëª…ë‹¨', 'ë™ì˜ìƒ', 'ì‚¬ì§„', 'ì§‘ì•ˆ', 'ì†ë³´', 'ë‹¨ë…', 'ê²°êµ­', 'MBC', 'ë‰´ìŠ¤', 'ì´ë¯¸ì§€', 'ë„ˆë¬´', 'ë‹¤ë¥¸', 'ì•Œê³ ë³´ë‹ˆ', 'ã„·ã„·', 'ì§„ì§œ', 'ì •ë§', 'ì˜ìƒ', 'ì‚¬ëŒ', 'ìƒê°', 'ì˜¤ëŠ˜ë°¤', 'ì˜¤ëŠ˜', 'ë‚´ì¼', 'ì§€ê¸ˆ', 'ëª»ë„˜ê¸´ë‹¤', 'ë„˜ê¸´ë‹¤', 'ì´ìœ ', 'ì™œ', 'ì•ˆ']
    nouns = re.findall(r'[ê°€-í£]{2,}', text)
    return list(dict.fromkeys([n for n in nouns if n not in noise]))

# ğŸŒŸ í•€í¬ì¸íŠ¸ ì¿¼ë¦¬ ìƒì„± (Chunking + SOV Back-Merge)
def generate_pinpoint_query(title, hashtags):
    clean_text = title + " " + " ".join([h.replace("#", "") for h in hashtags])
    words = clean_text.split()
    
    subject_chunk = ""
    object_word = ""
    vital_word = ""
    
    for vital in VITAL_KEYWORDS:
        if vital in clean_text:
            vital_word = vital
            break
            
    for i, word in enumerate(words):
        match = re.match(r'([ê°€-í£A-Za-z0-9]+)(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì—|ì—ê²Œ|ë¡œì„œ|ë¡œ)', word)
        
        if match:
            noun = match.group(1)
            josa = match.group(2)
            
            if noun in ['ì˜¤ëŠ˜ë°¤', 'ì§€ê¸ˆ', 'ì´ìœ ', 'ê²°êµ­']: continue

            # ì£¼ì–´ ì°¾ê¸° + ì—­ë°©í–¥ ê²°í•©
            if not subject_chunk and josa in ['ì€', 'ëŠ”', 'ì´', 'ê°€']:
                prev_noun = ""
                if i > 0:
                    prev_word = words[i-1]
                    if re.fullmatch(r'[ê°€-í£A-Za-z0-9]+', prev_word):
                        if prev_word not in VITAL_KEYWORDS and prev_word not in ['ì¶©ê²©', 'ì†ë³´']:
                            prev_noun = prev_word
                
                if prev_noun:
                    subject_chunk = f"{prev_noun} {noun}"
                else:
                    subject_chunk = noun
            
            # ëª©ì ì–´ ì°¾ê¸°
            elif not object_word and josa in ['ì„', 'ë¥¼', 'ì—', 'ì—ê²Œ', 'ë¡œ']:
                if noun not in VITAL_KEYWORDS and noun not in subject_chunk:
                    object_word = noun
    
    if not subject_chunk:
        nouns = extract_nouns(title)
        return " ".join(nouns[:3])
    
    query_parts = []
    if subject_chunk: query_parts.append(subject_chunk)
    if object_word: query_parts.append(object_word)
    if vital_word: query_parts.append(vital_word)
    
    return " ".join(query_parts)

def summarize_transcript(text, max_sentences=3):
    if not text or len(text) < 50:
        return "âš ï¸ ìš”ì•½í•  ìë§‰ ë‚´ìš©ì´ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    sentences = re.split(r'(?<=[.?!])\s+', text)
    if len(sentences) <= max_sentences: return text
    nouns = re.findall(r'[ê°€-í£]{2,}', text)
    word_freq = Counter(nouns)
    ranked_sentences = []
    for i, sent in enumerate(sentences):
        sent_nouns = re.findall(r'[ê°€-í£]{2,}', sent)
        if not sent_nouns: continue
        score = sum(word_freq[w] for w in sent_nouns)
        if 10 < len(sent) < 150:
            ranked_sentences.append((i, sent, score / len(sent_nouns)))
    top_sentences = sorted(ranked_sentences, key=lambda x: x[2], reverse=True)[:max_sentences]
    top_sentences.sort(key=lambda x: x[0])
    summary = " ".join([s[1] for s in top_sentences])
    return f"ğŸ“Œ **í•µì‹¬ ìš”ì•½**: {summary}"

def clean_html(raw_html):
    soup = BeautifulSoup(raw_html, "html.parser")
    return soup.get_text()

def detect_ai_content(info):
    is_ai = False
    reasons = []
    ai_keywords = ['ai', 'artificial intelligence', 'chatgpt', 'midjourney', 'sora', 'deepfake', 'synthetic', 'ì¸ê³µì§€ëŠ¥', 'ë”¥í˜ì´í¬', 'ê°€ìƒì¸ê°„', 'ë²„ì¶”ì–¼', 'gpt']
    text_to_check = (info.get('title', '') + " " + info.get('description', '') + " " + " ".join(info.get('tags', []))).lower()
    for kw in ai_keywords:
        if re.search(r'\b{}\b'.format(re.escape(kw)), text_to_check):
            is_ai = True
            reasons.append(f"í‚¤ì›Œë“œ ê°ì§€: {kw}")
            break
    return is_ai, ", ".join(reasons)

def check_is_official(channel_name):
    norm_name = channel_name.upper().replace(" ", "")
    for official in OFFICIAL_CHANNELS:
        if official in norm_name: return True
    return False

def extract_nouns_list(text):
    noise = ['ì¶©ê²©', 'ê²½ì•…', 'ì‹¤ì²´', 'ë‚œë¦¬', 'ê³µê°œ', 'ë°˜ì‘', 'ëª…ë‹¨', 'ë™ì˜ìƒ', 'ì‚¬ì§„', 'ì§‘ì•ˆ', 'ì†ë³´', 'ë‹¨ë…', 'ê²°êµ­', 'MBC', 'ë‰´ìŠ¤', 'ì´ë¯¸ì§€', 'ë„ˆë¬´', 'ë‹¤ë¥¸', 'ì•Œê³ ë³´ë‹ˆ', 'ã„·ã„·', 'ì§„ì§œ', 'ì •ë§', 'ì˜ìƒ', 'ì‚¬ëŒ', 'ìƒê°', 'ìµœê³ ', 'ì‘ì›', 'í™”ì´íŒ…', 'ì‚¬ë‘']
    nouns = re.findall(r'[ê°€-í£]{2,}', text)
    return [n for n in nouns if n not in noise]

def count_sensational_words(text):
    triggers = ['ì¶©ê²©', 'ê²½ì•…', 'ì‹¤ì²´', 'í­ë¡œ', 'ë‚œë¦¬', 'ì†ë³´', 'ê¸´ê¸‰', 'ì†Œë¦„', 'ã„·ã„·', 'ì§„ì§œ', 'ê²°êµ­', 'ê³„ì‹œ', 'ì˜ˆì–¸', 'ìœ„ë…', 'ì‚¬ë§', 'ì¤‘íƒœ']
    count = 0
    for w in triggers: count += text.count(w)
    return count

def check_tag_abuse(title, hashtags, channel_name):
    is_official = check_is_official(channel_name)
    if is_official: return 0, "ê³µì‹ ì±„ë„ ë©´ì œ"
    if not hashtags: return 0, "í•´ì‹œíƒœê·¸ ì—†ìŒ"
    title_nouns = extract_nouns(title)
    tag_nouns = set()
    for t in hashtags:
        val = t.replace("#", "").split(":")[-1].strip()
        tag_nouns.add(val)
    if len(tag_nouns) < 2: return 0, "ì–‘í˜¸"
    # ğŸŒŸ [Fix] set ë³€í™˜
    intersection = set(title_nouns).intersection(tag_nouns)
    if not intersection: return PENALTY_ABUSE, "ğŸš¨ ì‹¬ê° (ë¶ˆì¼ì¹˜)"
    return 0, "ì–‘í˜¸"

def fetch_real_transcript(info_dict):
    sub_url = None
    if 'subtitles' in info_dict and 'ko' in info_dict['subtitles']:
        for fmt in info_dict['subtitles']['ko']:
            if fmt['ext'] == 'vtt': sub_url = fmt['url']; break
    if not sub_url and 'automatic_captions' in info_dict and 'ko' in info_dict['automatic_captions']:
        for fmt in info_dict['automatic_captions']['ko']:
            if fmt['ext'] == 'vtt': sub_url = fmt['url']; break
    if not sub_url: return None, "ìë§‰ ì—†ìŒ (ì„¤ëª…ë€ ëŒ€ì²´)"
    try:
        response = requests.get(sub_url)
        if response.status_code == 200:
            lines = response.text.splitlines()
            clean_lines = []
            seen = set()
            for line in lines:
                line = line.strip()
                if '-->' in line or line == 'WEBVTT' or not line: continue
                line = re.sub(r'<[^>]+>', '', line)
                if line and line not in seen:
                    clean_lines.append(line)
                    seen.add(line)
            return " ".join(clean_lines), "âœ… ì‹¤ì œ ìë§‰ ìˆ˜ì§‘ ì„±ê³µ"
    except: pass
    return None, "ìë§‰ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨"

def fetch_comments_via_api(video_id):
    url = "https://www.googleapis.com/youtube/v3/commentThreads"
    params = {'part': 'snippet', 'videoId': video_id, 'key': YOUTUBE_API_KEY, 'maxResults': 100, 'order': 'relevance'}
    try:
        res = requests.get(url, params=params)
        if res.status_code == 200:
            data = res.json()
            items = data.get('items', [])
            comment_data = []
            for item in items:
                snippet = item['snippet']['topLevelComment']['snippet']
                text = snippet['textDisplay']
                likes = snippet.get('likeCount', 0)
                comment_data.append({'text': text, 'likes': likes})
            comment_data.sort(key=lambda x: x['likes'], reverse=True)
            top_comments = [c['text'] for c in comment_data[:50]]
            return top_comments, f"âœ… API ìˆ˜ì§‘ ì„±ê³µ (Top {len(top_comments)} by Likes)"
        elif res.status_code == 403: return [], "âš ï¸ API ê¶Œí•œ ì˜¤ë¥˜"
        elif res.status_code == 404: return [], "âš ï¸ ëŒ“ê¸€ ì‚¬ìš© ì¤‘ì§€ë¨"
        else: return [], f"âš ï¸ API ì˜¤ë¥˜ ({res.status_code})"
    except Exception as e: return [], f"âŒ API í†µì‹  ì‹¤íŒ¨"

def calculate_dual_match(news_item, query_nouns, transcript):
    if not news_item or not transcript: return 0, 0, 0
    news_title = news_item.get('title', '')
    title_nouns = extract_nouns(news_title)
    
    # ğŸŒŸ [Fix] set ë³€í™˜
    intersection = len(set(query_nouns).intersection(set(title_nouns)))
    title_score = 1.0 if intersection >= 2 else (0.5 if intersection == 1 else 0)
    
    news_desc = news_item.get('desc', '')
    desc_nouns = extract_nouns(news_desc)
    
    if not desc_nouns:
        content_score = 0
    else:
        match_count = 0
        for noun in desc_nouns:
            if noun in transcript:
                match_count += 1
        content_ratio = match_count / len(desc_nouns)
        content_score = 1.0 if content_ratio >= 0.3 else (0.5 if content_ratio >= 0.15 else 0)
    total_score = (title_score * 0.3) + (content_score * 0.7)
    return int(total_score * 100), int(title_score * 100), int(content_score * 100)

def analyze_comment_relevance(comments, context_text):
    if not comments: return [], 0, "ë¶„ì„ ë¶ˆê°€"
    all_comments_text = " ".join(comments)
    comment_nouns = extract_nouns_list(all_comments_text)
    if not comment_nouns: return [], 0, "ìœ íš¨ í‚¤ì›Œë“œ ì—†ìŒ"
    top_keywords = Counter(comment_nouns).most_common(5)
    top_words_only = [word for word, cnt in top_keywords]
    context_nouns = extract_nouns(context_text)
    match_count = 0
    context_set = set(context_nouns)
    for word in top_words_only:
        if word in context_set: match_count += 1
    relevance_score = int((match_count / len(top_keywords)) * 100)
    if relevance_score >= 60: relevance_msg = "âœ… ì£¼ì œ ì§‘ì¤‘ í† ë¡ í˜•"
    elif relevance_score >= 20: relevance_msg = "âš ï¸ ì¼ë¶€ ê´€ë ¨ / ì¡ë‹´ í˜¼ì¬"
    else: relevance_msg = "âŒ ë¬´ê´€í•œ ë”´ì†Œë¦¬ / ë§¹ëª©ì  ì§€ì§€"
    formatted_keywords = [f"{w}({c})" for w, c in top_keywords]
    return formatted_keywords, relevance_score, relevance_msg

def check_red_flags(comments):
    red_flag_keywords = ['ê°€ì§œë‰´ìŠ¤', 'ê°€ì§œ ë‰´ìŠ¤', 'ì£¼ì‘', 'ì‚¬ê¸°', 'ê±°ì§“ë§', 'í—ˆìœ„', 'êµ¬ë¼', 'í•©ì„±', 'ì„ ë™', 'ì†Œì„¤']
    count = 0
    detected = []
    for c in comments:
        for k in red_flag_keywords:
            if k in c:
                count += 1
                detected.append(k)
    return count, list(set(detected))

# --- [8. ì‹¤í–‰ë¶€] ---
def run_forensic_main(url):
    total_intelligence = train_dynamic_vector_engine()
    witty_loading_sequence(total_intelligence)
    
    video_id = None
    match = re.search(r'(?:v=|\/)([0-9A-Za-z_-]{11}).*', url)
    if match: video_id = match.group(1)

    ydl_opts = {
        'quiet': True, 'skip_download': True, 'get_comments': False,
        'writesubtitles': True, 'writeautomaticsub': True,
        'subtitleslangs': ['ko'],
        'extractor_args': {'youtube': {'skip': ['dash', 'hls']}}
    }
    
    with yt_dlp.YoutubeDL(ydl_opts) as ydl:
        try:
            info = ydl.extract_info(url, download=False)
            title = info.get('title', 'ì œëª© ì—†ìŒ')
            uploader = info.get('uploader', 'ì•Œ ìˆ˜ ì—†ìŒ')
            all_hashtags = info.get('tags', [])
            description = info.get('description', '')
            
            is_official = check_is_official(uploader)
            is_ai_content, ai_reason = detect_ai_content(info)
            
            current_weight_news = WEIGHT_NEWS_DEFAULT
            current_weight_vector = WEIGHT_VECTOR
            current_weight_sentiment = WEIGHT_SENTIMENT_DEFAULT
            
            if is_ai_content:
                current_weight_news = 70  
                current_weight_vector = 10 
            
            refined_query = generate_pinpoint_query(title, all_hashtags)
            hashtag_display = ", ".join([f"#{t}" for t in all_hashtags]) if all_hashtags else "í•´ì‹œíƒœê·¸ ì—†ìŒ"
            abuse_score, abuse_status = check_tag_abuse(title, all_hashtags, uploader)
            
            transcript_text, transcript_status = fetch_real_transcript(info)
            analysis_text = transcript_text if transcript_text else description
            
            summary_text = summarize_transcript(analysis_text)
            
            agitation_count = count_sensational_words(analysis_text + title)
            agitation_level = "ë†’ìŒ (ìœ„í—˜)" if agitation_count > 3 else "ë³´í†µ" if agitation_count > 0 else "ë‚®ìŒ (ì•ˆì „)"
            
            t_sim, f_sim = vector_engine.analyze_position(refined_query + " " + title)
            t_impact = int(t_sim * current_weight_vector) * -1 
            f_impact = int(f_sim * current_weight_vector)

            max_news_sim, news_ev, news_collected_cnt, news_used_cnt = 0, [], 0, 0
            search_q = refined_query 
            max_dual_score = 0
            best_veracity_display = "0%"
            best_match_content_score = 0
            
            try:
                rss_url = f"https://news.google.com/rss/search?q={requests.utils.quote(search_q)}&hl=ko&gl=KR"
                r = requests.get(rss_url, timeout=5)
                root = ET.fromstring(r.content)
                items = root.findall('.//item')
                
                if not items:
                    fallback_q = " ".join(search_q.split()[:2])
                    rss_url = f"https://news.google.com/rss/search?q={requests.utils.quote(fallback_q)}&hl=ko&gl=KR"
                    r = requests.get(rss_url, timeout=5)
                    root = ET.fromstring(r.content)
                    items = root.findall('.//item')
                    if items: search_q = fallback_q
                
                news_collected_cnt = len(items)
                news_used_cnt = min(len(items), 3)
                
                for i, item in enumerate(items[:3]):
                    try:
                        nt = item.find('title').text
                        raw_desc = item.find('description').text if item.find('description') is not None else ""
                        clean_desc = clean_html(raw_desc)
                        news_item_dict = {'title': nt, 'desc': clean_desc}
                        query_nouns = extract_nouns(search_q)
                        
                        total, t_sc, c_sc = calculate_dual_match(news_item_dict, query_nouns, analysis_text)
                        
                        if total > max_dual_score: 
                            max_dual_score = total
                            best_veracity_display = f"{c_sc}% (Content Match)"
                            best_match_content_score = c_sc
                        
                        news_ev.append({"ë‰´ìŠ¤ ì œëª©": nt, "ìµœì¢… ì¼ì¹˜ë„": f"{total}%", "ìƒì„¸": f"(ì œëª©:{t_sc}%, ë‚´ìš©:{c_sc}%)"})
                    except: continue
            except: pass
            
            total_text_len = len(analysis_text.split())
            analyzed_nouns = len(extract_nouns(analysis_text))
            info_density = round((analyzed_nouns / total_text_len) * 100, 1) if total_text_len > 0 else 0
            
            comments_list, comments_status = fetch_comments_via_api(video_id)
            cmts_collected_cnt = len(comments_list)
            used_comments = comments_list
            top_keywords, relevance_score, relevance_msg = analyze_comment_relevance(used_comments, title + " " + analysis_text)
            
            red_flag_count, red_flag_words = check_red_flags(used_comments)
            is_controversial = False
            
            if red_flag_count > 0:
                is_controversial = True
                current_weight_news = 65
                current_weight_sentiment = 0
            
            final_sim_ratio = max_dual_score / 100.0
            adjusted_ratio = math.pow(final_sim_ratio, 2)
            
            silent_echo_penalty = 0
            is_effective_silence = (news_collected_cnt == 0) or (news_collected_cnt > 0 and max_dual_score < 20)
            
            if is_effective_silence:
                if agitation_count >= 3: 
                    silent_echo_penalty = PENALTY_SILENT_ECHO 
                    t_impact *= 2
                    f_impact *= 2
                news_safety_score = 0
                news_note = "No Relevant News (Effective Silent Echo)"
                
            elif is_controversial:
                if max_dual_score < 60:
                    news_safety_score = PENALTY_NO_FACT 
                    news_note = "Penalty: Unverified despite Controversy"
                else:
                    news_safety_score = int(adjusted_ratio * current_weight_news) * -1
                    news_note = f"Max -{current_weight_news} (Verified Conflict)"
            else:
                news_safety_score = int(adjusted_ratio * current_weight_news) * -1
                if 0 < best_match_content_score < 70:
                    news_safety_score = int(news_safety_score * 0.5)
                news_note = f"Max -{current_weight_news} (Standard)"

            is_misleading = (news_collected_cnt > 0) and (max_dual_score < 20)
            mismatch_penalty = 0
            if is_official:
                is_misleading = False
                news_safety_score = -50
                mismatch_penalty = 0
                news_note = "Official Channel Bonus"
            elif is_misleading:
                news_safety_score = 0
                mismatch_penalty = PENALTY_MISMATCH 
                news_note = "Score Voided (Mismatch)"

            sentiment_score = 0
            if cmts_collected_cnt > 0 and not is_controversial:
                s_counts = Counter([s for c in used_comments for s in ['ê°€ì§œ', 'ë‚šì‹œ', 'ì¡°ì‘', 'ì„ ë™'] if s in c])
                neg_ratio = min(1.0, sum(s_counts.values()) / len(used_comments)) if used_comments else 0
                sentiment_score = int(neg_ratio * current_weight_sentiment)

            clickbait_words = ['ì¶©ê²©', 'ê²½ì•…', 'ì‹¤ì²´', 'ë‚œë¦¬', 'ê²°êµ­', 'í­ë¡œ']
            clickbait_score = 10 if any(w in title for w in clickbait_words) else -5
            
            base_score = 50
            total_score = base_score + t_impact + f_impact + news_safety_score + sentiment_score + clickbait_score + abuse_score + mismatch_penalty + silent_echo_penalty
            final_prob = max(5, min(99, total_score))
            
            save_analysis(uploader, title, final_prob, url, refined_query)

            # --- UI ---
            st.subheader("ğŸ•µï¸ í•µì‹¬ ë¶„ì„ ì§€í‘œ (Key Indicators)")
            col_a, col_b, col_c = st.columns(3)
            with col_a:
                st.metric("ìµœì¢… ê°€ì§œë‰´ìŠ¤ í™•ë¥ ", f"{final_prob}%", delta=f"{total_score - base_score}")
            with col_b:
                icon = "ğŸŸ¢" if final_prob < 30 else "ğŸ”´" if final_prob > 60 else "ğŸŸ "
                verdict = "ë§¤ìš° ì•ˆì „" if final_prob < 30 else "ìœ„í—˜ ê°ì§€" if final_prob > 60 else "ì£¼ì˜ ìš”ë§"
                st.metric("ì¢…í•© AI íŒì •", f"{icon} {verdict}")
            with col_c:
                st.metric("AI Intelligence Level", f"{total_intelligence} Knowledge Nodes", delta="+1 Added")

            if is_ai_content:
                st.warning(f"ğŸ¤– **AI ìƒì„± ì½˜í…ì¸  ê°ì§€ë¨**: {ai_reason}")
            if is_official:
                st.success(f"ğŸ›¡ï¸ **ê³µì‹ ì–¸ë¡ ì‚¬ ì±„ë„({uploader})ì…ë‹ˆë‹¤.**")
            
            if silent_echo_penalty > 0:
                st.error(f"ğŸ”‡ **ì¹¨ë¬µì˜ ë©”ì•„ë¦¬(Silent Echo) ê²½ê³ **: ìê·¹ì ì¸ ì£¼ì¥ì´ë‚˜ ì˜í˜¹ì„ ì œê¸°í•˜ê³  ìˆìœ¼ë‚˜, ì´ë¥¼ ë’·ë°›ì¹¨í•  ì œë„ê¶Œ ì–¸ë¡  ë³´ë„ê°€ ì „ë¬´í•©ë‹ˆë‹¤. ê°€ì§œë‰´ìŠ¤ì¼ í™•ë¥ ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤.")

            st.divider()
            col1, col2 = st.columns([1, 1.4])
            with col1:
                st.write("**[ì˜ìƒ ìƒì„¸ ì •ë³´]**")
                meta_df = pd.DataFrame({
                    "í•­ëª©": ["ì˜ìƒ ì œëª©", "ì±„ë„ëª…", "ì¹´í…Œê³ ë¦¬", "íƒ€ì…", "ì—…ë¡œë“œì¼", "ì¡°íšŒìˆ˜", "í•´ì‹œíƒœê·¸"],
                    "ë‚´ìš©": [title, uploader, info.get('categories', ['ë¯¸ë¶„ë¥˜'])[0], "ì‡¼ì¸ " if "shorts" in url else "ì¼ë°˜", info.get('upload_date', 'N/A'), f"{info.get('view_count', 0):,}íšŒ", hashtag_display]
                })
                st.table(meta_df)
                
                st.info(f"ğŸ¯ **í•€í¬ì¸íŠ¸ ë‰´ìŠ¤ ê²€ìƒ‰ì–´**: {search_q}")
                
                with st.container(border=True):
                    st.markdown("ğŸ“ **ì˜ìƒ ë‚´ìš© ìš”ì•½ (AI Abstract)**")
                    st.caption("ìë§‰ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ í•µì‹¬ ë¬¸ì¥ 3ê°œë¥¼ ì¶”ì¶œí•œ ê²°ê³¼ì…ë‹ˆë‹¤.")
                    st.write(summary_text)
                
                st.write("**[Score Breakdown]**")
                score_data = [
                    ["ê¸°ë³¸ ìœ„í—˜ë„", 50, "Base Score"],
                    ["ì§„ì‹¤ ë§¥ë½ ë³´ë„ˆìŠ¤ (ë²¡í„°)", t_impact, f"Dynamic Weight: x{2 if silent_echo_penalty else 1}"],
                    ["ê°€ì§œ íŒ¨í„´ ê°€ì  (ë²¡í„°)", f_impact, f"Dynamic Weight: x{2 if silent_echo_penalty else 1}"],
                    ["ë‰´ìŠ¤ êµì°¨ ëŒ€ì¡° (Dual)", news_safety_score, news_note],
                    ["ì¹¨ë¬µì˜ ë©”ì•„ë¦¬ (No News)", f"+{silent_echo_penalty}" if silent_echo_penalty else "0", "Penalty for No/Irrelevant News"],
                    ["ì—¬ë¡ /ì œëª©/ìë§‰ ê°€ê°", sentiment_score + clickbait_score, f"Sent: {sentiment_score}"],
                    ["ë‚´ìš© ë¶ˆì¼ì¹˜ ê¸°ë§Œ", mismatch_penalty, f"Penalty +{PENALTY_MISMATCH} (Title Baiting)"],
                    ["í•´ì‹œíƒœê·¸ ì–´ë·°ì§•", f"+{abuse_score}" if abuse_score > 0 else "0 (ë©´ì œ/ì •ìƒ)", f"Penalty +{PENALTY_ABUSE}"]
                ]
                render_score_breakdown(score_data)

            with col2:
                st.subheader("ğŸ“Š 5ëŒ€ ì •ë°€ ë¶„ì„ ì¦ê±°")
                
                st.markdown("**[ì¦ê±° 0] Semantic Vector Space (ì§„ì‹¤/ê±°ì§“ ë¶„í¬)**")
                st.caption(f"ğŸ’¡ Intelligence Level {total_intelligence} ê¸°ë°˜ ë¶„ì„")
                colored_progress_bar("âœ… ì§„ì‹¤ ì˜ì—­ ê·¼ì ‘ë„", t_sim, "#2ecc71")
                colored_progress_bar("ğŸš¨ ê±°ì§“ ì˜ì—­ ê·¼ì ‘ë„", f_sim, "#e74c3c")
                
                st.write("---")
                
                st.markdown(f"**[ì¦ê±° 1] ë‰´ìŠ¤ êµì°¨ ëŒ€ì¡° (Query: {search_q})**")
                st.caption(f"ğŸ“¡ ìˆ˜ì§‘: **{news_collected_cnt}ê±´** | ğŸ§ª ë¶„ì„: **ìƒìœ„ {news_used_cnt}ê±´**")
                if news_ev: st.table(pd.DataFrame(news_ev))
                else: st.warning("ğŸ” ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (Silent Echo Risk Increased)")
                
                st.markdown("**[ì¦ê±° 2] ì‹œì²­ì ì—¬ë¡  ì‹¬ì¸µ ë¶„ì„**")
                st.caption(f"ğŸ’¬ ìƒíƒœ: **{comments_status}**")
                if cmts_collected_cnt > 0:
                    opinion_df = pd.DataFrame([
                        ["ìµœë‹¤ ë¹ˆì¶œ í‚¤ì›Œë“œ", ", ".join(top_keywords)],
                        ["ë…¼ë€ ê°ì§€ ì—¬ë¶€", f"{'âš ï¸ ê°ì§€ë¨' if is_controversial else 'âœ… ì•ˆì •ì '} ({red_flag_count}íšŒ)"],
                        ["ì£¼ì œ ì¼ì¹˜ë„", f"{relevance_score}% ({relevance_msg})"]
                    ], columns=["í•­ëª©", "ë‚´ìš©"])
                    st.table(opinion_df)
                else: st.warning("âš ï¸ ëŒ“ê¸€ ìˆ˜ì§‘ ë¶ˆê°€.")
                
                st.markdown("**[ì¦ê±° 3] ìë§‰ ì„¸ë§Œí‹± ì‹¬ì¸µ ëŒ€ì¡°**")
                st.caption(f"ğŸ“ **{transcript_status}** | ğŸ“š ì „ì²´ ë‹¨ì–´: **{total_text_len}ê°œ**")
                semantic_df = pd.DataFrame([
                    ["ì œëª© ë‚šì‹œì–´", f"{', '.join([w for w in clickbait_words if w in title]) if any(w in title for w in clickbait_words) else 'ì—†ìŒ'}"],
                    ["ì •ë³´ ë°€ë„ (ëª…ì‚¬/ì „ì²´)", f"{info_density}% ({'ë†’ìŒ' if info_density > 20 else 'ë‚®ìŒ'})"],
                    ["ì„ ë™ì„± ì§€ìˆ˜", f"{agitation_level} ({agitation_count}íšŒ)"],
                    ["ê¸°ì‚¬-ì˜ìƒ ì¼ì¹˜ë„", f"{max_dual_score}% (ì¢…í•©) / {best_veracity_display}"]
                ], columns=["ë¶„ì„ í•­ëª©", "íŒì • ê²°ê³¼"])
                st.table(semantic_df)
                
                st.markdown("**[ì¦ê±° 4] AI ìµœì¢… ë¶„ì„ íŒë‹¨**")
                result_text = f"í˜„ì¬ ë¶„ì„ëœ ì¢…í•© ì ìˆ˜ëŠ” {final_prob}ì ì…ë‹ˆë‹¤. "
                if is_official:
                    result_text += "ğŸ›¡ï¸ **ê³µì‹ ì–¸ë¡ ì‚¬ ì±„ë„**ë¡œ í™•ì¸ë˜ì–´ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì •ë³´ì…ë‹ˆë‹¤. "
                elif silent_echo_penalty > 0:
                    result_text += "ğŸ”‡ **ìê·¹ì  ì£¼ì¥ì„ ë’·ë°›ì¹¨í•  ì–¸ë¡  ë³´ë„ê°€ ì—†ê±°ë‚˜ ê´€ë ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤(Silent Echo).** ê°€ì§œë‰´ìŠ¤ì¼ í™•ë¥ ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤. "
                elif is_controversial and max_dual_score < 60:
                    result_text += "ğŸš¨ **ê²½ê³ : ì˜ìƒì— ëŒ€í•œ ë…¼ë€(ê°€ì§œë‰´ìŠ¤ ì˜í˜¹)ì´ ìˆìœ¼ë‚˜, ì´ë¥¼ ë’·ë°›ì¹¨í•  ëª…í™•í•œ ë‰´ìŠ¤ ë³´ë„ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤(Fact Deficit).** ìœ„í—˜ë„ê°€ ìƒí–¥ ì¡°ì •ë˜ì—ˆìŠµë‹ˆë‹¤. "
                elif is_misleading:
                    result_text += "ğŸš¨ **ê²½ê³ : ì œëª©ê³¼ ë‚´ìš©ì´ ë¶ˆì¼ì¹˜í•˜ê±°ë‚˜, ì‹¤ì œ ë³´ë„ ë‚´ìš©ê³¼ ë‹¤ë¥¸ 'ë‚šì‹œì„± ì˜ìƒ'ìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤.** "
                
                if final_prob < 30 and not is_misleading:
                    result_text += "ì•ˆì „í•œ ì½˜í…ì¸ ë¡œ íŒë‹¨ë©ë‹ˆë‹¤."
                elif final_prob > 60:
                    result_text += "ì£¼ì˜ê°€ í•„ìš”í•œ ì½˜í…ì¸ ì…ë‹ˆë‹¤."
                st.success(f"ğŸ” {result_text}")
                
                if final_prob < 30 or final_prob > 70:
                    st.toast(f"ğŸ¤– AIê°€ ì´ ê²°ê³¼ë¥¼ í•™ìŠµí–ˆìŠµë‹ˆë‹¤!", icon="ğŸ§ ")

        except Exception as e: st.error(f"ì˜¤ë¥˜: {e}")

# --- [9. ì‹¤í–‰ë¶€] ---
st.title("âš–ï¸ Triple-Evidence Intelligence Forensic v47.1")

with st.container(border=True):
    st.markdown("""
    ### ğŸ›¡ï¸ ë²•ì  ê³ ì§€ ë° ì±…ì„ í•œê³„ (Disclaimer)
    ë³¸ ì„œë¹„ìŠ¤ëŠ” **ì¸ê³µì§€ëŠ¥(AI) ë° ì•Œê³ ë¦¬ì¦˜ ê¸°ë°˜**ìœ¼ë¡œ ì˜ìƒì˜ ì‹ ë¢°ë„ë¥¼ ë¶„ì„í•˜ëŠ” ë³´ì¡° ë„êµ¬ì…ë‹ˆë‹¤. 
    * **ìµœì¢… íŒë‹¨ì˜ ì£¼ì²´:** ì •ë³´ì˜ ì§„ìœ„ ì—¬ë¶€ì— ëŒ€í•œ ìµœì¢…ì ì¸ íŒë‹¨ê³¼ ê·¸ì— ë”°ë¥¸ ì±…ì„ì€ **ì‚¬ìš©ì ë³¸ì¸**ì—ê²Œ ìˆìŠµë‹ˆë‹¤.
    """)
    agree = st.checkbox("ìœ„ ë‚´ìš©ì„ í™•ì¸í•˜ì˜€ìœ¼ë©°, ì´ì— ë™ì˜í•©ë‹ˆë‹¤. (ë™ì˜ ì‹œ ë¶„ì„ ë²„íŠ¼ í™œì„±í™”)")

url_input = st.text_input("ğŸ”— ë¶„ì„í•  ìœ íŠœë¸Œ URL")

if st.button("ğŸš€ ì •ë°€ ë¶„ì„ ì‹œì‘", use_container_width=True, disabled=not agree):
    if url_input: run_forensic_main(url_input)
    else: st.warning("URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")

st.divider()
st.subheader("ğŸ—‚ï¸ í•™ìŠµ ë°ì´í„° ê´€ë¦¬ (Cloud Knowledge Base)")
st.caption("â˜ï¸ ì´ ë°ì´í„°ëŠ” ì„œë²„ê°€ ì¬ë¶€íŒ…ë˜ì–´ë„ ì‚¬ë¼ì§€ì§€ ì•ŠëŠ” ì˜êµ¬ì ì¸ ì§‘ë‹¨ì§€ì„± ë°ì´í„°ì…ë‹ˆë‹¤.")

try:
    response = supabase.table("analysis_history").select("*").order("id", desc=True).execute()
    df = pd.DataFrame(response.data)
except:
    df = pd.DataFrame()

if not df.empty:
    df['Delete'] = False
    cols = ['Delete', 'id', 'analysis_date', 'video_title', 'fake_prob', 'keywords']
    df = df[cols]

    # ğŸŒŸ ê´€ë¦¬ì ê¶Œí•œ í™•ì¸ (ì‚­ì œ ë²„íŠ¼ ì œì–´)
    if st.session_state.get("is_admin", False):
        edited_df = st.data_editor(
            df,
            column_config={
                "Delete": st.column_config.CheckboxColumn("ì„ íƒ ì‚­ì œ", default=False),
                "fake_prob": st.column_config.ProgressColumn("ê°€ì§œ í™•ë¥ ", format="%d%%", min_value=0, max_value=100),
            },
            disabled=["id", "analysis_date", "video_title", "keywords"],
            hide_index=True,
            use_container_width=True
        )

        to_delete = edited_df[edited_df.Delete]
        if not to_delete.empty:
            if st.button(f"ğŸ—‘ï¸ ì„ íƒí•œ {len(to_delete)}ê±´ì˜ ê¸°ë¡ ì˜êµ¬ ì‚­ì œ", type="primary"):
                try:
                    for index, row in to_delete.iterrows():
                        supabase.table("analysis_history").delete().eq("id", row['id']).execute()
                    
                    st.success("âœ… í´ë¼ìš°ë“œ DBì—ì„œ ë°ì´í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    time.sleep(1)
                    st.rerun()
                except Exception as e:
                    st.error(f"ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    else:
        # ì¼ë°˜ ì‚¬ìš©ìëŠ” ì½ê¸° ì „ìš©
        st.dataframe(df.drop(columns=['Delete']), hide_index=True, use_container_width=True)
        st.info("ğŸ”’ ë°ì´í„° ì‚­ì œ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤. (ê´€ë¦¬ì ë¡œê·¸ì¸ í•„ìš”)")
else:
    st.info("â˜ï¸ í´ë¼ìš°ë“œ DBì— ì €ì¥ëœ ë¶„ì„ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
